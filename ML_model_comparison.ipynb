{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "main_folder_path = '/dhc/home/youngbin.ko/brain_data/data/train_gp_new_120'\n",
    "\n",
    "input_csv_path = '/dhc/home/youngbin.ko/brain_data/survival_info.csv'\n",
    "output_csv_path = '/dhc/home/youngbin.ko/brain_data/data/survival_gp.csv'\n",
    "\n",
    "with open(output_csv_path, 'w', newline='') as csv_output:\n",
    "    writer = csv.writer(csv_output)\n",
    "    \n",
    "    for subfolder_name in sorted(os.listdir(main_folder_path)):\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder_name)\n",
    "        \n",
    "        if os.path.isdir(subfolder_path):\n",
    "            with open(input_csv_path, 'r') as csv_input:\n",
    "                reader = csv.reader(csv_input)\n",
    "                \n",
    "                filtered_rows = filter(lambda row: row[0] == subfolder_name, reader)\n",
    "                sorted_rows = sorted(filtered_rows, key=lambda row: row[0])\n",
    "                \n",
    "                writer.writerows(sorted_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = np.load(\"/dhc/home/youngbin.ko/random.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "source_folder_path_1 = \"/dhc/home/youngbin.ko/brain_data/data/train_for_nf_344\"\n",
    "source_folder_path_2 = \"/dhc/home/youngbin.ko/brain_data/train_for_guassian_120\"\n",
    "target_path = \"/dhc/home/youngbin.ko/brain_data/data/train_nf\"\n",
    "\n",
    "for folder_name in os.listdir(source_folder_path_1):\n",
    "    src_path = os.path.join(source_folder_path_1, folder_name)\n",
    "    dst_path = os.path.join(target_path, folder_name)\n",
    "    shutil.copytree(src_path, dst_path)\n",
    "\n",
    "for folder_name in os.listdir(source_folder_path_2):\n",
    "    src_path = os.path.join(source_folder_path_2, folder_name)\n",
    "    dst_path = os.path.join(target_path, folder_name)\n",
    "    shutil.copytree(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder_path = \"/dhc/home/youngbin.ko/brain_data/data/train_nf\"\n",
    "target_path = \"/dhc/home/youngbin.ko/brain_data/data/train_gp_new_120\"\n",
    "\n",
    "folders_to_move = path\n",
    "\n",
    "for folder_name in os.listdir(parent_folder_path):\n",
    "    if folder_name in folders_to_move:\n",
    "        src_path = os.path.join(parent_folder_path, folder_name)\n",
    "        dst_path = os.path.join(target_path, folder_name)\n",
    "        shutil.move(src_path, dst_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/data_gp/slices_64_new\" # Shape (155, 64, 64, 4) npy\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "input_data = np.concatenate([np.expand_dims((np.load(os.path.join(input_dir, f))),0) for f in input_files], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_input = np.reshape(input_data, (120, 155*64*64*4))\n",
    "print(reshaped_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/model/output\" # Shape (1, 2539520) npy\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "input_data = np.concatenate([np.load(os.path.join(input_dir, f)) for f in input_files], axis=0)\n",
    "print(input_data.shape) #\n",
    "train_x = torch.from_numpy(input_data).float().to(device)\n",
    "\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "train_y = torch.from_numpy(output_data.flatten()).float().view(-1, 1).to(device)\n",
    "#train_y = torch.from_numpy(output_data).float().view(-1, 1)  # Reshape to (120, 1)\n",
    "print(train_y.shape)\n",
    "\n",
    "k=4\n",
    "input_folds = np.array_split(input_data, k)\n",
    "output_folds = np.array_split(output_data, k)\n",
    "\n",
    "class GaussianProcessModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()  # Use a constant mean function\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel()  # Use an RBF kernel\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "#gpytorch.likelihoods.BetaLikelihood\n",
    "model = GaussianProcessModel(train_x, train_y, likelihood)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(train_x)\n",
    "    loss = -likelihood(output).log_prob(train_y).mean()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/model/output\" # Shape (1,2539520 ) npy\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "input_data = np.concatenate([np.load(os.path.join(input_dir, f)) for f in input_files], axis=0)\n",
    "print(input_data.shape) #(120,2539520)\n",
    "train_x = torch.from_numpy(input_data).float().to(device)\n",
    "\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "train_y = torch.from_numpy(output_data.flatten()).float().view(-1, 1).to(device)  # Reshape to (120, 1)\n",
    "print(train_y.shape) #(120, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcessModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()  # Use a constant mean function\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel()  # Use an RBF kernel\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "num_epochs = 100\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kf.split(train_x)):\n",
    "    # Get train and validation data\n",
    "    train_x_fold = train_x[train_index]\n",
    "    train_y_fold = train_y[train_index]\n",
    "    valid_x_fold = train_x[valid_index]\n",
    "    valid_y_fold = train_y[valid_index]\n",
    "\n",
    "    # Initialize model and likelihood\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GaussianProcessModel(train_x_fold, train_y_fold, likelihood)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.LBFGS(model.parameters(), line_search_fn='strong_wolfe')\n",
    "\n",
    "    # Train the model\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x_fold)\n",
    "        loss = -likelihood(output).log_prob(train_y_fold).mean()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.step(closure)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Fold {fold}, Epoch {epoch}, Loss {closure().item()}\")\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(valid_x_fold)\n",
    "        loss = -likelihood(output).log_prob(valid_y_fold).mean()\n",
    "        print(f\"Fold {fold}, Validation Loss: {loss.item()}\")\n",
    "\n",
    "        f_preds = model(valid_x_fold)\n",
    "        y_preds = likelihood(model(valid_x_fold))\n",
    "        f_mean = f_preds.mean\n",
    "        f_var = f_preds.variance\n",
    "        f_covar = f_preds.covariance_matrix\n",
    "        f_samples = f_preds.sample(sample_shape=torch.Size(1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/model/output\" # Shape (1,2539520 ) npy\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "input_data = np.concatenate([np.load(os.path.join(input_dir, f)) for f in input_files], axis=0)\n",
    "print(input_data.shape) #(120,2539520)\n",
    "train_x = torch.from_numpy(input_data).float().to(device)\n",
    "\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "train_y = torch.from_numpy(output_data).float().view(-1, 1).to(device)  # Reshape to (120, 1)\n",
    "print(train_y.shape) #(120, 1)\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "num_epochs = 100\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kf.split(train_x)):\n",
    "    # Get train and validation data\n",
    "    train_x_fold = train_x[train_index]\n",
    "    train_y_fold = train_y[train_index]\n",
    "    valid_x_fold = train_x[valid_index]\n",
    "    valid_y_fold = train_y[valid_index]\n",
    "\n",
    "    # Initialize model and likelihood\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GaussianProcessModel(train_x_fold, train_y_fold, likelihood)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.LBFGS(model.parameters(), line_search_fn='strong_wolfe')\n",
    "\n",
    "    # Train the model\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x_fold)\n",
    "        loss = -likelihood(output).log_prob(train_y_fold).mean()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.step(closure)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Fold {fold}, Epoch {epoch}, Loss {closure().item()}\")\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(valid_x_fold)\n",
    "        loss = -likelihood(output).log_prob(valid_y_fold).mean()\n",
    "        print(f\"Fold {fold}, Validation Loss: {loss.item()}\")\n",
    "\n",
    "        f_preds = model(valid_x_fold)\n",
    "        y_preds = likelihood(model(valid_x_fold))\n",
    "        f_mean = f_preds.mean\n",
    "        f_var = f_preds.variance\n",
    "        f_covar = f_preds.covariance_matrix\n",
    "        f_samples = f_preds.sample(sample_shape=torch.Size(1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcessModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()  # Use a constant mean function\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel()  # Use an RBF kernel\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        #mean_x = mean_x.view(-1, 1)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dhc/home/youngbin.ko/conda3/envs/copy_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "(120, 2539520)\n",
      "torch.Size([120])\n",
      "Fold 0, Prior mean: 0.0\n",
      "Fold 0, Training labels shape: torch.Size([90])\n",
      "Fold 0, Epoch 0, Loss 144536.71875\n",
      "Fold 0, Epoch 10, Loss 77837.0859375\n",
      "Fold 0, Epoch 20, Loss 52163.8359375\n",
      "Fold 0, Epoch 30, Loss 40709.44140625\n",
      "Fold 0, Epoch 40, Loss 34614.015625\n",
      "Fold 0, Epoch 50, Loss 30844.60546875\n",
      "Fold 0, Epoch 60, Loss 28225.6640625\n",
      "Fold 0, Epoch 70, Loss 26246.900390625\n",
      "Fold 0, Epoch 80, Loss 24664.056640625\n",
      "Fold 0, Epoch 90, Loss 23348.8203125\n",
      "Fold 0, Validation Loss: 556400.1875\n",
      "Fold 1, Prior mean: 0.0\n",
      "Fold 1, Training labels shape: torch.Size([90])\n",
      "Fold 1, Epoch 0, Loss 121153.8671875\n",
      "Fold 1, Epoch 10, Loss 65233.2578125\n",
      "Fold 1, Epoch 20, Loss 43710.99609375\n",
      "Fold 1, Epoch 30, Loss 34108.875\n",
      "Fold 1, Epoch 40, Loss 28999.029296875\n",
      "Fold 1, Epoch 50, Loss 25838.91796875\n",
      "Fold 1, Epoch 60, Loss 23643.162109375\n",
      "Fold 1, Epoch 70, Loss 21984.01953125\n",
      "Fold 1, Epoch 80, Loss 20656.763671875\n",
      "Fold 1, Epoch 90, Loss 19553.8359375\n",
      "Fold 1, Validation Loss: 957625.4375\n",
      "Fold 2, Prior mean: 0.0\n",
      "Fold 2, Training labels shape: torch.Size([90])\n",
      "Fold 2, Epoch 0, Loss 123711.6796875\n",
      "Fold 2, Epoch 10, Loss 66609.6875\n",
      "Fold 2, Epoch 20, Loss 44632.875\n",
      "Fold 2, Epoch 30, Loss 34827.96875\n",
      "Fold 2, Epoch 40, Loss 29610.1953125\n",
      "Fold 2, Epoch 50, Loss 26383.322265625\n",
      "Fold 2, Epoch 60, Loss 24141.16796875\n",
      "Fold 2, Epoch 70, Loss 22446.95703125\n",
      "Fold 2, Epoch 80, Loss 21091.625\n",
      "Fold 2, Epoch 90, Loss 19965.369140625\n",
      "Fold 2, Validation Loss: 940253.125\n",
      "Fold 3, Prior mean: 0.0\n",
      "Fold 3, Training labels shape: torch.Size([90])\n",
      "Fold 3, Epoch 0, Loss 138640.859375\n",
      "Fold 3, Epoch 10, Loss 74658.7265625\n",
      "Fold 3, Epoch 20, Loss 50032.05078125\n",
      "Fold 3, Epoch 30, Loss 39044.66015625\n",
      "Fold 3, Epoch 40, Loss 33197.73828125\n",
      "Fold 3, Epoch 50, Loss 29581.927734375\n",
      "Fold 3, Epoch 60, Loss 27069.689453125\n",
      "Fold 3, Epoch 70, Loss 25171.4921875\n",
      "Fold 3, Epoch 80, Loss 23653.072265625\n",
      "Fold 3, Epoch 90, Loss 22391.345703125\n",
      "Fold 3, Validation Loss: 666748.8125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "# check if GPU is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "#device = torch.device('cpu')\n",
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/model/output\" # Shape (1,2539520 ) npy\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "input_data = np.concatenate([np.load(os.path.join(input_dir, f)) for f in input_files], axis=0)\n",
    "print(input_data.shape) #(120,2539520)\n",
    "train_x = torch.from_numpy(input_data).float().to(device)\n",
    "#train_x = torch.from_numpy(input_data).float()\n",
    "\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "train_y = torch.from_numpy(output_data.flatten()).float().to(device)\n",
    "#train_y = torch.from_numpy(output_data.flatten()).float().view(1, -1).to(device)  # Reshape to (120, 1)\n",
    "#train_y = torch.from_numpy(output_data.flatten()).float().view(-1, 1) \n",
    "print(train_y.shape) #(120, 1)\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "num_epochs = 100\n",
    "\n",
    "class GaussianProcessModel(gpytorch.models.ExactGP): #change to linear regression (output from NF) \n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()  # Use a constant mean function\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel()  # Use an RBF kernel\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kf.split(train_x)):\n",
    "    # Get train and validation data\n",
    "    train_x_fold = train_x[train_index]\n",
    "    train_y_fold = train_y[train_index]\n",
    "    valid_x_fold = train_x[valid_index]\n",
    "    valid_y_fold = train_y[valid_index]\n",
    "\n",
    "    # Initialize model and likelihood\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "    model = GaussianProcessModel(train_x_fold, train_y_fold, likelihood).to(device)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "    print(f\"Fold {fold}, Prior mean: {model.mean_module.constant.item()}\")\n",
    "    print(f\"Fold {fold}, Training labels shape: {train_y_fold.shape}\")\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x_fold)\n",
    "        #loss = -likelihood(output).log_prob(train_y_fold).mean()\n",
    "        #loss = ((train_y_fold-output)**2)\n",
    "        loss = -mll(output, train_y_fold)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Fold {fold}, Epoch {epoch}, Loss {loss.item()}\")\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(valid_x_fold)\n",
    "        loss = -likelihood(output).log_prob(valid_y_fold).mean()\n",
    "        print(f\"Fold {fold}, Validation Loss: {loss.item()}\")\n",
    "\n",
    "        f_preds = model(valid_x_fold)\n",
    "        y_preds = likelihood(model(valid_x_fold))\n",
    "        f_mean = f_preds.mean\n",
    "        f_var = f_preds.variance\n",
    "        f_covar = f_preds.covariance_matrix\n",
    "        #f_samples = f_preds.sample(sample_shape=torch.Size(1000,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258,\n",
       "        6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258,\n",
       "        6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258,\n",
       "        6.3258, 6.3258, 6.3258])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds.mean.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpytorch.distributions.multivariate_normal.MultivariateNormal"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258,\n",
      "        6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258,\n",
      "        6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258,\n",
      "        6.3258, 6.3258, 6.3258], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 788.,  465.,  336., 1283.,   33.,  229., 1592.,  614.,   58.,  416.,\n",
      "         240.,  300.,  630.,  630.,  355.,  737.,   67.,  828.,  747.,  103.,\n",
      "         382.,  372.,  232.,  434.,  244.,   82.,  147.,  260.,  184.,   62.],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "(120, 2539520)\n",
      "Training labels shape: torch.Size([72])\n",
      "Epoch 0, Batch 0, Loss 554946.0625\n",
      "Validation Loss: 54195292.0\n",
      "Epoch 1, Batch 0, Loss 590920512.0\n",
      "Validation Loss: 62038020.666666664\n",
      "Epoch 2, Batch 0, Loss 393443552.0\n",
      "Validation Loss: 49332696.666666664\n",
      "Epoch 3, Batch 0, Loss 90849408.0\n",
      "Validation Loss: 59123485.0\n",
      "Epoch 4, Batch 0, Loss 186308864.0\n",
      "Validation Loss: 53932918.0\n",
      "Epoch 5, Batch 0, Loss 153213168.0\n",
      "Validation Loss: 57937386.333333336\n",
      "Epoch 6, Batch 0, Loss 149286384.0\n",
      "Validation Loss: 31123444.5\n",
      "Epoch 7, Batch 0, Loss 168872880.0\n",
      "Validation Loss: 26625763.666666668\n",
      "Epoch 8, Batch 0, Loss 66833356.0\n",
      "Validation Loss: 3795137.8125\n",
      "Epoch 9, Batch 0, Loss 10387434.0\n",
      "Validation Loss: 213051.58951822916\n",
      "Epoch 10, Batch 0, Loss 59301652.0\n",
      "Validation Loss: 25611759.333333332\n",
      "Epoch 11, Batch 0, Loss 19512442.0\n",
      "Validation Loss: 50561393.666666664\n",
      "Epoch 12, Batch 0, Loss 70840848.0\n",
      "Validation Loss: 113487710.66666667\n",
      "Epoch 13, Batch 0, Loss 124057000.0\n",
      "Validation Loss: 69686667.33333333\n",
      "Epoch 14, Batch 0, Loss 111338984.0\n",
      "Validation Loss: 94382159.33333333\n",
      "Epoch 15, Batch 0, Loss 170349776.0\n",
      "Validation Loss: 16652849.75\n",
      "Epoch 16, Batch 0, Loss 90757536.0\n",
      "Validation Loss: 49490910.0\n",
      "Epoch 17, Batch 0, Loss 41054436.0\n",
      "Validation Loss: 69711880.66666667\n",
      "Epoch 18, Batch 0, Loss 194099168.0\n",
      "Validation Loss: 76762342.0\n",
      "Epoch 19, Batch 0, Loss 873189824.0\n",
      "Validation Loss: 18479336.0\n",
      "Epoch 20, Batch 0, Loss 35556548.0\n",
      "Validation Loss: 3927715.9166666665\n",
      "Epoch 21, Batch 0, Loss 33412106.0\n",
      "Validation Loss: 339943.8502604167\n",
      "Epoch 22, Batch 0, Loss 20191976.0\n",
      "Validation Loss: 1950989.3125\n",
      "Epoch 23, Batch 0, Loss 10216184.0\n",
      "Validation Loss: 28165606.0\n",
      "Epoch 24, Batch 0, Loss 133741256.0\n",
      "Validation Loss: 21348871.166666668\n",
      "Epoch 25, Batch 0, Loss 62703196.0\n",
      "Validation Loss: 22774605.166666668\n",
      "Epoch 26, Batch 0, Loss 34497368.0\n",
      "Validation Loss: 4993041.333333333\n",
      "Epoch 27, Batch 0, Loss 15624038.0\n",
      "Validation Loss: 11588795.333333334\n",
      "Epoch 28, Batch 0, Loss 32478382.0\n",
      "Validation Loss: 2641553.1458333335\n",
      "Epoch 29, Batch 0, Loss 49511924.0\n",
      "Validation Loss: 1635433.65625\n",
      "Epoch 30, Batch 0, Loss 13823808.0\n",
      "Validation Loss: 3242678.2291666665\n",
      "Epoch 31, Batch 0, Loss 8165902.0\n",
      "Validation Loss: 4031231.1875\n",
      "Epoch 32, Batch 0, Loss 30042954.0\n",
      "Validation Loss: 8517842.458333334\n",
      "Epoch 33, Batch 0, Loss 9218148.0\n",
      "Validation Loss: 4298729.0\n",
      "Epoch 34, Batch 0, Loss 19154084.0\n",
      "Validation Loss: 55319948.0\n",
      "Epoch 35, Batch 0, Loss 67658328.0\n",
      "Validation Loss: 170743457.33333334\n",
      "Epoch 36, Batch 0, Loss 264568784.0\n",
      "Validation Loss: 254175570.66666666\n",
      "Epoch 37, Batch 0, Loss 448459360.0\n",
      "Validation Loss: 132304912.0\n",
      "Epoch 38, Batch 0, Loss 265917440.0\n",
      "Validation Loss: 6254900.833333333\n",
      "Epoch 39, Batch 0, Loss 126805440.0\n",
      "Validation Loss: 26824946.833333332\n",
      "Epoch 40, Batch 0, Loss 200787856.0\n",
      "Validation Loss: 89611718.66666667\n",
      "Epoch 41, Batch 0, Loss 732005568.0\n",
      "Validation Loss: 72136039.33333333\n",
      "Epoch 42, Batch 0, Loss 188748544.0\n",
      "Validation Loss: 25623316.0\n",
      "Epoch 43, Batch 0, Loss 131556704.0\n",
      "Validation Loss: 26362794.166666668\n",
      "Epoch 44, Batch 0, Loss 68364960.0\n",
      "Validation Loss: 21619624.0\n",
      "Epoch 45, Batch 0, Loss 852269440.0\n",
      "Validation Loss: 95155008.66666667\n",
      "Epoch 46, Batch 0, Loss 152356720.0\n",
      "Validation Loss: 100525590.66666667\n",
      "Epoch 47, Batch 0, Loss 140964240.0\n",
      "Validation Loss: 116243646.0\n",
      "Epoch 48, Batch 0, Loss 242709008.0\n",
      "Validation Loss: 20407154.0\n",
      "Epoch 49, Batch 0, Loss 53622220.0\n",
      "Validation Loss: 2075988.0\n",
      "Epoch 50, Batch 0, Loss 124962280.0\n",
      "Validation Loss: 5267302.208333333\n",
      "Epoch 51, Batch 0, Loss 229515200.0\n",
      "Validation Loss: 1007981.8333333334\n",
      "Epoch 52, Batch 0, Loss 5042635.0\n",
      "Validation Loss: 961970.7395833334\n",
      "Epoch 53, Batch 0, Loss 47471032.0\n",
      "Validation Loss: 8680375.5\n",
      "Epoch 54, Batch 0, Loss 18395020.0\n",
      "Validation Loss: 13929329.416666666\n",
      "Epoch 55, Batch 0, Loss 44886904.0\n",
      "Validation Loss: 13251535.25\n",
      "Epoch 56, Batch 0, Loss 12342525.0\n",
      "Validation Loss: 4885672.5\n",
      "Epoch 57, Batch 0, Loss 13993395.0\n",
      "Validation Loss: 3376050.3541666665\n",
      "Epoch 58, Batch 0, Loss 66169332.0\n",
      "Validation Loss: 804983.5208333334\n",
      "Epoch 59, Batch 0, Loss 2346241.0\n",
      "Validation Loss: 3103179.9791666665\n",
      "Epoch 60, Batch 0, Loss 7391152.5\n",
      "Validation Loss: 7572168.583333333\n",
      "Epoch 61, Batch 0, Loss 3711745.0\n",
      "Validation Loss: 14417783.25\n",
      "Epoch 62, Batch 0, Loss 42786048.0\n",
      "Validation Loss: 50354095.333333336\n",
      "Epoch 63, Batch 0, Loss 62783352.0\n",
      "Validation Loss: 36203967.0\n",
      "Epoch 64, Batch 0, Loss 82503216.0\n",
      "Validation Loss: 24006069.0\n",
      "Epoch 65, Batch 0, Loss 67652920.0\n",
      "Validation Loss: 276196.0403645833\n",
      "Epoch 66, Batch 0, Loss 11439247.0\n",
      "Validation Loss: 1538155.9270833333\n",
      "Epoch 67, Batch 0, Loss 194217536.0\n",
      "Validation Loss: 23617282.166666668\n",
      "Epoch 68, Batch 0, Loss 26383462.0\n",
      "Validation Loss: 9413365.666666666\n",
      "Epoch 69, Batch 0, Loss 431997792.0\n",
      "Validation Loss: 11052042.0\n",
      "Epoch 70, Batch 0, Loss 59850348.0\n",
      "Validation Loss: 38770873.666666664\n",
      "Epoch 71, Batch 0, Loss 35156888.0\n",
      "Validation Loss: 341324.8255208333\n",
      "Epoch 72, Batch 0, Loss 344715392.0\n",
      "Validation Loss: 30149006.666666668\n",
      "Epoch 73, Batch 0, Loss 151147664.0\n",
      "Validation Loss: 66273211.333333336\n",
      "Epoch 74, Batch 0, Loss 77478728.0\n",
      "Validation Loss: 40091336.666666664\n",
      "Epoch 75, Batch 0, Loss 116236952.0\n",
      "Validation Loss: 27475444.333333332\n",
      "Epoch 76, Batch 0, Loss 109069144.0\n",
      "Validation Loss: 10284588.5\n",
      "Epoch 77, Batch 0, Loss 26870190.0\n",
      "Validation Loss: 3623079.0\n",
      "Epoch 78, Batch 0, Loss 28574494.0\n",
      "Validation Loss: 8754473.833333334\n",
      "Epoch 79, Batch 0, Loss 224236272.0\n",
      "Validation Loss: 98657778.66666667\n",
      "Epoch 80, Batch 0, Loss 2327833088.0\n",
      "Validation Loss: 41099260.0\n",
      "Epoch 81, Batch 0, Loss 156303920.0\n",
      "Validation Loss: 41628766.666666664\n",
      "Epoch 82, Batch 0, Loss 79936424.0\n",
      "Validation Loss: 41041318.333333336\n",
      "Epoch 83, Batch 0, Loss 58314352.0\n",
      "Validation Loss: 18512596.666666668\n",
      "Epoch 84, Batch 0, Loss 58120840.0\n",
      "Validation Loss: 807275.8072916666\n",
      "Epoch 85, Batch 0, Loss 115550536.0\n",
      "Validation Loss: 1131819.171875\n",
      "Epoch 86, Batch 0, Loss 16711808.0\n",
      "Validation Loss: 1912082.3229166667\n",
      "Epoch 87, Batch 0, Loss 44698696.0\n",
      "Validation Loss: 22524800.166666668\n",
      "Epoch 88, Batch 0, Loss 48201564.0\n",
      "Validation Loss: 17702305.333333332\n",
      "Epoch 89, Batch 0, Loss 46606008.0\n",
      "Validation Loss: 23640928.833333332\n",
      "Epoch 90, Batch 0, Loss 16677734.0\n",
      "Validation Loss: 16334329.833333334\n",
      "Epoch 91, Batch 0, Loss 75096448.0\n",
      "Validation Loss: 1267650.4270833333\n",
      "Epoch 92, Batch 0, Loss 128750848.0\n",
      "Validation Loss: 12754482.166666666\n",
      "Epoch 93, Batch 0, Loss 19030848.0\n",
      "Validation Loss: 11888209.416666666\n",
      "Epoch 94, Batch 0, Loss 143543984.0\n",
      "Validation Loss: 46853460.333333336\n",
      "Epoch 95, Batch 0, Loss 22428630.0\n",
      "Validation Loss: 38124799.666666664\n",
      "Epoch 96, Batch 0, Loss 375795296.0\n",
      "Validation Loss: 21553032.5\n",
      "Epoch 97, Batch 0, Loss 74515328.0\n",
      "Validation Loss: 6060369.0\n",
      "Epoch 98, Batch 0, Loss 40877876.0\n",
      "Validation Loss: 69062617.33333333\n",
      "Epoch 99, Batch 0, Loss 141613120.0\n",
      "Validation Loss: 178099384.0\n",
      "Test Loss: 178346153.33333334\n"
     ]
    }
   ],
   "source": [
    "#linear regression with output from NF\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/model/output\" # Shape (1,2539520 ) npy\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "input_data = np.concatenate([np.load(os.path.join(input_dir, f)) for f in input_files], axis=0)\n",
    "print(input_data.shape) #(120,2539520)\n",
    "\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "input_data = torch.from_numpy(input_data).float().to(device)\n",
    "output_data = torch.from_numpy(output_data.flatten()).float().to(device)\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(input_data, output_data, test_size=0.2, random_state=10)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=0.25, random_state=10)\n",
    "\n",
    "class BrainDataset(Dataset):\n",
    "    def __init__(self, input_data, output_data):\n",
    "        self.input_data = input_data\n",
    "        self.output_data = output_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.input_data[index]\n",
    "        y = self.output_data[index]\n",
    "        return x, y\n",
    "\n",
    "dataset = BrainDataset(input_data, output_data)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_X, train_y), batch_size=10, shuffle=True) # shape (72, 155, 64, 64, 4)\n",
    "valid_loader = DataLoader(TensorDataset(valid_X, valid_y), batch_size=10)\n",
    "test_loader = DataLoader(TensorDataset(test_X, test_y), batch_size=10)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "class LinearRegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_data.shape[1], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "model = LinearRegressionModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "def mse_loss(preds, targets):\n",
    "    return torch.mean((preds - targets)**2)\n",
    "\n",
    "print(f\"Training labels shape: {train_y.shape}\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        loss = mse_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}, Loss {loss.item()}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for data, target in valid_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss += mse_loss(output.flatten(), target).item() * data.size(0)\n",
    "        val_loss /= len(valid_loader.dataset)\n",
    "        print(f\"Validation Loss: {val_loss}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += mse_loss(output.flatten(), target).item() * data.size(0)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13354.630407964623"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math \n",
    "math.sqrt(178346153.33333334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 236800.9062, Val Loss: 140198.2682\n",
      "Epoch [2/100], Train Loss: 138817.2422, Val Loss: 164167.1217\n",
      "Epoch [3/100], Train Loss: 115249.2087, Val Loss: 139529.9017\n",
      "Epoch [4/100], Train Loss: 114156.2715, Val Loss: 162080.0202\n",
      "Epoch [5/100], Train Loss: 120935.0771, Val Loss: 143291.2266\n",
      "Epoch [6/100], Train Loss: 111241.9325, Val Loss: 150475.8145\n",
      "Epoch [7/100], Train Loss: 117123.2451, Val Loss: 143783.2845\n",
      "Epoch [8/100], Train Loss: 124452.5962, Val Loss: 145489.3887\n",
      "Epoch [9/100], Train Loss: 120290.6357, Val Loss: 153915.1549\n",
      "Epoch [10/100], Train Loss: 156452.8516, Val Loss: 139449.2077\n",
      "Epoch [11/100], Train Loss: 134681.5356, Val Loss: 148795.8275\n",
      "Epoch [12/100], Train Loss: 122880.5576, Val Loss: 139511.2210\n",
      "Epoch [13/100], Train Loss: 112056.1023, Val Loss: 151516.5477\n",
      "Epoch [14/100], Train Loss: 116363.6484, Val Loss: 148350.7012\n",
      "Epoch [15/100], Train Loss: 112022.3271, Val Loss: 148635.8504\n",
      "Epoch [16/100], Train Loss: 112136.6595, Val Loss: 140904.3190\n",
      "Epoch [17/100], Train Loss: 211328.2769, Val Loss: 140645.3672\n",
      "Epoch [18/100], Train Loss: 178766.4897, Val Loss: 140357.3438\n",
      "Epoch [19/100], Train Loss: 127669.3186, Val Loss: 164382.6989\n",
      "Epoch [20/100], Train Loss: 130515.8099, Val Loss: 151681.3701\n",
      "Epoch [21/100], Train Loss: 115752.2920, Val Loss: 141078.5319\n",
      "Epoch [22/100], Train Loss: 113822.8167, Val Loss: 150148.4007\n",
      "Epoch [23/100], Train Loss: 185430.5020, Val Loss: 139865.8112\n",
      "Epoch [24/100], Train Loss: 130411.8232, Val Loss: 148074.7962\n",
      "Epoch [25/100], Train Loss: 114573.8318, Val Loss: 141379.0999\n",
      "Epoch [26/100], Train Loss: 177836.7412, Val Loss: 142810.1868\n",
      "Epoch [27/100], Train Loss: 118175.0627, Val Loss: 147338.2415\n",
      "Epoch [28/100], Train Loss: 210385.5537, Val Loss: 140522.6410\n",
      "Epoch [29/100], Train Loss: 142733.0107, Val Loss: 165172.7972\n",
      "Epoch [30/100], Train Loss: 158190.5044, Val Loss: 217429.8724\n",
      "Epoch [31/100], Train Loss: 121792.5593, Val Loss: 144845.4740\n",
      "Epoch [32/100], Train Loss: 226932.7107, Val Loss: 150198.0645\n",
      "Epoch [33/100], Train Loss: 114869.0381, Val Loss: 139136.0514\n",
      "Epoch [34/100], Train Loss: 120480.8601, Val Loss: 160332.8613\n",
      "Epoch [35/100], Train Loss: 115523.3553, Val Loss: 140206.9232\n",
      "Epoch [36/100], Train Loss: 125321.7317, Val Loss: 142786.8958\n",
      "Epoch [37/100], Train Loss: 205224.9126, Val Loss: 139209.7259\n",
      "Epoch [38/100], Train Loss: 120578.3726, Val Loss: 141858.3034\n",
      "Epoch [39/100], Train Loss: 119270.1675, Val Loss: 148417.7480\n",
      "Epoch [40/100], Train Loss: 123318.4199, Val Loss: 141313.1410\n",
      "Epoch [41/100], Train Loss: 124016.6665, Val Loss: 155267.2520\n",
      "Epoch [42/100], Train Loss: 113890.1519, Val Loss: 140068.5273\n",
      "Epoch [43/100], Train Loss: 110505.4205, Val Loss: 145970.4232\n",
      "Epoch [44/100], Train Loss: 130215.6189, Val Loss: 143251.2936\n",
      "Epoch [45/100], Train Loss: 119361.4524, Val Loss: 143545.6569\n",
      "Epoch [46/100], Train Loss: 126759.2615, Val Loss: 152561.2643\n",
      "Epoch [47/100], Train Loss: 112937.2898, Val Loss: 139543.5352\n",
      "Epoch [48/100], Train Loss: 111941.4265, Val Loss: 150264.9424\n",
      "Epoch [49/100], Train Loss: 116196.1971, Val Loss: 149236.2409\n",
      "Epoch [50/100], Train Loss: 115218.5537, Val Loss: 139775.7520\n",
      "Epoch [51/100], Train Loss: 152343.1694, Val Loss: 142091.0651\n",
      "Epoch [52/100], Train Loss: 116951.1309, Val Loss: 147151.7806\n",
      "Epoch [53/100], Train Loss: 120628.2063, Val Loss: 141663.6361\n",
      "Epoch [54/100], Train Loss: 117641.3962, Val Loss: 151427.6090\n",
      "Epoch [55/100], Train Loss: 117720.9414, Val Loss: 140633.7493\n",
      "Epoch [56/100], Train Loss: 115036.0288, Val Loss: 148945.8410\n",
      "Epoch [57/100], Train Loss: 116150.2676, Val Loss: 140370.8835\n",
      "Epoch [58/100], Train Loss: 112727.1152, Val Loss: 145446.0645\n",
      "Epoch [59/100], Train Loss: 127380.7461, Val Loss: 140927.8607\n",
      "Epoch [60/100], Train Loss: 153254.9629, Val Loss: 140466.0690\n",
      "Epoch [61/100], Train Loss: 115717.9097, Val Loss: 142525.0186\n",
      "Epoch [62/100], Train Loss: 177339.9443, Val Loss: 145386.6172\n",
      "Epoch [63/100], Train Loss: 120416.5420, Val Loss: 139758.8096\n",
      "Epoch [64/100], Train Loss: 149718.1833, Val Loss: 150297.5957\n",
      "Epoch [65/100], Train Loss: 110210.4403, Val Loss: 140175.5404\n",
      "Epoch [66/100], Train Loss: 121741.6475, Val Loss: 144071.5518\n",
      "Epoch [67/100], Train Loss: 117148.4015, Val Loss: 158235.6172\n",
      "Epoch [68/100], Train Loss: 114430.1024, Val Loss: 142027.4648\n",
      "Epoch [69/100], Train Loss: 117364.3628, Val Loss: 145983.2070\n",
      "Epoch [70/100], Train Loss: 127744.8567, Val Loss: 164192.8480\n",
      "Epoch [71/100], Train Loss: 195453.3799, Val Loss: 140707.9935\n",
      "Epoch [72/100], Train Loss: 128628.4075, Val Loss: 145223.8470\n",
      "Epoch [73/100], Train Loss: 127647.0068, Val Loss: 160179.2764\n",
      "Epoch [74/100], Train Loss: 113871.0984, Val Loss: 140012.3968\n",
      "Epoch [75/100], Train Loss: 124734.0947, Val Loss: 141113.6032\n",
      "Epoch [76/100], Train Loss: 112317.9226, Val Loss: 155688.3451\n",
      "Epoch [77/100], Train Loss: 123803.1218, Val Loss: 140921.1022\n",
      "Epoch [78/100], Train Loss: 158151.9314, Val Loss: 145664.2038\n",
      "Epoch [79/100], Train Loss: 113824.7197, Val Loss: 142560.2702\n",
      "Epoch [80/100], Train Loss: 111014.3121, Val Loss: 149162.9486\n",
      "Epoch [81/100], Train Loss: 119016.4148, Val Loss: 141257.1296\n",
      "Epoch [82/100], Train Loss: 121215.6924, Val Loss: 147626.5807\n",
      "Epoch [83/100], Train Loss: 181976.5400, Val Loss: 160785.3480\n",
      "Epoch [84/100], Train Loss: 133004.9258, Val Loss: 141501.2038\n",
      "Epoch [85/100], Train Loss: 126259.2683, Val Loss: 153529.5811\n",
      "Epoch [86/100], Train Loss: 119504.5549, Val Loss: 140815.7943\n",
      "Epoch [87/100], Train Loss: 119118.8831, Val Loss: 155256.5247\n",
      "Epoch [88/100], Train Loss: 116013.1528, Val Loss: 140067.7191\n",
      "Epoch [89/100], Train Loss: 121703.4253, Val Loss: 151628.6341\n",
      "Epoch [90/100], Train Loss: 116118.6484, Val Loss: 146715.7285\n",
      "Epoch [91/100], Train Loss: 116544.6528, Val Loss: 146336.7812\n",
      "Epoch [92/100], Train Loss: 113587.3826, Val Loss: 148432.9160\n",
      "Epoch [93/100], Train Loss: 121915.0122, Val Loss: 144052.2152\n",
      "Epoch [94/100], Train Loss: 118761.1831, Val Loss: 144231.6436\n",
      "Epoch [95/100], Train Loss: 112984.6249, Val Loss: 151473.1549\n",
      "Epoch [96/100], Train Loss: 110839.8018, Val Loss: 140377.5033\n",
      "Epoch [97/100], Train Loss: 113794.2070, Val Loss: 143740.4544\n",
      "Epoch [98/100], Train Loss: 120945.2584, Val Loss: 155103.4629\n",
      "Epoch [99/100], Train Loss: 121042.5420, Val Loss: 139719.8379\n",
      "Epoch [100/100], Train Loss: 116567.4951, Val Loss: 152835.9313\n",
      "Test Loss: 208203.9362\n"
     ]
    }
   ],
   "source": [
    "# cnn with original data\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/data_gp/slices_64_new\"\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "X = []\n",
    "for f in input_files:\n",
    "    data = np.load(os.path.join(input_dir, f))  # Shape (155, 64, 64, 4)\n",
    "    X.append(data)\n",
    "X = np.stack(X, axis=0)  # Shape (120, 155, 64, 64, 4)\n",
    "X = np.transpose(X, (0, 4, 2, 3, 1))\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, output_data, test_size=0.2, random_state=10)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=0.25, random_state=10)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(train_X).float().to(device), torch.from_numpy(train_y).float().to(device)), batch_size=10, shuffle=True) # shape (72, 155, 64, 64, 4)\n",
    "valid_loader = DataLoader(TensorDataset(torch.from_numpy(valid_X).float().to(device), torch.from_numpy(valid_y).float().to(device)), batch_size=10)\n",
    "test_loader = DataLoader(TensorDataset(torch.from_numpy(test_X).float().to(device), torch.from_numpy(test_y).float().to(device)), batch_size=10)\n",
    "\n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(4, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16 * 19, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 16 * 16 * 19)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "model = MyCNN().to(device)\n",
    "\n",
    "#criterion = nn.MSELoss()\n",
    "def mse_loss(preds, targets):\n",
    "    return torch.mean((preds - targets)**2)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, mse_loss, optimizer)\n",
    "    val_loss = evaluate(model, valid_loader, mse_loss)\n",
    "    print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}'.format(epoch+1, num_epochs, train_loss, val_loss))\n",
    "\n",
    "test_loss = evaluate(model, test_loader, mse_loss)\n",
    "print('Test Loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390.94236314321324"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(152835.9313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dhc/home/youngbin.ko/conda3/envs/copy_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 2539520)\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/data_gp/slices_64_new\"\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "result = []\n",
    "\n",
    "for f in input_files:\n",
    "  data = np.load(os.path.join(input_dir, f))\n",
    "  result.append(data.reshape(1, -1))\n",
    "\n",
    "result = np.concatenate(result, axis=0)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 303609.6875\n",
      "Epoch 10, Loss 302756.53125\n",
      "Epoch 20, Loss 301905.75\n",
      "Epoch 30, Loss 301057.5\n",
      "Epoch 40, Loss 300212.125\n",
      "Epoch 50, Loss 299369.5625\n",
      "Epoch 60, Loss 298529.90625\n",
      "Epoch 70, Loss 297693.1875\n",
      "Epoch 80, Loss 296859.4375\n",
      "Epoch 90, Loss 296028.625\n",
      "Epoch 100, Loss 295200.75\n",
      "Epoch 110, Loss 294375.75\n",
      "Epoch 120, Loss 293553.6875\n",
      "Epoch 130, Loss 292734.5625\n",
      "Epoch 140, Loss 291918.28125\n",
      "Epoch 150, Loss 291104.90625\n",
      "Epoch 160, Loss 290294.40625\n",
      "Epoch 170, Loss 289486.75\n",
      "Epoch 180, Loss 288681.9375\n",
      "Epoch 190, Loss 287880.0625\n",
      "Validation Loss: 410864.9375\n"
     ]
    }
   ],
   "source": [
    "#GP using MSE, original data\n",
    "input_data = result\n",
    "\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "train_x, test_x, train_y, test_y = train_test_split(input_data, output_data, test_size=0.2, random_state=10)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.25, random_state=10)\n",
    "\n",
    "train_x = torch.from_numpy(train_x).float().to(device)\n",
    "train_y = torch.from_numpy(train_y).float().to(device)\n",
    "valid_x = torch.from_numpy(valid_x).float().to(device)\n",
    "valid_y = torch.from_numpy(valid_y).float().to(device)\n",
    "test_x = torch.from_numpy(test_x).float().to(device)\n",
    "test_y = torch.from_numpy(test_y).float().to(device)\n",
    "\n",
    "# Define Gaussian Process Regression model\n",
    "class GaussianProcessRegression(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y):\n",
    "        super().__init__(train_x, train_y, gpytorch.likelihoods.GaussianLikelihood())\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Initialize model\n",
    "model = GaussianProcessRegression(train_x, train_y).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = loss_fn(output.mean, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss {loss.item()}\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(valid_x)\n",
    "    loss = loss_fn(output.mean, valid_y)\n",
    "    print(f\"Validation Loss: {loss.item()}\")\n",
    "\n",
    "    y_preds = output.mean\n",
    "    y_var = output.variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 109497.2421875\n",
      "Validation Loss: 185085.234375, Validation MSE 430641.3125\n",
      "Validation Loss: 172625.625, Validation MSE 430539.25\n",
      "Validation Loss: 161330.984375, Validation MSE 430437.6875\n",
      "Validation Loss: 151100.03125, Validation MSE 430336.5625\n",
      "Validation Loss: 141836.46875, Validation MSE 430236.25\n",
      "Validation Loss: 133449.78125, Validation MSE 430136.75\n",
      "Validation Loss: 125855.53125, Validation MSE 430038.25\n",
      "Validation Loss: 118975.7421875, Validation MSE 429940.75\n",
      "Validation Loss: 112738.875, Validation MSE 429844.3125\n",
      "Validation Loss: 107079.609375, Validation MSE 429749.09375\n",
      "Epoch 10, Loss 58947.0546875\n",
      "Validation Loss: 101938.84375, Validation MSE 429655.1875\n",
      "Validation Loss: 97263.09375, Validation MSE 429562.4375\n",
      "Validation Loss: 93004.328125, Validation MSE 429471.09375\n",
      "Validation Loss: 89119.46875, Validation MSE 429381.09375\n",
      "Validation Loss: 85569.921875, Validation MSE 429292.4375\n",
      "Validation Loss: 82321.28125, Validation MSE 429205.09375\n",
      "Validation Loss: 79342.8671875, Validation MSE 429119.21875\n",
      "Validation Loss: 76607.28125, Validation MSE 429034.6875\n",
      "Validation Loss: 74090.2265625, Validation MSE 428951.46875\n",
      "Validation Loss: 71769.9375, Validation MSE 428869.5625\n",
      "Epoch 20, Loss 39493.4609375\n",
      "Validation Loss: 69627.15625, Validation MSE 428789.0\n",
      "Validation Loss: 67644.640625, Validation MSE 428709.75\n",
      "Validation Loss: 65807.0625, Validation MSE 428631.75\n",
      "Validation Loss: 64100.69921875, Validation MSE 428554.96875\n",
      "Validation Loss: 62513.390625, Validation MSE 428479.4375\n",
      "Validation Loss: 61034.140625, Validation MSE 428405.0\n",
      "Validation Loss: 59653.203125, Validation MSE 428331.75\n",
      "Validation Loss: 58361.796875, Validation MSE 428259.59375\n",
      "Validation Loss: 57152.0703125, Validation MSE 428188.5\n",
      "Validation Loss: 56016.94921875, Validation MSE 428118.375\n",
      "Epoch 30, Loss 30814.509765625\n",
      "Validation Loss: 54950.0859375, Validation MSE 428049.3125\n",
      "Validation Loss: 53945.7421875, Validation MSE 427981.09375\n",
      "Validation Loss: 52998.76171875, Validation MSE 427913.84375\n",
      "Validation Loss: 52104.4765625, Validation MSE 427847.4375\n",
      "Validation Loss: 51258.66796875, Validation MSE 427781.9375\n",
      "Validation Loss: 50457.515625, Validation MSE 427717.125\n",
      "Validation Loss: 49697.55859375, Validation MSE 427653.21875\n",
      "Validation Loss: 48975.6484375, Validation MSE 427589.96875\n",
      "Validation Loss: 48288.9140625, Validation MSE 427527.4375\n",
      "Validation Loss: 47634.76171875, Validation MSE 427465.5625\n",
      "Epoch 40, Loss 26195.8203125\n",
      "Validation Loss: 47010.8203125, Validation MSE 427404.34375\n",
      "Validation Loss: 46414.91796875, Validation MSE 427343.6875\n",
      "Validation Loss: 45845.0859375, Validation MSE 427283.625\n",
      "Validation Loss: 45299.5, Validation MSE 427224.1875\n",
      "Validation Loss: 44776.52734375, Validation MSE 427165.25\n",
      "Validation Loss: 44274.640625, Validation MSE 427106.75\n",
      "Validation Loss: 43792.43359375, Validation MSE 427048.8125\n",
      "Validation Loss: 43328.65625, Validation MSE 426991.3125\n",
      "Validation Loss: 42882.1015625, Validation MSE 426934.1875\n",
      "Validation Loss: 42451.71484375, Validation MSE 426877.46875\n",
      "Epoch 50, Loss 23339.302734375\n",
      "Validation Loss: 42036.484375, Validation MSE 426821.21875\n",
      "Validation Loss: 41635.5, Validation MSE 426765.34375\n",
      "Validation Loss: 41247.9140625, Validation MSE 426709.75\n",
      "Validation Loss: 40872.94140625, Validation MSE 426654.59375\n",
      "Validation Loss: 40509.859375, Validation MSE 426599.6875\n",
      "Validation Loss: 40157.984375, Validation MSE 426545.0625\n",
      "Validation Loss: 39816.71875, Validation MSE 426490.75\n",
      "Validation Loss: 39485.46875, Validation MSE 426436.75\n",
      "Validation Loss: 39163.71484375, Validation MSE 426383.0\n",
      "Validation Loss: 38850.953125, Validation MSE 426329.5\n",
      "Epoch 60, Loss 21354.359375\n",
      "Validation Loss: 38546.7265625, Validation MSE 426276.25\n",
      "Validation Loss: 38250.6015625, Validation MSE 426223.25\n",
      "Validation Loss: 37962.1875, Validation MSE 426170.46875\n",
      "Validation Loss: 37681.1015625, Validation MSE 426117.84375\n",
      "Validation Loss: 37407.0, Validation MSE 426065.46875\n",
      "Validation Loss: 37139.55859375, Validation MSE 426013.25\n",
      "Validation Loss: 36878.4765625, Validation MSE 425961.25\n",
      "Validation Loss: 36623.4609375, Validation MSE 425909.375\n",
      "Validation Loss: 36374.25390625, Validation MSE 425857.6875\n",
      "Validation Loss: 36130.609375, Validation MSE 425806.1875\n",
      "Epoch 70, Loss 19854.412109375\n",
      "Validation Loss: 35892.2890625, Validation MSE 425754.84375\n",
      "Validation Loss: 35659.06640625, Validation MSE 425703.59375\n",
      "Validation Loss: 35430.75, Validation MSE 425652.5\n",
      "Validation Loss: 35207.13671875, Validation MSE 425601.59375\n",
      "Validation Loss: 34988.046875, Validation MSE 425550.75\n",
      "Validation Loss: 34773.3046875, Validation MSE 425500.09375\n",
      "Validation Loss: 34562.75, Validation MSE 425449.46875\n",
      "Validation Loss: 34356.234375, Validation MSE 425399.0\n",
      "Validation Loss: 34153.6015625, Validation MSE 425348.6875\n",
      "Validation Loss: 33954.73046875, Validation MSE 425298.5\n",
      "Epoch 80, Loss 18654.419921875\n",
      "Validation Loss: 33759.46875, Validation MSE 425248.34375\n",
      "Validation Loss: 33567.71484375, Validation MSE 425198.375\n",
      "Validation Loss: 33379.34375, Validation MSE 425148.4375\n",
      "Validation Loss: 33194.24609375, Validation MSE 425098.625\n",
      "Validation Loss: 33012.3125, Validation MSE 425048.875\n",
      "Validation Loss: 32833.44921875, Validation MSE 424999.25\n",
      "Validation Loss: 32657.5546875, Validation MSE 424949.6875\n",
      "Validation Loss: 32484.552734375, Validation MSE 424900.21875\n",
      "Validation Loss: 32314.3359375, Validation MSE 424850.84375\n",
      "Validation Loss: 32146.83203125, Validation MSE 424801.5\n",
      "Epoch 90, Loss 17657.18359375\n",
      "Validation Loss: 31981.9609375, Validation MSE 424752.3125\n",
      "Validation Loss: 31819.65234375, Validation MSE 424703.1875\n",
      "Validation Loss: 31659.83203125, Validation MSE 424654.0\n",
      "Validation Loss: 31502.427734375, Validation MSE 424605.0\n",
      "Validation Loss: 31347.380859375, Validation MSE 424556.09375\n",
      "Validation Loss: 31194.623046875, Validation MSE 424507.25\n",
      "Validation Loss: 31044.083984375, Validation MSE 424458.375\n",
      "Validation Loss: 30895.724609375, Validation MSE 424409.6875\n",
      "Validation Loss: 30749.48046875, Validation MSE 424361.0625\n",
      "Validation Loss: 30605.294921875, Validation MSE 424312.4375\n",
      "Test Loss: 34206.1953125, Test MSE: 481830.5\n"
     ]
    }
   ],
   "source": [
    "#early stopping\n",
    "#original data\n",
    "#input_data = result\n",
    "\n",
    "#Glow output\n",
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/model/output\" # Shape (1,2539520 ) npy\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "input_data = np.concatenate([np.load(os.path.join(input_dir, f)) for f in input_files], axis=0)\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "train_x, test_x, train_y, test_y = train_test_split(input_data, output_data, test_size=0.2, random_state=10)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.25, random_state=10)\n",
    "\n",
    "train_x = torch.from_numpy(train_x).float().to(device)\n",
    "train_y = torch.from_numpy(train_y).float().to(device)\n",
    "valid_x = torch.from_numpy(valid_x).float().to(device)\n",
    "valid_y = torch.from_numpy(valid_y).float().to(device)\n",
    "test_x = torch.from_numpy(test_x).float().to(device)\n",
    "test_y = torch.from_numpy(test_y).float().to(device)\n",
    "\n",
    "# Define Gaussian Process Regression model\n",
    "class GaussianProcessRegression(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Initialize model and likelihood\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "model = GaussianProcessRegression(train_x, train_y, likelihood).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.train()\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "best_loss = float(\"inf\")\n",
    "patience = 10  # Stop training if validation loss does not improve for 10 epochs\n",
    "counter = 0\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss {loss.item()}\")\n",
    "\n",
    "    # Evaluate the model on the validation set and check for improvement\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(valid_x)\n",
    "        mse_valid = torch.nn.functional.mse_loss(output.mean, valid_y)\n",
    "        loss = -mll(output, valid_y)\n",
    "        print(f\"Validation Loss: {loss.item()}, Validation MSE {mse_valid}\")\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"No improvement in {patience} epochs, stopping early.\")\n",
    "            break\n",
    "    model.train()\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(test_x)\n",
    "    mse_test = torch.nn.functional.mse_loss(output.mean, test_y)\n",
    "    loss = -mll(output, test_y)\n",
    "    print(f\"Test Loss: {loss.item()}, Test MSE: {mse_test}\")\n",
    "\n",
    "    f_preds = model(test_x)\n",
    "    y_preds = likelihood(f_preds)\n",
    "    f_mean = f_preds.mean\n",
    "    f_var = f_preds.variance\n",
    "    f_covar = f_preds.covariance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694.1401155386425"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(481830.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 2539520)\n",
      "Epoch 0, Training Loss 109497.2421875, Validation Loss 193661.5625, Validation MSE 430641.3125\n",
      "Epoch 10, Training Loss 75780.28125, Validation Loss 146922.625, Validation MSE 429637.84375\n",
      "Epoch 20, Training Loss 55354.41015625, Validation Loss 114798.9375, Validation MSE 428712.25\n",
      "Epoch 30, Training Loss 44045.98828125, Validation Loss 95311.28125, Validation MSE 427888.4375\n",
      "Epoch 40, Training Loss 37472.64453125, Validation Loss 83253.71875, Validation MSE 427154.6875\n",
      "Epoch 50, Training Loss 33284.71484375, Validation Loss 75232.296875, Validation MSE 426487.21875\n",
      "Epoch 60, Training Loss 30366.12109375, Validation Loss 69465.8515625, Validation MSE 425864.8125\n",
      "Epoch 70, Training Loss 28177.5390625, Validation Loss 65041.12109375, Validation MSE 425272.3125\n",
      "Epoch 80, Training Loss 26444.099609375, Validation Loss 61474.54296875, Validation MSE 424700.0\n",
      "Epoch 90, Training Loss 25016.2734375, Validation Loss 58495.9765625, Validation MSE 424142.09375\n",
      "Test Loss: 61457.68359375, Test MSE: 481132.0\n"
     ]
    }
   ],
   "source": [
    "# GP using GLOW output, MSE\n",
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/model/output\" # Shape (1,2539520 ) npy\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "input_data = np.concatenate([np.load(os.path.join(input_dir, f)) for f in input_files], axis=0)\n",
    "print(input_data.shape)\n",
    "# input_dir = \"/dhc/home/youngbin.ko/glow_brain/data_gp/slices_64_new\"\n",
    "# input_files = sorted(os.listdir(input_dir))\n",
    "# result = []\n",
    "\n",
    "# for f in input_files:\n",
    "#   data = np.load(os.path.join(input_dir, f))\n",
    "#   result.append(data.reshape(1, -1))\n",
    "\n",
    "# input_data = np.concatenate(result, axis=0)\n",
    "\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "train_x, test_x, train_y, test_y = train_test_split(input_data, output_data, test_size=0.2, random_state=10)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.25, random_state=10)\n",
    "\n",
    "train_x = torch.from_numpy(train_x).float().to(device)\n",
    "train_y = torch.from_numpy(train_y).float().to(device)\n",
    "valid_x = torch.from_numpy(valid_x).float().to(device)\n",
    "valid_y = torch.from_numpy(valid_y).float().to(device)\n",
    "test_x = torch.from_numpy(test_x).float().to(device)\n",
    "test_y = torch.from_numpy(test_y).float().to(device)\n",
    "\n",
    "class GaussianProcessRegression(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y):\n",
    "        super().__init__(train_x, train_y, gpytorch.likelihoods.GaussianLikelihood())\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x, x_valid=None):\n",
    "        if x_valid is None:\n",
    "            mean_x = self.mean_module(x)\n",
    "            covar_x = self.covar_module(x)\n",
    "        else:\n",
    "            mean_x = self.mean_module(x_valid)\n",
    "            covar_x = self.covar_module(x_valid)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Initialize model\n",
    "model = GaussianProcessRegression(train_x, train_y).to(device)\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "#loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Define hyperparameters\n",
    "n_iterations = 10\n",
    "patience = 5\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    # loss = loss_fn(output.mean, train_y)\n",
    "    # train_loss = loss.item()\n",
    "    train_loss = -mll(output, train_y)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % n_iterations == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(valid_x, valid_x)\n",
    "            mse_valid = torch.nn.functional.mse_loss(output.mean, valid_y)\n",
    "            loss = -mll(output, valid_y)\n",
    "            valid_loss = loss.item()\n",
    "            \n",
    "            if valid_loss < best_loss:\n",
    "                best_loss = valid_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                \n",
    "            if counter >= patience:\n",
    "                print(f\"No improvement in {patience} epochs, stopping early.\")\n",
    "                break\n",
    "                \n",
    "        model.train()\n",
    "        print(f\"Epoch {epoch}, Training Loss {train_loss}, Validation Loss {valid_loss}, Validation MSE {mse_valid}\")\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(test_x, test_x)\n",
    "    mse_test = torch.nn.functional.mse_loss(output.mean, test_y)\n",
    "    loss = -mll(output, test_y)\n",
    "    print(f\"Test Loss: {loss.item()}, Test MSE: {mse_test}\")\n",
    "    y_preds = output.mean\n",
    "    y_var = output.variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "651.261924075099"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(424142.09375)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m ax\u001b[39m.\u001b[39mplot(train_x\u001b[39m.\u001b[39mcpu(), train_y\u001b[39m.\u001b[39mcpu(), \u001b[39m'\u001b[39m\u001b[39mk*\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Plot predictive means as blue line\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m ax\u001b[39m.\u001b[39mplot(test_x\u001b[39m.\u001b[39mcpu(), y_preds\u001b[39m.\u001b[39;49mmean\u001b[39m.\u001b[39;49mnumpy(), \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Shade between the lower and upper confidence bounds\u001b[39;00m\n\u001b[1;32m     12\u001b[0m ax\u001b[39m.\u001b[39mfill_between(test_x\u001b[39m.\u001b[39mcpu(), lower\u001b[39m.\u001b[39mnumpy(), upper\u001b[39m.\u001b[39mnumpy(), alpha\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = y_preds.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(train_x.cpu(), train_y.cpu(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.cpu(), y_preds.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.cpu(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glowenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
