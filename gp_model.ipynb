{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "main_folder_path = '/dhc/home/youngbin.ko/brain_data/data/train_gp_new_120'\n",
    "\n",
    "input_csv_path = '/dhc/home/youngbin.ko/brain_data/survival_info.csv'\n",
    "output_csv_path = '/dhc/home/youngbin.ko/brain_data/data/survival_gp.csv'\n",
    "\n",
    "with open(output_csv_path, 'w', newline='') as csv_output:\n",
    "    writer = csv.writer(csv_output)\n",
    "    \n",
    "    for subfolder_name in sorted(os.listdir(main_folder_path)):\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder_name)\n",
    "        \n",
    "        if os.path.isdir(subfolder_path):\n",
    "            with open(input_csv_path, 'r') as csv_input:\n",
    "                reader = csv.reader(csv_input)\n",
    "                \n",
    "                filtered_rows = filter(lambda row: row[0] == subfolder_name, reader)\n",
    "                sorted_rows = sorted(filtered_rows, key=lambda row: row[0])\n",
    "                \n",
    "                writer.writerows(sorted_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = np.load(\"/dhc/home/youngbin.ko/random.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "source_folder_path_1 = \"/dhc/home/youngbin.ko/brain_data/data/train_for_nf_344\"\n",
    "source_folder_path_2 = \"/dhc/home/youngbin.ko/brain_data/train_for_guassian_120\"\n",
    "target_path = \"/dhc/home/youngbin.ko/brain_data/data/train_nf\"\n",
    "\n",
    "for folder_name in os.listdir(source_folder_path_1):\n",
    "    src_path = os.path.join(source_folder_path_1, folder_name)\n",
    "    dst_path = os.path.join(target_path, folder_name)\n",
    "    shutil.copytree(src_path, dst_path)\n",
    "\n",
    "for folder_name in os.listdir(source_folder_path_2):\n",
    "    src_path = os.path.join(source_folder_path_2, folder_name)\n",
    "    dst_path = os.path.join(target_path, folder_name)\n",
    "    shutil.copytree(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder_path = \"/dhc/home/youngbin.ko/brain_data/data/train_nf\"\n",
    "target_path = \"/dhc/home/youngbin.ko/brain_data/data/train_gp_new_120\"\n",
    "\n",
    "folders_to_move = path\n",
    "\n",
    "for folder_name in os.listdir(parent_folder_path):\n",
    "    if folder_name in folders_to_move:\n",
    "        src_path = os.path.join(parent_folder_path, folder_name)\n",
    "        dst_path = os.path.join(target_path, folder_name)\n",
    "        shutil.move(src_path, dst_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/data_gp/slices_64_new\" # Shape (155, 64, 64, 4) npy\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "input_data = np.concatenate([np.expand_dims((np.load(os.path.join(input_dir, f))),0) for f in input_files], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_input = np.reshape(input_data, (120, 155*64*64*4))\n",
    "print(reshaped_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/model/output\" # Shape (1, 2539520) npy\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "input_data = np.concatenate([np.load(os.path.join(input_dir, f)) for f in input_files], axis=0)\n",
    "print(input_data.shape) #\n",
    "train_x = torch.from_numpy(input_data).float().to(device)\n",
    "\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "train_y = torch.from_numpy(output_data.flatten()).float().view(-1, 1).to(device)\n",
    "#train_y = torch.from_numpy(output_data).float().view(-1, 1)  # Reshape to (120, 1)\n",
    "print(train_y.shape)\n",
    "\n",
    "k=4\n",
    "input_folds = np.array_split(input_data, k)\n",
    "output_folds = np.array_split(output_data, k)\n",
    "\n",
    "class GaussianProcessModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()  # Use a constant mean function\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel()  # Use an RBF kernel\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "#gpytorch.likelihoods.BetaLikelihood\n",
    "model = GaussianProcessModel(train_x, train_y, likelihood)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(train_x)\n",
    "    loss = -likelihood(output).log_prob(train_y).mean()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/model/output\" # Shape (1,2539520 ) npy\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "input_data = np.concatenate([np.load(os.path.join(input_dir, f)) for f in input_files], axis=0)\n",
    "print(input_data.shape) #(120,2539520)\n",
    "train_x = torch.from_numpy(input_data).float().to(device)\n",
    "\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "train_y = torch.from_numpy(output_data.flatten()).float().view(-1, 1).to(device)  # Reshape to (120, 1)\n",
    "print(train_y.shape) #(120, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcessModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()  # Use a constant mean function\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel()  # Use an RBF kernel\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "num_epochs = 100\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kf.split(train_x)):\n",
    "    # Get train and validation data\n",
    "    train_x_fold = train_x[train_index]\n",
    "    train_y_fold = train_y[train_index]\n",
    "    valid_x_fold = train_x[valid_index]\n",
    "    valid_y_fold = train_y[valid_index]\n",
    "\n",
    "    # Initialize model and likelihood\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GaussianProcessModel(train_x_fold, train_y_fold, likelihood)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.LBFGS(model.parameters(), line_search_fn='strong_wolfe')\n",
    "\n",
    "    # Train the model\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x_fold)\n",
    "        loss = -likelihood(output).log_prob(train_y_fold).mean()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.step(closure)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Fold {fold}, Epoch {epoch}, Loss {closure().item()}\")\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(valid_x_fold)\n",
    "        loss = -likelihood(output).log_prob(valid_y_fold).mean()\n",
    "        print(f\"Fold {fold}, Validation Loss: {loss.item()}\")\n",
    "\n",
    "        f_preds = model(valid_x_fold)\n",
    "        y_preds = likelihood(model(valid_x_fold))\n",
    "        f_mean = f_preds.mean\n",
    "        f_var = f_preds.variance\n",
    "        f_covar = f_preds.covariance_matrix\n",
    "        f_samples = f_preds.sample(sample_shape=torch.Size(1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/model/output\" # Shape (1,2539520 ) npy\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "input_data = np.concatenate([np.load(os.path.join(input_dir, f)) for f in input_files], axis=0)\n",
    "print(input_data.shape) #(120,2539520)\n",
    "train_x = torch.from_numpy(input_data).float().to(device)\n",
    "\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "train_y = torch.from_numpy(output_data).float().view(-1, 1).to(device)  # Reshape to (120, 1)\n",
    "print(train_y.shape) #(120, 1)\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "num_epochs = 100\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kf.split(train_x)):\n",
    "    # Get train and validation data\n",
    "    train_x_fold = train_x[train_index]\n",
    "    train_y_fold = train_y[train_index]\n",
    "    valid_x_fold = train_x[valid_index]\n",
    "    valid_y_fold = train_y[valid_index]\n",
    "\n",
    "    # Initialize model and likelihood\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GaussianProcessModel(train_x_fold, train_y_fold, likelihood)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.LBFGS(model.parameters(), line_search_fn='strong_wolfe')\n",
    "\n",
    "    # Train the model\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x_fold)\n",
    "        loss = -likelihood(output).log_prob(train_y_fold).mean()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.step(closure)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Fold {fold}, Epoch {epoch}, Loss {closure().item()}\")\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(valid_x_fold)\n",
    "        loss = -likelihood(output).log_prob(valid_y_fold).mean()\n",
    "        print(f\"Fold {fold}, Validation Loss: {loss.item()}\")\n",
    "\n",
    "        f_preds = model(valid_x_fold)\n",
    "        y_preds = likelihood(model(valid_x_fold))\n",
    "        f_mean = f_preds.mean\n",
    "        f_var = f_preds.variance\n",
    "        f_covar = f_preds.covariance_matrix\n",
    "        f_samples = f_preds.sample(sample_shape=torch.Size(1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcessModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()  # Use a constant mean function\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel()  # Use an RBF kernel\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        #mean_x = mean_x.view(-1, 1)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "(120, 2539520)\n",
      "torch.Size([120])\n",
      "Fold 0, Prior mean: 0.0\n",
      "Fold 0, Training labels shape: torch.Size([90])\n",
      "Fold 0, Epoch 0, Loss 144536.71875\n",
      "Fold 0, Epoch 10, Loss 77837.0859375\n",
      "Fold 0, Epoch 20, Loss 52163.8359375\n",
      "Fold 0, Epoch 30, Loss 40709.44140625\n",
      "Fold 0, Epoch 40, Loss 34614.015625\n",
      "Fold 0, Epoch 50, Loss 30844.60546875\n",
      "Fold 0, Epoch 60, Loss 28225.6640625\n",
      "Fold 0, Epoch 70, Loss 26246.900390625\n",
      "Fold 0, Epoch 80, Loss 24664.056640625\n",
      "Fold 0, Epoch 90, Loss 23348.8203125\n",
      "Fold 0, Validation Loss: 556400.1875\n",
      "Fold 1, Prior mean: 0.0\n",
      "Fold 1, Training labels shape: torch.Size([90])\n",
      "Fold 1, Epoch 0, Loss 121153.8671875\n",
      "Fold 1, Epoch 10, Loss 65233.2578125\n",
      "Fold 1, Epoch 20, Loss 43710.99609375\n",
      "Fold 1, Epoch 30, Loss 34108.875\n",
      "Fold 1, Epoch 40, Loss 28999.029296875\n",
      "Fold 1, Epoch 50, Loss 25838.91796875\n",
      "Fold 1, Epoch 60, Loss 23643.162109375\n",
      "Fold 1, Epoch 70, Loss 21984.01953125\n",
      "Fold 1, Epoch 80, Loss 20656.763671875\n",
      "Fold 1, Epoch 90, Loss 19553.8359375\n",
      "Fold 1, Validation Loss: 957625.4375\n",
      "Fold 2, Prior mean: 0.0\n",
      "Fold 2, Training labels shape: torch.Size([90])\n",
      "Fold 2, Epoch 0, Loss 123711.6796875\n",
      "Fold 2, Epoch 10, Loss 66609.6875\n",
      "Fold 2, Epoch 20, Loss 44632.875\n",
      "Fold 2, Epoch 30, Loss 34827.96875\n",
      "Fold 2, Epoch 40, Loss 29610.1953125\n",
      "Fold 2, Epoch 50, Loss 26383.322265625\n",
      "Fold 2, Epoch 60, Loss 24141.16796875\n",
      "Fold 2, Epoch 70, Loss 22446.95703125\n",
      "Fold 2, Epoch 80, Loss 21091.625\n",
      "Fold 2, Epoch 90, Loss 19965.369140625\n",
      "Fold 2, Validation Loss: 940253.125\n",
      "Fold 3, Prior mean: 0.0\n",
      "Fold 3, Training labels shape: torch.Size([90])\n",
      "Fold 3, Epoch 0, Loss 138640.859375\n",
      "Fold 3, Epoch 10, Loss 74658.7265625\n",
      "Fold 3, Epoch 20, Loss 50032.05078125\n",
      "Fold 3, Epoch 30, Loss 39044.66015625\n",
      "Fold 3, Epoch 40, Loss 33197.73828125\n",
      "Fold 3, Epoch 50, Loss 29581.927734375\n",
      "Fold 3, Epoch 60, Loss 27069.689453125\n",
      "Fold 3, Epoch 70, Loss 25171.4921875\n",
      "Fold 3, Epoch 80, Loss 23653.072265625\n",
      "Fold 3, Epoch 90, Loss 22391.345703125\n",
      "Fold 3, Validation Loss: 666748.8125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "# check if GPU is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "#device = torch.device('cpu')\n",
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/model/output\" # Shape (1,2539520 ) npy\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "input_data = np.concatenate([np.load(os.path.join(input_dir, f)) for f in input_files], axis=0)\n",
    "print(input_data.shape) #(120,2539520)\n",
    "train_x = torch.from_numpy(input_data).float().to(device)\n",
    "#train_x = torch.from_numpy(input_data).float()\n",
    "\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "train_y = torch.from_numpy(output_data.flatten()).float().to(device)\n",
    "#train_y = torch.from_numpy(output_data.flatten()).float().view(1, -1).to(device)  # Reshape to (120, 1)\n",
    "#train_y = torch.from_numpy(output_data.flatten()).float().view(-1, 1) \n",
    "print(train_y.shape) #(120, 1)\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "num_epochs = 100\n",
    "\n",
    "class GaussianProcessModel(gpytorch.models.ExactGP): #change to linear regression (output from NF) \n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()  # Use a constant mean function\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel()  # Use an RBF kernel\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kf.split(train_x)):\n",
    "    # Get train and validation data\n",
    "    train_x_fold = train_x[train_index]\n",
    "    train_y_fold = train_y[train_index]\n",
    "    valid_x_fold = train_x[valid_index]\n",
    "    valid_y_fold = train_y[valid_index]\n",
    "\n",
    "    # Initialize model and likelihood\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "    model = GaussianProcessModel(train_x_fold, train_y_fold, likelihood).to(device)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "    print(f\"Fold {fold}, Prior mean: {model.mean_module.constant.item()}\")\n",
    "    print(f\"Fold {fold}, Training labels shape: {train_y_fold.shape}\")\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x_fold)\n",
    "        #loss = -likelihood(output).log_prob(train_y_fold).mean()\n",
    "        #loss = ((train_y_fold-output)**2)\n",
    "        loss = -mll(output, train_y_fold)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Fold {fold}, Epoch {epoch}, Loss {loss.item()}\")\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(valid_x_fold)\n",
    "        loss = -likelihood(output).log_prob(valid_y_fold).mean()\n",
    "        print(f\"Fold {fold}, Validation Loss: {loss.item()}\")\n",
    "\n",
    "        f_preds = model(valid_x_fold)\n",
    "        y_preds = likelihood(model(valid_x_fold))\n",
    "        f_mean = f_preds.mean\n",
    "        f_var = f_preds.variance\n",
    "        f_covar = f_preds.covariance_matrix\n",
    "        #f_samples = f_preds.sample(sample_shape=torch.Size(1000,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258,\n",
       "        6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258,\n",
       "        6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258,\n",
       "        6.3258, 6.3258, 6.3258])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds.mean.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpytorch.distributions.multivariate_normal.MultivariateNormal"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258,\n",
      "        6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258,\n",
      "        6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258, 6.3258,\n",
      "        6.3258, 6.3258, 6.3258], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 788.,  465.,  336., 1283.,   33.,  229., 1592.,  614.,   58.,  416.,\n",
      "         240.,  300.,  630.,  630.,  355.,  737.,   67.,  828.,  747.,  103.,\n",
      "         382.,  372.,  232.,  434.,  244.,   82.,  147.,  260.,  184.,   62.],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "(120, 2539520)\n",
      "Training labels shape: torch.Size([96])\n",
      "Epoch 0, Loss 390123.46875\n",
      "Epoch 10, Loss 178450512.0\n",
      "Epoch 20, Loss 40174408.0\n",
      "Epoch 30, Loss 386331.84375\n",
      "Epoch 40, Loss 1737234.5\n",
      "Epoch 50, Loss 1351676.5\n",
      "Epoch 60, Loss 498259.5\n",
      "Epoch 70, Loss 175103.0625\n",
      "Epoch 80, Loss 184468.890625\n",
      "Epoch 90, Loss 208995.421875\n",
      "tensor([[494.8763],\n",
      "        [505.1411],\n",
      "        [487.5457],\n",
      "        [493.6921],\n",
      "        [523.8223],\n",
      "        [503.0511],\n",
      "        [510.7884],\n",
      "        [483.9029],\n",
      "        [484.8311],\n",
      "        [495.2319],\n",
      "        [497.0187],\n",
      "        [468.2321],\n",
      "        [480.0355],\n",
      "        [503.2893],\n",
      "        [490.9646],\n",
      "        [531.8245],\n",
      "        [467.5026],\n",
      "        [526.8805],\n",
      "        [509.3709],\n",
      "        [507.7156],\n",
      "        [502.5511],\n",
      "        [510.7158],\n",
      "        [495.3963],\n",
      "        [479.2571]], device='cuda:0')\n",
      "Validation Loss: 116807.40625\n"
     ]
    }
   ],
   "source": [
    "#linear regression with output from NF\n",
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/model/output\" # Shape (1,2539520 ) npy\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "input_data = np.concatenate([np.load(os.path.join(input_dir, f)) for f in input_files], axis=0)\n",
    "print(input_data.shape) #(120,2539520)\n",
    "\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "input_data = torch.from_numpy(input_data).float().to(device)\n",
    "output_data = torch.from_numpy(output_data.flatten()).float().to(device)\n",
    "\n",
    "train_size = int(0.8 * len(input_data))\n",
    "train_x, valid_x = input_data[:train_size], input_data[train_size:]\n",
    "train_y, valid_y = output_data[:train_size], output_data[train_size:]\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "class LinearRegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_data.shape[1], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "model = LinearRegressionModel().to(device)\n",
    "criterion = torch.nn.MSELoss(reduction = 'mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "print(f\"Training labels shape: {train_y.shape}\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = criterion(output, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss {loss.item()}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(valid_x)\n",
    "    print(output)\n",
    "    # loss = criterion(output, valid_y)\n",
    "    # print(f\"Validation Loss: {loss.item()}\")\n",
    "\n",
    "    y_preds = output.flatten()\n",
    "    loss = criterion(y_preds, valid_y)\n",
    "    # print(f\"Validation Loss: {loss.item()}\")\n",
    "    print(f\"Validation Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 448.,  296.,  626.,  372.,  121.,   30.,  232.,  434.,    5.,  244.,\n",
      "          82.,  346.,  147., 1076.,  903., 1293.,  260.,  351.,  880.,  184.,\n",
      "         197.,  456.,  540.,   62.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([496.7550, 500.4889, 487.6410, 496.5576, 526.0389, 501.9958, 511.0885,\n",
      "        482.8078, 484.6608, 497.4300, 500.1317, 466.7158, 479.1836, 503.2531,\n",
      "        486.0622, 533.7513, 468.9354, 526.1563, 511.6059, 505.7967, 505.9279,\n",
      "        513.1158, 494.8851, 480.3745], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(467.5026, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(min(y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(531.8245, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(max(y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[494.0627],\n",
      "        [501.9978],\n",
      "        [486.6934],\n",
      "        [492.3466],\n",
      "        [523.3304],\n",
      "        [505.7557],\n",
      "        [512.6681],\n",
      "        [484.4684],\n",
      "        [480.3996],\n",
      "        [496.4120],\n",
      "        [500.8266],\n",
      "        [466.0086],\n",
      "        [477.9927],\n",
      "        [504.4886],\n",
      "        [488.8863],\n",
      "        [534.4900],\n",
      "        [470.8798],\n",
      "        [526.5191],\n",
      "        [513.6910],\n",
      "        [507.5645],\n",
      "        [505.8149],\n",
      "        [512.7242],\n",
      "        [493.8711],\n",
      "        [480.9348]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.sum((y_preds - valid_y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(69.8177, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print((test**0.5)/24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(346.6943, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(((torch.sum((output - valid_y)**2))**0.5)/24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 155, 64, 64, 4])\n",
      "torch.Size([72])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(train_loader)\n",
    "batch_X, batch_y = data_iter.next()\n",
    "\n",
    "print(batch_X.shape) # should output (155, 120, 64, 64)\n",
    "print(batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.22 GiB (GPU 0; 10.76 GiB total capacity; 6.19 GiB already allocated; 3.54 GiB free; 6.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n\u001b[1;32m     44\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m---> 47\u001b[0m model \u001b[39m=\u001b[39m MyCNN()\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     48\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m     49\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters())\n",
      "File \u001b[0;32m~/conda3/envs/copy_env/lib/python3.10/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/conda3/envs/copy_env/lib/python3.10/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/conda3/envs/copy_env/lib/python3.10/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/conda3/envs/copy_env/lib/python3.10/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.22 GiB (GPU 0; 10.76 GiB total capacity; 6.19 GiB already allocated; 3.54 GiB free; 6.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# cnn with original data\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/data_gp/slices_64_new\"\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "X = []\n",
    "for f in input_files:\n",
    "    data = np.load(os.path.join(input_dir, f))  # Shape (155, 64, 64, 4)\n",
    "    X.append(data)\n",
    "X = np.stack(X, axis=0)  # Shape (120, 155, 64, 64, 4)\n",
    "\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, output_data, test_size=0.2, random_state=10)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=0.25, random_state=10)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(train_X).float(), torch.from_numpy(train_y).float()), batch_size=155, shuffle=True)\n",
    "valid_loader = DataLoader(TensorDataset(torch.from_numpy(valid_X).float(), torch.from_numpy(valid_y).float()), batch_size=155)\n",
    "test_loader = DataLoader(TensorDataset(torch.from_numpy(test_X).float(), torch.from_numpy(test_y).float()), batch_size=155)\n",
    "\n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(155, 16, kernel_size=3, padding=(1, 1, 0)) # Add padding\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv3d(16, 32, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(32 * 15 * 15 * 38 * 155, 128) # Update the input size\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 15 * 15 * 38 * 155)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MyCNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "patience = 10  # Stop training if validation loss does not improve for 10 epochs\n",
    "counter = 0\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = criterion(outputs, targets.unsqueeze(1).to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Evaluate the model on the validation set and check for improvement\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in valid_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        valid_loss /= len(valid_loader.dataset)\n",
    "        print(f\"Validation Loss: {valid_loss}\")\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"No improvement in {patience} epochs, stopping early.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dhc/home/youngbin.ko/conda3/envs/copy_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/dhc/home/youngbin.ko/conda3/envs/copy_env/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/dhc/home/youngbin.ko/conda3/envs/copy_env/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/dhc/home/youngbin.ko/conda3/envs/copy_env/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 191366.3442, Val Loss: 144019.0052\n",
      "Epoch [2/50], Train Loss: 134887.0339, Val Loss: 151883.8652\n",
      "Epoch [3/50], Train Loss: 109609.8215, Val Loss: 139236.1113\n",
      "Epoch [4/50], Train Loss: 127884.9961, Val Loss: 174003.9613\n",
      "Epoch [5/50], Train Loss: 117897.9119, Val Loss: 141428.8584\n",
      "Epoch [6/50], Train Loss: 112901.4451, Val Loss: 162786.6169\n",
      "Epoch [7/50], Train Loss: 122966.2534, Val Loss: 139710.2347\n",
      "Epoch [8/50], Train Loss: 117669.6528, Val Loss: 161341.1618\n",
      "Epoch [9/50], Train Loss: 125232.2095, Val Loss: 139087.8499\n",
      "Epoch [10/50], Train Loss: 121653.4888, Val Loss: 150815.9730\n",
      "Epoch [11/50], Train Loss: 115747.3616, Val Loss: 140362.4049\n",
      "Epoch [12/50], Train Loss: 120952.1147, Val Loss: 145027.9668\n",
      "Epoch [13/50], Train Loss: 116788.0151, Val Loss: 148000.3836\n",
      "Epoch [14/50], Train Loss: 117355.7483, Val Loss: 140147.1878\n",
      "Epoch [15/50], Train Loss: 118796.7305, Val Loss: 165730.6240\n",
      "Epoch [16/50], Train Loss: 123238.6455, Val Loss: 147535.7002\n",
      "Epoch [17/50], Train Loss: 112545.8850, Val Loss: 139414.4079\n",
      "Epoch [18/50], Train Loss: 117662.1909, Val Loss: 149892.1234\n",
      "Epoch [19/50], Train Loss: 118368.6943, Val Loss: 148474.2842\n",
      "Epoch [20/50], Train Loss: 153236.9297, Val Loss: 139496.8678\n",
      "Epoch [21/50], Train Loss: 116707.9535, Val Loss: 152755.0661\n",
      "Epoch [22/50], Train Loss: 115697.5122, Val Loss: 140505.9362\n",
      "Epoch [23/50], Train Loss: 122951.4790, Val Loss: 180337.2240\n",
      "Epoch [24/50], Train Loss: 221378.1289, Val Loss: 139971.8841\n",
      "Epoch [25/50], Train Loss: 222183.4121, Val Loss: 181492.5651\n",
      "Epoch [26/50], Train Loss: 128480.0391, Val Loss: 142095.2305\n",
      "Epoch [27/50], Train Loss: 127210.6021, Val Loss: 164837.9834\n",
      "Epoch [28/50], Train Loss: 119662.5645, Val Loss: 140356.7503\n",
      "Epoch [29/50], Train Loss: 114299.4471, Val Loss: 148898.7977\n",
      "Epoch [30/50], Train Loss: 118692.5925, Val Loss: 144608.5732\n",
      "Epoch [31/50], Train Loss: 116358.1909, Val Loss: 152544.7500\n",
      "Epoch [32/50], Train Loss: 129290.2080, Val Loss: 150171.3132\n",
      "Epoch [33/50], Train Loss: 128978.0586, Val Loss: 157856.9108\n",
      "Epoch [34/50], Train Loss: 125913.8340, Val Loss: 147418.2210\n",
      "Epoch [35/50], Train Loss: 109769.2402, Val Loss: 139219.8711\n",
      "Epoch [36/50], Train Loss: 114198.8188, Val Loss: 158995.3027\n",
      "Epoch [37/50], Train Loss: 156377.4814, Val Loss: 138971.0667\n",
      "Epoch [38/50], Train Loss: 182653.3267, Val Loss: 159013.8089\n",
      "Epoch [39/50], Train Loss: 128090.9502, Val Loss: 139705.7786\n",
      "Epoch [40/50], Train Loss: 117822.7116, Val Loss: 147869.8060\n",
      "Epoch [41/50], Train Loss: 124014.7844, Val Loss: 157558.3242\n",
      "Epoch [42/50], Train Loss: 127893.0430, Val Loss: 139210.9085\n",
      "Epoch [43/50], Train Loss: 161450.8015, Val Loss: 158789.7054\n",
      "Epoch [44/50], Train Loss: 122575.3267, Val Loss: 138986.1387\n",
      "Epoch [45/50], Train Loss: 119604.9810, Val Loss: 163027.1211\n",
      "Epoch [46/50], Train Loss: 182923.2876, Val Loss: 140265.9808\n",
      "Epoch [47/50], Train Loss: 206860.6450, Val Loss: 138981.9023\n",
      "Epoch [48/50], Train Loss: 122646.0693, Val Loss: 153055.1012\n",
      "Epoch [49/50], Train Loss: 115233.6348, Val Loss: 147678.5791\n",
      "Epoch [50/50], Train Loss: 122123.3320, Val Loss: 146252.5579\n",
      "Test Loss: 195694.2240\n"
     ]
    }
   ],
   "source": [
    "# cnn with original data\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/data_gp/slices_64_new\"\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "X = []\n",
    "for f in input_files:\n",
    "    data = np.load(os.path.join(input_dir, f))  # Shape (155, 64, 64, 4)\n",
    "    X.append(data)\n",
    "X = np.stack(X, axis=0)  # Shape (120, 155, 64, 64, 4)\n",
    "X = np.transpose(X, (0, 4, 2, 3, 1))\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, output_data, test_size=0.2, random_state=10)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=0.25, random_state=10)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(train_X).float(), torch.from_numpy(train_y).float()), batch_size=10, shuffle=True) # shape (72, 155, 64, 64, 4)\n",
    "valid_loader = DataLoader(TensorDataset(torch.from_numpy(valid_X).float(), torch.from_numpy(valid_y).float()), batch_size=10)\n",
    "test_loader = DataLoader(TensorDataset(torch.from_numpy(test_X).float(), torch.from_numpy(test_y).float()), batch_size=10)\n",
    "\n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(4, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16 * 19, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 16 * 16 * 19)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = criterion(outputs, targets.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "model = MyCNN().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_loss = evaluate(model, valid_loader, criterion)\n",
    "    print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}'.format(epoch+1, num_epochs, train_loss, val_loss))\n",
    "\n",
    "test_loss = evaluate(model, test_loader, criterion)\n",
    "print('Test Loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling layer 'cnn_7' (type CNN).\n\n'torch.Size' object has no attribute 'rank'\n\nCall arguments received by layer 'cnn_7' (type CNN):\n  • inputs=tensor([[[[[-0.4976, -0.4994, -0.4994, -0.4962],\n           [-0.4993, -0.4994, -0.4965, -0.4983],\n           [-0.4992, -0.4986, -0.4972, -0.5000],\n           ...,\n           [-0.4983, -0.4969, -0.4986, -0.4962],\n           [-0.4977, -0.4991, -0.4973, -0.4964],\n           [-0.4991, -0.4979, -0.4993, -0.4977]],\n\n          [[-0.4987, -0.4995, -0.4986, -0.4986],\n           [-0.4982, -0.4987, -0.4991, -0.4973],\n           [-0.4967, -0.4962, -0.4962, -0.4966],\n           ...,\n           [-0.4991, -0.4996, -0.4978, -0.4990],\n           [-0.4985, -0.4998, -0.4968, -0.4984],\n           [-0.4976, -0.4997, -0.4989, -0.4978]],\n\n          [[-0.4983, -0.4987, -0.4984, -0.4997],\n           [-0.4990, -0.4962, -0.4994, -0.4986],\n           [-0.4965, -0.4979, -0.4978, -0.4965],\n           ...,\n           [-0.4962, -0.4990, -0.4991, -0.4963],\n           [-0.4963, -0.4985, -0.4983, -0.4979],\n           [-0.4961, -0.4987, -0.4991, -0.4991]],\n\n          ...,\n\n          [[-0.4964, -0.4983, -0.4965, -0.4967],\n           [-0.4976, -0.4964, -0.4963, -0.4981],\n           [-0.4997, -0.4980, -0.4972, -0.4982],\n           ...,\n           [-0.4977, -0.4987, -0.4982, -0.4987],\n           [-0.4961, -0.4977, -0.4973, -0.4987],\n           [-0.4983, -0.5000, -0.4999, -0.4990]],\n\n          [[-0.4993, -0.4980, -0.4984, -0.4980],\n           [-0.4984, -0.4985, -0.4986, -0.4970],\n           [-0.4976, -0.4966, -0.4964, -0.4995],\n           ...,\n           [-0.4993, -0.4995, -0.4999, -0.4998],\n           [-0.4964, -0.4981, -0.4974, -0.4962],\n           [-0.4994, -0.4977, -0.4989, -0.4979]],\n\n          [[-0.4967, -0.4985, -0.4991, -0.4996],\n           [-0.4973, -0.4992, -0.4972, -0.4992],\n           [-0.4978, -0.4967, -0.4964, -0.4993],\n           ...,\n           [-0.4990, -0.4963, -0.4990, -0.4979],\n           [-0.4973, -0.4994, -0.4965, -0.4999],\n           [-0.4987, -0.4969, -0.4963, -0.4968]]],\n\n\n         [[[-0.4974, -0.4991, -0.4971, -0.4972],\n           [-0.4997, -0.5000, -0.4974, -0.4999],\n           [-0.4962, -0.4982, -0.4962, -0.4969],\n           ...,\n           [-0.4966, -0.4988, -0.4996, -0.4976],\n           [-0.4976, -0.4984, -0.4969, -0.4987],\n           [-0.4984, -0.4990, -0.4981, -0.4979]],\n\n          [[-0.4987, -0.4970, -0.4983, -0.4991],\n           [-0.4976, -0.4993, -0.4984, -0.4973],\n           [-0.4995, -0.4982, -0.4986, -0.4966],\n           ...,\n           [-0.4969, -0.4994, -0.4962, -0.4988],\n           [-0.4966, -0.4981, -0.4994, -0.4972],\n           [-0.4962, -0.4980, -0.4971, -0.4964]],\n\n          [[-0.4981, -0.4970, -0.4969, -0.4987],\n           [-0.4986, -0.4996, -0.4976, -0.4963],\n           [-0.4984, -0.4982, -0.4971, -0.4980],\n           ...,\n           [-0.4992, -0.4968, -0.4962, -0.4981],\n           [-0.4980, -0.4981, -0.4982, -0.4999],\n           [-0.4989, -0.4983, -0.4996, -0.4999]],\n\n          ...,\n\n          [[-0.4961, -0.4986, -0.4994, -0.4968],\n           [-0.4987, -0.4988, -0.5000, -0.4990],\n           [-0.4968, -0.4988, -0.4976, -0.4965],\n           ...,\n           [-0.4997, -0.4979, -0.4978, -0.4982],\n           [-0.4987, -0.4991, -0.4966, -0.4981],\n           [-0.4972, -0.4980, -0.4966, -0.4992]],\n\n          [[-0.4996, -0.4964, -0.4983, -0.4974],\n           [-0.4987, -0.4978, -0.4963, -0.4999],\n           [-0.4977, -0.4984, -0.4972, -0.4999],\n           ...,\n           [-0.4968, -0.4962, -0.4977, -0.4977],\n           [-0.4980, -0.4995, -0.4994, -0.4982],\n           [-0.4981, -0.4973, -0.4997, -0.4976]],\n\n          [[-0.4988, -0.4971, -0.4999, -0.4976],\n           [-0.4982, -0.4984, -0.4973, -0.4979],\n           [-0.4991, -0.4988, -0.4974, -0.4981],\n           ...,\n           [-0.4963, -0.4999, -0.4986, -0.4997],\n           [-0.4994, -0.4970, -0.4975, -0.4984],\n           [-0.4995, -0.4966, -0.4991, -0.4967]]],\n\n\n         [[[-0.4996, -0.4999, -0.4985, -0.4992],\n           [-0.4988, -0.5000, -0.4993, -0.4994],\n           [-0.4972, -0.4996, -0.4997, -0.4967],\n           ...,\n           [-0.4980, -0.4981, -0.4971, -0.4967],\n           [-0.4968, -0.4962, -0.4973, -0.4984],\n           [-0.4980, -0.4998, -0.4996, -0.4993]],\n\n          [[-0.4991, -0.4964, -0.4998, -0.4987],\n           [-0.4991, -0.4995, -0.4998, -0.4964],\n           [-0.4980, -0.4975, -0.4979, -0.4999],\n           ...,\n           [-0.4985, -0.4986, -0.4973, -0.4991],\n           [-0.4999, -0.4962, -0.4964, -0.4973],\n           [-0.4984, -0.4967, -0.4982, -0.4979]],\n\n          [[-0.4991, -0.4981, -0.4963, -0.4986],\n           [-0.4984, -0.4989, -0.4972, -0.4964],\n           [-0.4995, -0.4996, -0.4993, -0.4995],\n           ...,\n           [-0.4968, -0.4982, -0.4967, -0.4981],\n           [-0.4992, -0.4983, -0.4983, -0.4976],\n           [-0.4980, -0.4969, -0.4983, -0.4982]],\n\n          ...,\n\n          [[-0.4987, -0.4969, -0.4979, -0.4969],\n           [-0.4984, -0.4974, -0.4980, -0.4992],\n           [-0.4966, -0.4986, -0.4962, -0.4996],\n           ...,\n           [-0.4994, -0.4978, -0.4973, -0.4989],\n           [-0.4990, -0.4980, -0.4999, -0.4973],\n           [-0.4992, -0.4962, -0.4989, -0.4962]],\n\n          [[-0.4978, -0.4987, -0.4967, -0.4967],\n           [-0.4979, -0.4982, -0.4967, -0.4982],\n           [-0.4964, -0.4973, -0.4992, -0.4970],\n           ...,\n           [-0.4973, -0.4964, -0.4963, -0.4987],\n           [-0.4983, -0.4984, -0.4969, -0.4992],\n           [-0.4999, -0.4986, -0.4995, -0.4990]],\n\n          [[-0.4976, -0.4997, -0.4965, -0.4975],\n           [-0.4975, -0.4982, -0.4968, -0.4988],\n           [-0.4999, -0.4992, -0.4986, -0.4972],\n           ...,\n           [-0.4990, -0.4980, -0.4971, -0.4994],\n           [-0.4985, -0.4982, -0.4968, -0.4989],\n           [-0.4980, -0.4962, -0.4980, -0.4968]]],\n\n\n         ...,\n\n\n         [[[-0.4972, -0.4963, -0.4986, -0.4967],\n           [-0.4985, -0.4966, -0.4974, -0.4964],\n           [-0.4983, -0.4988, -0.4968, -0.4986],\n           ...,\n           [-0.4963, -0.4998, -0.4972, -0.4969],\n           [-0.4980, -0.4967, -0.4975, -0.4977],\n           [-0.4995, -0.4979, -0.4984, -0.4986]],\n\n          [[-0.4970, -0.4979, -0.4982, -0.4988],\n           [-0.4982, -0.4969, -0.4972, -0.4961],\n           [-0.4965, -0.4987, -0.4962, -0.4981],\n           ...,\n           [-0.4986, -0.4996, -0.4981, -0.4982],\n           [-0.4989, -0.4998, -0.4998, -0.4982],\n           [-0.4970, -0.4972, -0.4965, -0.4996]],\n\n          [[-0.4963, -0.4970, -0.4965, -0.4993],\n           [-0.4968, -0.4991, -0.4989, -0.4987],\n           [-0.4987, -0.4973, -0.4975, -0.4996],\n           ...,\n           [-0.4978, -0.4996, -0.4996, -0.4980],\n           [-0.4966, -0.4973, -0.4990, -0.4976],\n           [-0.5000, -0.4993, -0.4988, -0.4978]],\n\n          ...,\n\n          [[-0.4980, -0.4964, -0.4983, -0.4999],\n           [-0.4993, -0.4994, -0.4991, -0.4975],\n           [-0.4969, -0.4991, -0.4991, -0.4968],\n           ...,\n           [-0.4970, -0.4968, -0.4978, -0.4997],\n           [-0.4961, -0.4973, -0.4995, -0.4999],\n           [-0.4984, -0.4983, -0.4969, -0.4991]],\n\n          [[-0.4984, -0.4965, -0.4996, -0.4985],\n           [-0.4964, -0.4986, -0.4967, -0.4971],\n           [-0.5000, -0.4961, -0.4971, -0.4992],\n           ...,\n           [-0.4983, -0.4966, -0.4984, -0.4963],\n           [-0.4986, -0.4997, -0.4988, -0.4969],\n           [-0.4998, -0.4998, -0.4975, -0.4971]],\n\n          [[-0.4978, -0.4981, -0.4985, -0.4981],\n           [-0.4966, -0.4967, -0.4984, -0.4981],\n           [-0.4982, -0.4995, -0.4997, -0.4989],\n           ...,\n           [-0.4998, -0.4979, -0.4983, -0.4973],\n           [-0.4968, -0.4973, -0.4986, -0.4964],\n           [-0.4964, -0.4969, -0.4986, -0.5000]]],\n\n\n         [[[-0.4990, -0.4987, -0.4986, -0.4967],\n           [-0.4974, -0.4971, -0.4981, -0.4971],\n           [-0.4987, -0.4976, -0.4977, -0.4994],\n           ...,\n           [-0.4980, -0.5000, -0.4979, -0.4964],\n           [-0.4992, -0.4988, -0.4976, -0.4965],\n           [-0.4989, -0.4998, -0.4986, -0.4995]],\n\n          [[-0.4976, -0.4976, -0.4987, -0.4961],\n           [-0.4962, -0.4965, -0.4977, -0.4980],\n           [-0.4987, -0.4987, -0.4970, -0.4993],\n           ...,\n           [-0.4972, -0.4981, -0.4973, -0.4970],\n           [-0.4979, -0.4970, -0.4972, -0.4975],\n           [-0.4990, -0.4975, -0.4973, -0.4987]],\n\n          [[-0.4963, -0.4966, -0.4974, -0.4979],\n           [-0.4984, -0.4982, -0.4978, -0.4987],\n           [-0.4993, -0.4979, -0.4963, -0.4997],\n           ...,\n           [-0.4999, -0.4992, -0.4990, -0.4965],\n           [-0.4976, -0.4981, -0.4980, -0.4989],\n           [-0.4969, -0.4995, -0.4998, -0.4964]],\n\n          ...,\n\n          [[-0.4984, -0.4981, -0.4981, -0.4999],\n           [-0.4966, -0.4963, -0.4997, -0.4998],\n           [-0.4989, -0.4997, -0.4982, -0.4993],\n           ...,\n           [-0.4977, -0.4971, -0.4999, -0.4980],\n           [-0.4979, -0.4999, -0.4986, -0.4963],\n           [-0.4971, -0.4986, -0.4978, -0.4992]],\n\n          [[-0.4964, -0.4981, -0.4987, -0.4972],\n           [-0.4985, -0.4994, -0.4985, -0.4974],\n           [-0.4964, -0.4984, -0.4963, -0.4995],\n           ...,\n           [-0.4983, -0.4989, -0.4961, -0.4965],\n           [-0.4979, -0.4982, -0.4999, -0.4987],\n           [-0.4993, -0.4965, -0.4968, -0.4971]],\n\n          [[-0.4996, -0.4998, -0.4973, -0.4994],\n           [-0.4985, -0.4982, -0.4968, -0.4983],\n           [-0.4992, -0.4983, -0.4976, -0.4993],\n           ...,\n           [-0.4992, -0.4984, -0.4963, -0.4988],\n           [-0.4980, -0.4986, -0.4976, -0.4990],\n           [-0.4979, -0.4995, -0.4966, -0.4997]]],\n\n\n         [[[-0.4962, -0.4964, -0.4967, -0.5000],\n           [-0.4962, -0.4976, -0.4996, -0.4965],\n           [-0.4968, -0.4972, -0.4973, -0.4970],\n           ...,\n           [-0.4997, -0.4963, -0.4985, -0.4963],\n           [-0.4967, -0.4990, -0.4985, -0.4987],\n           [-0.4997, -0.4998, -0.4978, -0.4976]],\n\n          [[-0.4997, -0.5000, -0.5000, -0.4976],\n           [-0.4985, -0.4961, -0.4982, -0.4971],\n           [-0.4996, -0.4993, -0.4963, -0.4981],\n           ...,\n           [-0.4988, -0.4987, -0.4971, -0.4975],\n           [-0.4993, -0.4982, -0.4977, -0.4967],\n           [-0.4964, -0.4972, -0.4995, -0.4986]],\n\n          [[-0.4977, -0.4991, -0.4972, -0.4966],\n           [-0.4993, -0.4996, -0.4985, -0.4971],\n           [-0.4991, -0.4978, -0.4962, -0.4999],\n           ...,\n           [-0.4986, -0.4972, -0.4979, -0.4965],\n           [-0.4979, -0.4966, -0.4972, -0.4967],\n           [-0.4970, -0.4972, -0.4962, -0.4990]],\n\n          ...,\n\n          [[-0.4971, -0.4979, -0.4988, -0.4981],\n           [-0.4969, -0.4975, -0.4991, -0.4962],\n           [-0.4966, -0.4983, -0.4984, -0.4966],\n           ...,\n           [-0.4978, -0.4968, -0.4999, -0.4997],\n           [-0.4979, -0.4993, -0.4978, -0.4987],\n           [-0.4982, -0.4972, -0.4974, -0.4993]],\n\n          [[-0.4963, -0.4996, -0.4964, -0.4986],\n           [-0.4991, -0.4975, -0.4965, -0.4967],\n           [-0.4978, -0.4991, -0.4990, -0.4963],\n           ...,\n           [-0.4966, -0.4978, -0.4996, -0.4979],\n           [-0.4990, -0.4964, -0.4974, -0.4972],\n           [-0.4982, -0.4991, -0.4980, -0.4972]],\n\n          [[-0.4977, -0.4978, -0.4979, -0.4969],\n           [-0.4999, -0.4988, -0.4965, -0.4983],\n           [-0.4986, -0.4982, -0.4969, -0.4995],\n           ...,\n           [-0.4964, -0.4968, -0.4989, -0.4975],\n           [-0.4995, -0.4976, -0.4998, -0.4979],\n           [-0.4985, -0.4974, -0.4978, -0.4998]]]],\n\n\n\n        [[[[-0.4978, -0.4995, -0.4982, -0.4983],\n           [-0.4976, -0.4986, -0.4999, -0.4983],\n           [-0.4977, -0.4994, -0.4976, -0.4966],\n           ...,\n           [-0.4987, -0.4991, -0.4979, -0.4963],\n           [-0.4988, -0.4995, -0.4995, -0.4996],\n           [-0.4994, -0.4969, -0.4982, -0.4969]],\n\n          [[-0.4974, -0.4982, -0.4982, -0.4997],\n           [-0.4987, -0.4965, -0.4979, -0.4980],\n           [-0.4974, -0.4964, -0.4997, -0.4971],\n           ...,\n           [-0.4980, -0.4976, -0.4968, -0.4968],\n           [-0.4990, -0.4987, -0.4973, -0.4981],\n           [-0.4982, -0.4962, -0.4976, -0.4968]],\n\n          [[-0.4975, -0.4990, -0.4962, -0.4970],\n           [-0.4966, -0.4975, -0.4979, -0.4977],\n           [-0.4965, -0.4965, -0.4967, -0.4963],\n           ...,\n           [-0.4991, -0.4988, -0.4993, -0.4978],\n           [-0.4969, -0.4962, -0.4999, -0.4987],\n           [-0.4973, -0.4969, -0.4979, -0.4981]],\n\n          ...,\n\n          [[-0.4987, -0.4969, -0.4967, -0.4991],\n           [-0.4992, -0.4979, -0.4972, -0.4987],\n           [-0.4991, -0.4971, -0.4995, -0.4975],\n           ...,\n           [-0.4993, -0.4999, -0.4991, -0.4964],\n           [-0.4970, -0.4970, -0.4984, -0.4967],\n           [-0.4963, -0.4983, -0.4974, -0.4990]],\n\n          [[-0.4980, -0.4993, -0.4965, -0.4973],\n           [-0.4961, -0.4967, -0.4971, -0.4981],\n           [-0.4991, -0.4964, -0.4975, -0.4982],\n           ...,\n           [-0.4961, -0.4999, -0.4991, -0.4988],\n           [-0.4966, -0.4982, -0.4964, -0.4975],\n           [-0.4999, -0.4988, -0.4969, -0.4973]],\n\n          [[-0.4973, -0.4982, -0.4992, -0.4980],\n           [-0.4967, -0.4968, -0.4970, -0.4986],\n           [-0.4965, -0.4988, -0.4977, -0.4984],\n           ...,\n           [-0.4972, -0.4985, -0.4993, -0.4988],\n           [-0.4971, -0.4987, -0.4973, -0.4971],\n           [-0.4999, -0.4994, -0.4973, -0.4962]]],\n\n\n         [[[-0.4977, -0.4985, -0.4997, -0.4976],\n           [-0.4981, -0.4995, -0.4976, -0.4974],\n           [-0.4994, -0.4982, -0.4992, -0.4976],\n           ...,\n           [-0.4970, -0.4978, -0.4969, -0.4965],\n           [-0.4970, -0.4973, -0.4991, -0.4971],\n           [-0.4998, -0.4979, -0.4990, -0.4990]],\n\n          [[-0.4973, -0.4978, -0.4981, -0.4984],\n           [-0.4978, -0.4982, -0.4965, -0.4964],\n           [-0.4992, -0.4996, -0.4982, -0.4975],\n           ...,\n           [-0.4983, -0.4973, -0.4986, -0.4974],\n           [-0.4977, -0.4979, -0.4984, -0.4962],\n           [-0.4981, -0.4968, -0.4984, -0.4982]],\n\n          [[-0.4968, -0.4988, -0.4988, -0.4967],\n           [-0.4995, -0.4999, -0.4994, -0.4963],\n           [-0.4974, -0.4985, -0.4990, -0.4986],\n           ...,\n           [-0.4998, -0.4989, -0.4999, -0.4969],\n           [-0.4989, -0.4964, -0.4964, -0.4965],\n           [-0.4991, -0.4990, -0.4973, -0.4985]],\n\n          ...,\n\n          [[-0.4971, -0.4983, -0.4961, -0.4963],\n           [-0.4978, -0.4961, -0.4975, -0.4975],\n           [-0.4988, -0.4974, -0.4987, -0.4970],\n           ...,\n           [-0.4970, -0.4986, -0.4980, -0.4994],\n           [-0.4962, -0.4977, -0.4966, -0.4967],\n           [-0.4977, -0.5000, -0.4974, -0.4987]],\n\n          [[-0.4964, -0.4973, -0.4990, -0.4979],\n           [-0.4964, -0.4973, -0.4991, -0.4982],\n           [-0.4997, -0.4993, -0.4962, -0.4994],\n           ...,\n           [-0.4976, -0.4975, -0.5000, -0.4979],\n           [-0.4976, -0.4992, -0.4977, -0.4962],\n           [-0.4992, -0.4998, -0.4989, -0.4963]],\n\n          [[-0.4990, -0.4969, -0.4980, -0.4986],\n           [-0.4991, -0.4992, -0.4983, -0.4996],\n           [-0.4973, -0.4972, -0.4980, -0.4983],\n           ...,\n           [-0.4999, -0.4963, -0.4974, -0.4986],\n           [-0.4962, -0.4987, -0.4990, -0.4987],\n           [-0.4975, -0.4984, -0.4989, -0.4975]]],\n\n\n         [[[-0.4974, -0.4991, -0.4972, -0.4965],\n           [-0.4989, -0.4993, -0.4972, -0.4972],\n           [-0.4963, -0.5000, -0.4971, -0.4961],\n           ...,\n           [-0.4995, -0.4992, -0.4996, -0.4974],\n           [-0.4998, -0.4984, -0.4980, -0.4978],\n           [-0.4987, -0.4979, -0.4974, -0.4963]],\n\n          [[-0.4989, -0.4961, -0.4974, -0.4994],\n           [-0.4987, -0.4967, -0.4975, -0.4973],\n           [-0.4985, -0.4999, -0.4988, -0.4965],\n           ...,\n           [-0.4963, -0.4989, -0.4963, -0.4961],\n           [-0.4977, -0.4971, -0.4965, -0.4966],\n           [-0.4966, -0.4969, -0.5000, -0.4961]],\n\n          [[-0.4990, -0.4970, -0.4968, -0.4981],\n           [-0.4991, -0.4977, -0.4974, -0.4991],\n           [-0.4999, -0.4962, -0.4992, -0.4964],\n           ...,\n           [-0.4965, -0.4975, -0.4997, -0.4988],\n           [-0.4976, -0.4992, -0.4990, -0.4968],\n           [-0.4999, -0.4979, -0.4971, -0.4970]],\n\n          ...,\n\n          [[-0.4974, -0.5000, -0.4987, -0.4967],\n           [-0.4961, -0.4968, -0.4976, -0.4968],\n           [-0.4970, -0.4975, -0.4982, -0.4963],\n           ...,\n           [-0.4980, -0.4983, -0.4965, -0.4979],\n           [-0.4976, -0.4980, -0.4991, -0.4977],\n           [-0.4969, -0.4981, -0.4965, -0.4989]],\n\n          [[-0.4970, -0.4986, -0.4986, -0.4966],\n           [-0.4998, -0.4998, -0.4996, -0.4964],\n           [-0.4964, -0.4988, -0.4990, -0.4975],\n           ...,\n           [-0.4992, -0.4997, -0.4983, -0.4985],\n           [-0.4989, -0.4980, -0.4985, -0.4984],\n           [-0.4998, -0.4963, -0.4979, -0.4980]],\n\n          [[-0.4983, -0.4983, -0.4971, -0.4982],\n           [-0.4997, -0.4988, -0.4976, -0.4978],\n           [-0.4994, -0.4990, -0.4999, -0.5000],\n           ...,\n           [-0.4979, -0.4985, -0.4992, -0.4977],\n           [-0.4971, -0.4994, -0.4968, -0.4999],\n           [-0.4989, -0.4975, -0.4964, -0.4977]]],\n\n\n         ...,\n\n\n         [[[-0.4964, -0.4984, -0.4992, -0.4979],\n           [-0.4980, -0.4970, -0.4970, -0.4979],\n           [-0.4994, -0.4971, -0.4999, -0.4982],\n           ...,\n           [-0.4981, -0.4974, -0.4981, -0.4980],\n           [-0.4999, -0.4975, -0.4990, -0.4984],\n           [-0.4971, -0.4972, -0.4991, -0.4970]],\n\n          [[-0.4962, -0.4963, -0.4984, -0.4965],\n           [-0.4973, -0.4986, -0.4961, -0.4995],\n           [-0.4980, -0.5000, -0.5000, -0.4995],\n           ...,\n           [-0.4982, -0.4968, -0.4963, -0.4974],\n           [-0.4977, -0.4986, -0.4999, -0.4975],\n           [-0.4969, -0.4967, -0.4964, -0.4989]],\n\n          [[-0.4995, -0.4983, -0.4970, -0.4993],\n           [-0.4988, -0.4993, -0.4972, -0.4964],\n           [-0.4982, -0.4963, -0.4972, -0.4995],\n           ...,\n           [-0.4981, -0.4993, -0.4964, -0.4988],\n           [-0.4989, -0.4965, -0.4997, -0.4964],\n           [-0.4986, -0.4995, -0.4982, -0.4982]],\n\n          ...,\n\n          [[-0.4969, -0.4965, -0.4964, -0.4964],\n           [-0.4964, -0.4992, -0.4998, -0.4961],\n           [-0.4978, -0.4996, -0.4979, -0.4983],\n           ...,\n           [-0.4971, -0.4971, -0.4976, -0.4972],\n           [-0.4974, -0.4976, -0.4961, -0.4964],\n           [-0.4966, -0.4983, -0.4963, -0.4996]],\n\n          [[-0.4985, -0.4978, -0.4983, -0.4998],\n           [-0.4993, -0.4984, -0.4963, -0.4975],\n           [-0.4967, -0.4970, -0.4997, -0.4996],\n           ...,\n           [-0.4971, -0.4969, -0.4970, -0.4962],\n           [-0.4980, -0.4988, -0.4991, -0.4980],\n           [-0.4983, -0.4992, -0.4968, -0.4997]],\n\n          [[-0.4973, -0.4972, -0.4971, -0.4990],\n           [-0.4975, -0.4991, -0.4965, -0.4983],\n           [-0.4989, -0.4983, -0.4965, -0.4961],\n           ...,\n           [-0.4962, -0.4991, -0.4963, -0.4982],\n           [-0.4978, -0.4998, -0.4967, -0.4997],\n           [-0.4995, -0.4963, -0.4994, -0.4970]]],\n\n\n         [[[-0.4981, -0.4977, -0.4995, -0.4985],\n           [-0.4972, -0.4972, -0.4968, -0.4984],\n           [-0.4978, -0.4998, -0.4983, -0.4964],\n           ...,\n           [-0.4999, -0.4978, -0.4994, -0.4980],\n           [-0.4964, -0.4997, -0.4997, -0.4972],\n           [-0.4993, -0.4984, -0.4997, -0.4974]],\n\n          [[-0.4972, -0.4999, -0.4997, -0.4977],\n           [-0.4971, -0.4965, -0.4964, -0.4981],\n           [-0.4979, -0.4978, -0.4964, -0.4972],\n           ...,\n           [-0.4986, -0.4974, -0.4997, -0.4987],\n           [-0.4964, -0.4994, -0.4982, -0.4967],\n           [-0.4979, -0.4978, -0.4985, -0.4981]],\n\n          [[-0.4994, -0.4972, -0.4978, -0.4978],\n           [-0.4989, -0.4978, -0.4969, -0.4974],\n           [-0.4999, -0.4967, -0.4999, -0.4975],\n           ...,\n           [-0.4990, -0.4978, -0.4990, -0.4982],\n           [-0.4987, -0.4972, -0.4983, -0.4963],\n           [-0.4973, -0.4964, -0.4992, -0.4966]],\n\n          ...,\n\n          [[-0.4971, -0.4973, -0.4966, -0.4965],\n           [-0.4997, -0.4987, -0.4985, -0.4981],\n           [-0.4974, -0.4980, -0.4981, -0.4994],\n           ...,\n           [-0.4980, -0.4990, -0.4998, -0.4998],\n           [-0.4981, -0.4985, -0.4985, -0.4978],\n           [-0.4979, -0.4993, -0.4993, -0.4995]],\n\n          [[-0.4996, -0.4999, -0.4990, -0.4977],\n           [-0.4973, -0.4964, -0.4973, -0.4964],\n           [-0.4988, -0.4979, -0.4967, -0.4977],\n           ...,\n           [-0.4996, -0.4981, -0.5000, -0.4997],\n           [-0.4988, -0.4977, -0.4978, -0.4968],\n           [-0.4971, -0.4995, -0.4964, -0.4990]],\n\n          [[-0.4971, -0.4974, -0.4974, -0.4975],\n           [-0.4982, -0.4993, -0.4995, -0.4972],\n           [-0.4996, -0.4993, -0.4974, -0.4998],\n           ...,\n           [-0.4993, -0.4971, -0.4984, -0.4982],\n           [-0.4978, -0.4962, -0.4998, -0.4987],\n           [-0.4971, -0.4966, -0.4969, -0.4976]]],\n\n\n         [[[-0.4995, -0.4992, -0.4963, -0.4979],\n           [-0.4962, -0.4971, -0.4970, -0.4985],\n           [-0.4979, -0.4981, -0.4968, -0.4965],\n           ...,\n           [-0.4976, -0.4973, -0.4996, -0.4994],\n           [-0.4983, -0.4989, -0.4978, -0.4968],\n           [-0.4990, -0.4987, -0.4992, -0.4981]],\n\n          [[-0.4993, -0.4998, -0.4976, -0.4965],\n           [-0.4991, -0.4991, -0.4980, -0.4975],\n           [-0.4984, -0.4984, -0.4966, -0.4999],\n           ...,\n           [-0.4993, -0.4988, -0.4991, -0.4990],\n           [-0.4980, -0.4983, -0.4961, -0.4997],\n           [-0.4988, -0.4987, -0.4993, -0.4996]],\n\n          [[-0.4978, -0.4984, -0.4969, -0.4992],\n           [-0.4980, -0.4984, -0.4992, -0.4994],\n           [-0.4976, -0.4982, -0.4984, -0.4976],\n           ...,\n           [-0.4997, -0.4998, -0.4999, -0.4987],\n           [-0.4974, -0.4966, -0.4979, -0.4982],\n           [-0.4999, -0.4982, -0.4981, -0.4986]],\n\n          ...,\n\n          [[-0.4980, -0.4978, -0.4982, -0.4991],\n           [-0.4978, -0.4978, -0.4975, -0.4966],\n           [-0.4968, -0.4989, -0.4971, -0.4997],\n           ...,\n           [-0.4967, -0.4979, -0.4996, -0.4967],\n           [-0.4978, -0.4972, -0.4976, -0.4986],\n           [-0.4973, -0.4994, -0.4999, -0.4988]],\n\n          [[-0.4993, -0.4975, -0.4982, -0.4968],\n           [-0.5000, -0.4992, -0.4961, -0.4972],\n           [-0.4977, -0.4972, -0.4965, -0.4970],\n           ...,\n           [-0.4980, -0.4988, -0.4973, -0.4968],\n           [-0.4977, -0.4968, -0.4995, -0.4976],\n           [-0.4965, -0.4997, -0.4974, -0.4996]],\n\n          [[-0.4986, -0.4978, -0.4991, -0.4991],\n           [-0.4997, -0.4995, -0.4990, -0.4982],\n           [-0.4976, -0.4987, -0.4968, -0.4990],\n           ...,\n           [-0.4966, -0.4969, -0.4976, -0.4999],\n           [-0.4984, -0.4983, -0.4993, -0.4968],\n           [-0.4989, -0.4987, -0.4978, -0.4964]]]],\n\n\n\n        [[[[-0.4976, -0.4966, -0.4968, -0.4978],\n           [-0.4971, -0.4972, -0.4989, -0.4984],\n           [-0.4972, -0.4992, -0.4967, -0.4968],\n           ...,\n           [-0.4986, -0.4972, -0.4991, -0.4974],\n           [-0.4990, -0.4993, -0.4989, -0.4973],\n           [-0.4990, -0.4980, -0.4980, -0.4991]],\n\n          [[-0.4998, -0.4977, -0.4964, -0.4998],\n           [-0.4983, -0.4971, -0.4999, -0.5000],\n           [-0.4977, -0.4975, -0.4983, -0.4995],\n           ...,\n           [-0.4965, -0.4989, -0.4961, -0.4986],\n           [-0.4967, -0.4970, -0.4991, -0.4991],\n           [-0.4967, -0.4970, -0.4968, -0.4993]],\n\n          [[-0.4992, -0.4976, -0.4982, -0.4970],\n           [-0.4978, -0.4995, -0.4976, -0.4989],\n           [-0.4993, -0.4965, -0.4985, -0.4970],\n           ...,\n           [-0.4986, -0.4971, -0.4965, -0.4982],\n           [-0.4983, -0.4981, -0.4984, -0.4991],\n           [-0.4980, -0.4995, -0.4967, -0.4989]],\n\n          ...,\n\n          [[-0.4963, -0.4963, -0.4989, -0.4999],\n           [-0.4994, -0.4962, -0.4995, -0.4984],\n           [-0.4982, -0.4994, -0.4966, -0.4973],\n           ...,\n           [-0.4974, -0.4963, -0.4972, -0.4982],\n           [-0.4962, -0.4961, -0.4972, -0.4978],\n           [-0.4979, -0.4985, -0.4992, -0.4991]],\n\n          [[-0.4998, -0.4989, -0.4989, -0.4983],\n           [-0.4968, -0.4977, -0.4966, -0.4987],\n           [-0.4982, -0.4994, -0.4980, -0.4994],\n           ...,\n           [-0.4971, -0.4998, -0.4964, -0.4972],\n           [-0.4971, -0.4980, -0.4968, -0.4997],\n           [-0.4996, -0.4983, -0.4994, -0.4995]],\n\n          [[-0.4996, -0.4984, -0.4967, -0.4991],\n           [-0.4964, -0.4991, -0.4972, -0.4979],\n           [-0.4991, -0.4978, -0.4998, -0.4997],\n           ...,\n           [-0.4982, -0.4995, -0.4976, -0.4963],\n           [-0.4995, -0.4964, -0.4979, -0.4987],\n           [-0.4988, -0.4980, -0.4974, -0.4966]]],\n\n\n         [[[-0.4984, -0.4986, -0.4993, -0.4977],\n           [-0.4999, -0.4975, -0.4993, -0.4962],\n           [-0.4967, -0.4981, -0.4965, -0.4994],\n           ...,\n           [-0.4992, -0.4961, -0.4991, -0.4977],\n           [-0.4990, -0.4983, -0.4974, -0.4977],\n           [-0.4972, -0.4967, -0.4981, -0.4975]],\n\n          [[-0.4992, -0.4996, -0.4991, -0.4961],\n           [-0.4977, -0.4996, -0.4972, -0.4992],\n           [-0.4983, -0.4991, -0.4975, -0.4972],\n           ...,\n           [-0.4979, -0.4976, -0.4967, -0.4978],\n           [-0.4982, -0.4983, -0.4987, -0.4995],\n           [-0.4967, -0.4975, -0.4978, -0.4984]],\n\n          [[-0.4963, -0.4987, -0.4984, -0.4966],\n           [-0.4964, -0.4961, -0.4966, -0.4967],\n           [-0.4966, -0.4985, -0.4994, -0.4996],\n           ...,\n           [-0.4997, -0.4979, -0.4975, -0.4972],\n           [-0.4971, -0.4982, -0.4982, -0.4996],\n           [-0.4977, -0.4987, -0.4992, -0.4970]],\n\n          ...,\n\n          [[-0.4966, -0.4963, -0.4995, -0.4968],\n           [-0.4970, -0.4984, -0.4988, -0.4995],\n           [-0.4990, -0.4993, -0.4963, -0.4961],\n           ...,\n           [-0.4999, -0.4976, -0.4972, -0.4969],\n           [-0.4979, -0.4961, -0.4987, -0.4981],\n           [-0.4986, -0.4997, -0.4988, -0.4991]],\n\n          [[-0.4977, -0.4967, -0.4999, -0.5000],\n           [-0.4977, -0.4983, -0.4991, -0.4981],\n           [-0.4980, -0.4962, -0.4966, -0.4973],\n           ...,\n           [-0.4991, -0.4968, -0.4966, -0.4965],\n           [-0.4975, -0.4987, -0.4998, -0.4992],\n           [-0.4986, -0.4994, -0.4993, -0.4982]],\n\n          [[-0.4988, -0.4990, -0.4972, -0.4981],\n           [-0.4973, -0.4976, -0.4961, -0.4966],\n           [-0.4979, -0.4972, -0.4995, -0.4978],\n           ...,\n           [-0.4987, -0.4979, -0.4993, -0.4988],\n           [-0.4977, -0.4974, -0.4989, -0.4991],\n           [-0.4984, -0.4998, -0.4976, -0.4963]]],\n\n\n         [[[-0.4968, -0.4991, -0.4998, -0.4989],\n           [-0.4989, -0.4976, -0.4962, -0.4996],\n           [-0.4972, -0.4983, -0.4986, -0.4962],\n           ...,\n           [-0.4996, -0.4995, -0.4996, -0.4982],\n           [-0.4996, -0.4987, -0.4976, -0.4982],\n           [-0.4990, -0.4982, -0.4975, -0.4999]],\n\n          [[-0.4996, -0.4965, -0.4997, -0.4969],\n           [-0.4983, -0.4976, -0.4979, -0.4982],\n           [-0.4992, -0.4988, -0.4982, -0.4994],\n           ...,\n           [-0.4984, -0.4999, -0.4990, -0.4997],\n           [-0.4984, -0.4992, -0.4992, -0.4986],\n           [-0.4994, -0.4975, -0.4997, -0.4977]],\n\n          [[-0.4976, -0.4964, -0.4981, -0.4992],\n           [-0.4990, -0.4965, -0.4991, -0.4995],\n           [-0.4964, -0.4990, -0.4963, -0.4963],\n           ...,\n           [-0.4963, -0.4990, -0.4991, -0.4980],\n           [-0.4998, -0.4961, -0.4969, -0.4968],\n           [-0.4993, -0.4986, -0.4969, -0.4974]],\n\n          ...,\n\n          [[-0.4981, -0.4963, -0.4983, -0.4985],\n           [-0.4979, -0.4992, -0.4981, -0.4974],\n           [-0.4969, -0.4993, -0.4973, -0.4993],\n           ...,\n           [-0.4978, -0.4983, -0.4988, -0.4993],\n           [-0.4987, -0.4962, -0.4976, -0.4966],\n           [-0.4979, -0.4973, -0.4969, -0.4961]],\n\n          [[-0.4972, -0.4962, -0.4986, -0.4978],\n           [-0.4975, -0.4980, -0.4973, -0.4964],\n           [-0.4969, -0.4972, -0.4970, -0.4987],\n           ...,\n           [-0.4966, -0.4980, -0.4985, -0.4962],\n           [-0.4989, -0.4964, -0.4989, -0.4994],\n           [-0.4974, -0.4979, -0.4979, -0.4991]],\n\n          [[-0.4980, -0.4989, -0.4963, -0.5000],\n           [-0.4970, -0.4988, -0.4991, -0.4983],\n           [-0.4977, -0.4973, -0.4988, -0.4962],\n           ...,\n           [-0.4975, -0.4981, -0.4975, -0.4990],\n           [-0.4969, -0.4967, -0.4964, -0.4982],\n           [-0.4991, -0.4984, -0.4966, -0.4964]]],\n\n\n         ...,\n\n\n         [[[-0.4971, -0.4997, -0.4996, -0.4990],\n           [-0.4967, -0.4987, -0.4975, -0.4984],\n           [-0.4994, -0.4988, -0.4961, -0.4968],\n           ...,\n           [-0.4971, -0.4965, -0.4994, -0.4970],\n           [-0.4981, -0.4999, -0.4977, -0.4985],\n           [-0.4985, -0.4992, -0.4993, -0.4996]],\n\n          [[-0.4963, -0.4983, -0.4990, -0.4991],\n           [-0.4986, -0.4970, -0.4962, -0.4973],\n           [-0.4971, -0.4966, -0.4974, -0.4998],\n           ...,\n           [-0.4967, -0.4962, -0.4966, -0.4995],\n           [-0.4996, -0.4970, -0.4968, -0.4964],\n           [-0.4996, -0.4981, -0.4973, -0.4986]],\n\n          [[-0.4978, -0.4997, -0.4997, -0.4967],\n           [-0.4970, -0.4977, -0.4968, -0.4964],\n           [-0.4972, -0.4980, -0.4983, -0.4968],\n           ...,\n           [-0.4977, -0.4968, -0.4979, -0.4974],\n           [-0.4970, -0.4996, -0.4997, -0.4984],\n           [-0.4978, -0.4961, -0.4977, -0.4964]],\n\n          ...,\n\n          [[-0.4996, -0.4983, -0.4981, -0.4980],\n           [-0.4991, -0.4971, -0.4970, -0.4986],\n           [-0.4990, -0.4979, -0.4985, -0.4998],\n           ...,\n           [-0.4978, -0.4988, -0.4999, -0.4962],\n           [-0.4984, -0.4993, -0.4979, -0.4987],\n           [-0.4973, -0.4972, -0.4989, -0.4981]],\n\n          [[-0.4998, -0.4990, -0.4967, -0.4997],\n           [-0.4964, -0.4972, -0.4993, -0.4968],\n           [-0.4990, -0.4984, -0.4992, -0.4974],\n           ...,\n           [-0.4986, -0.4993, -0.4966, -0.4990],\n           [-0.4974, -0.4982, -0.4986, -0.4973],\n           [-0.4993, -0.4972, -0.4989, -0.4970]],\n\n          [[-0.4984, -0.4981, -0.4978, -0.4988],\n           [-0.4993, -0.4986, -0.4996, -0.4979],\n           [-0.4964, -0.4962, -0.4988, -0.4981],\n           ...,\n           [-0.4968, -0.4979, -0.4965, -0.4986],\n           [-0.4974, -0.4969, -0.4992, -0.4988],\n           [-0.4988, -0.4987, -0.4974, -0.4980]]],\n\n\n         [[[-0.4992, -0.4981, -0.4967, -0.4999],\n           [-0.4986, -0.4980, -0.4970, -0.4997],\n           [-0.4964, -0.4993, -0.4969, -0.4995],\n           ...,\n           [-0.4984, -0.4991, -0.4974, -0.4987],\n           [-0.4972, -0.4983, -0.4967, -0.4999],\n           [-0.4996, -0.4990, -0.4968, -0.4980]],\n\n          [[-0.4987, -0.4992, -0.4987, -0.4972],\n           [-0.4982, -0.4996, -0.4990, -0.4973],\n           [-0.4963, -0.4989, -0.4961, -0.4985],\n           ...,\n           [-0.4972, -0.4993, -0.4993, -0.4992],\n           [-0.4968, -0.4968, -0.4976, -0.4964],\n           [-0.4992, -0.4966, -0.4995, -0.4976]],\n\n          [[-0.4971, -0.4980, -0.4963, -0.4997],\n           [-0.4962, -0.4973, -0.4998, -0.4985],\n           [-0.4999, -0.4963, -0.4991, -0.4979],\n           ...,\n           [-0.4991, -0.4980, -0.4992, -0.4981],\n           [-0.4989, -0.4963, -0.4988, -0.4988],\n           [-0.4986, -0.4999, -0.4962, -0.4976]],\n\n          ...,\n\n          [[-0.4973, -0.4986, -0.4971, -0.4981],\n           [-0.4983, -0.4981, -0.4977, -0.4982],\n           [-0.4976, -0.4993, -0.4999, -0.4972],\n           ...,\n           [-0.4978, -0.4979, -0.4971, -0.4994],\n           [-0.4976, -0.4974, -0.4997, -0.4974],\n           [-0.4989, -0.4961, -0.4967, -0.4963]],\n\n          [[-0.4985, -0.4988, -0.4977, -0.4988],\n           [-0.4978, -0.4975, -0.4990, -0.4962],\n           [-0.4987, -0.4966, -0.4973, -0.4994],\n           ...,\n           [-0.4988, -0.4962, -0.4972, -0.4986],\n           [-0.4996, -0.4982, -0.4963, -0.4994],\n           [-0.4977, -0.4970, -0.4979, -0.4998]],\n\n          [[-0.4996, -0.4994, -0.4970, -0.4982],\n           [-0.4973, -0.4991, -0.4989, -0.4992],\n           [-0.4998, -0.4995, -0.4976, -0.4999],\n           ...,\n           [-0.4985, -0.4985, -0.4993, -0.4991],\n           [-0.4961, -0.4992, -0.4984, -0.4983],\n           [-0.4996, -0.4965, -0.4982, -0.4985]]],\n\n\n         [[[-0.4980, -0.4967, -0.4973, -0.4987],\n           [-0.4975, -0.4977, -0.4966, -0.4997],\n           [-0.4976, -0.4978, -0.4964, -0.4970],\n           ...,\n           [-0.4975, -0.4996, -0.4966, -0.4962],\n           [-0.4984, -0.4983, -0.4981, -0.4973],\n           [-0.4983, -0.4995, -0.4996, -0.4963]],\n\n          [[-0.4969, -0.4984, -0.4971, -0.4969],\n           [-0.4967, -0.4985, -0.4993, -0.4993],\n           [-0.4967, -0.4984, -0.4964, -0.4974],\n           ...,\n           [-0.4967, -0.4976, -0.4961, -0.4972],\n           [-0.4981, -0.4962, -0.4988, -0.4992],\n           [-0.4972, -0.4970, -0.4986, -0.4972]],\n\n          [[-0.4982, -0.4994, -0.4984, -0.4989],\n           [-0.4973, -0.4994, -0.4990, -0.4969],\n           [-0.4966, -0.4968, -0.4968, -0.4962],\n           ...,\n           [-0.4999, -0.4984, -0.4984, -0.4991],\n           [-0.4997, -0.4965, -0.4991, -0.4985],\n           [-0.4987, -0.4978, -0.4964, -0.4972]],\n\n          ...,\n\n          [[-0.4984, -0.4996, -0.5000, -0.4987],\n           [-0.4963, -0.4963, -0.4994, -0.4994],\n           [-0.4974, -0.4991, -0.4971, -0.4982],\n           ...,\n           [-0.4968, -0.4975, -0.4976, -0.4995],\n           [-0.4989, -0.4983, -0.4998, -0.4998],\n           [-0.4981, -0.4985, -0.4964, -0.4970]],\n\n          [[-0.4996, -0.4966, -0.4993, -0.4985],\n           [-0.4989, -0.4990, -0.4975, -0.4985],\n           [-0.5000, -0.4985, -0.4977, -0.4967],\n           ...,\n           [-0.4968, -0.4973, -0.4984, -0.4997],\n           [-0.4972, -0.4999, -0.4966, -0.4991],\n           [-0.4995, -0.4984, -0.4979, -0.4968]],\n\n          [[-0.4999, -0.4978, -0.4988, -0.4994],\n           [-0.4980, -0.4979, -0.4982, -0.4969],\n           [-0.4993, -0.4981, -0.4992, -0.4979],\n           ...,\n           [-0.4966, -0.4963, -0.4977, -0.4966],\n           [-0.4973, -0.4980, -0.4990, -0.4997],\n           [-0.4992, -0.4976, -0.4991, -0.4970]]]],\n\n\n\n        ...,\n\n\n\n        [[[[-0.4961, -0.4993, -0.4964, -0.4962],\n           [-0.4966, -0.4987, -0.4990, -0.4982],\n           [-0.4976, -0.4973, -0.4999, -0.4985],\n           ...,\n           [-0.4979, -0.4983, -0.4991, -0.4997],\n           [-0.4964, -0.4999, -0.4964, -0.4968],\n           [-0.4969, -0.4966, -0.4987, -0.4987]],\n\n          [[-0.4967, -0.4979, -0.4995, -0.4987],\n           [-0.4988, -0.4961, -0.4997, -0.4972],\n           [-0.4980, -0.4991, -0.4964, -0.4996],\n           ...,\n           [-0.4997, -0.4965, -0.4973, -0.4980],\n           [-0.4995, -0.4999, -0.4990, -0.4969],\n           [-0.4986, -0.4984, -0.4971, -0.4973]],\n\n          [[-0.4966, -0.4982, -0.4977, -0.4983],\n           [-0.4988, -0.4964, -0.4969, -0.4990],\n           [-0.4980, -0.4972, -0.4962, -0.4974],\n           ...,\n           [-0.4967, -0.4973, -0.4991, -0.4975],\n           [-0.4965, -0.4966, -0.4995, -0.4965],\n           [-0.4965, -0.4966, -0.4965, -0.4996]],\n\n          ...,\n\n          [[-0.4968, -0.4991, -0.4963, -0.4970],\n           [-0.4970, -0.4964, -0.4992, -0.4970],\n           [-0.4973, -0.4974, -0.4981, -0.4979],\n           ...,\n           [-0.4999, -0.4965, -0.4993, -0.4988],\n           [-0.4964, -0.4999, -0.4978, -0.4993],\n           [-0.4963, -0.5000, -0.4979, -0.4993]],\n\n          [[-0.4967, -0.4967, -0.4995, -0.4981],\n           [-0.4995, -0.4973, -0.4979, -0.4970],\n           [-0.4968, -0.4999, -0.4998, -0.4973],\n           ...,\n           [-0.4968, -0.4966, -0.4971, -0.4962],\n           [-0.4963, -0.4974, -0.4985, -0.4998],\n           [-0.4964, -0.4983, -0.4971, -0.4999]],\n\n          [[-0.4995, -0.4972, -0.4969, -0.4963],\n           [-0.4970, -0.4985, -0.4966, -0.4996],\n           [-0.4961, -0.4965, -0.4981, -0.4962],\n           ...,\n           [-0.4982, -0.4979, -0.4977, -0.4985],\n           [-0.4969, -0.4988, -0.4968, -0.4993],\n           [-0.4985, -0.4968, -0.4983, -0.4969]]],\n\n\n         [[[-0.4964, -0.4973, -0.4995, -0.4974],\n           [-0.4968, -0.4995, -0.4976, -0.4988],\n           [-0.4966, -0.4984, -0.4997, -0.4983],\n           ...,\n           [-0.4997, -0.4966, -0.4969, -0.4989],\n           [-0.4972, -0.4980, -0.4966, -0.4962],\n           [-0.4984, -0.4970, -0.4964, -0.4963]],\n\n          [[-0.5000, -0.4978, -0.4989, -0.4977],\n           [-0.4979, -0.4979, -0.4963, -0.4973],\n           [-0.4962, -0.4970, -0.4996, -0.4965],\n           ...,\n           [-0.4993, -0.4963, -0.4966, -0.4985],\n           [-0.4994, -0.4979, -0.4971, -0.4988],\n           [-0.4987, -0.4987, -0.4993, -0.4963]],\n\n          [[-0.4993, -0.4995, -0.4968, -0.4981],\n           [-0.4993, -0.4987, -0.4979, -0.4997],\n           [-0.4988, -0.4988, -0.4964, -0.4975],\n           ...,\n           [-0.4974, -0.4974, -0.4965, -0.4974],\n           [-0.4968, -0.4964, -0.4970, -0.4994],\n           [-0.4965, -0.4971, -0.4998, -0.4977]],\n\n          ...,\n\n          [[-0.4963, -0.4972, -0.4966, -0.4984],\n           [-0.4968, -0.4964, -0.4975, -0.4980],\n           [-0.4966, -0.4971, -0.4967, -0.4979],\n           ...,\n           [-0.4985, -0.4988, -0.4962, -0.4982],\n           [-0.4989, -0.4985, -0.4975, -0.4961],\n           [-0.4988, -0.4969, -0.4964, -0.4969]],\n\n          [[-0.4979, -0.4987, -0.4967, -0.4986],\n           [-0.4979, -0.4972, -0.4971, -0.4984],\n           [-0.4984, -0.4963, -0.4978, -0.4979],\n           ...,\n           [-0.4986, -0.4980, -0.4965, -0.4992],\n           [-0.4969, -0.4982, -0.4979, -0.5000],\n           [-0.4992, -0.4986, -0.4987, -0.4994]],\n\n          [[-0.4996, -0.4988, -0.4994, -0.4981],\n           [-0.4977, -0.4993, -0.4986, -0.4991],\n           [-0.4964, -0.4997, -0.4995, -0.4976],\n           ...,\n           [-0.4967, -0.4984, -0.4979, -0.4970],\n           [-0.4973, -0.4996, -0.4984, -0.4976],\n           [-0.4980, -0.4976, -0.4972, -0.4984]]],\n\n\n         [[[-0.4969, -0.4975, -0.4968, -0.4987],\n           [-0.4983, -0.4998, -0.4985, -0.4989],\n           [-0.4986, -0.4985, -0.4965, -0.4969],\n           ...,\n           [-0.4994, -0.4996, -0.4984, -0.4981],\n           [-0.4988, -0.4992, -0.4997, -0.4999],\n           [-0.4988, -0.4974, -0.4997, -0.4993]],\n\n          [[-0.4979, -0.4975, -0.4984, -0.4980],\n           [-0.4989, -0.4974, -0.4978, -0.4971],\n           [-0.4969, -0.4996, -0.4972, -0.4975],\n           ...,\n           [-0.4999, -0.4999, -0.4965, -0.4981],\n           [-0.4982, -0.4988, -0.4968, -0.4962],\n           [-0.4976, -0.4980, -0.4974, -0.4964]],\n\n          [[-0.4987, -0.4975, -0.4964, -0.4975],\n           [-0.4977, -0.4972, -0.4988, -0.4992],\n           [-0.4969, -0.4967, -0.4963, -0.4975],\n           ...,\n           [-0.4994, -0.4968, -0.4970, -0.4964],\n           [-0.4969, -0.4965, -0.4964, -0.4997],\n           [-0.4970, -0.4967, -0.4991, -0.4977]],\n\n          ...,\n\n          [[-0.4983, -0.4988, -0.4998, -0.4989],\n           [-0.4968, -0.4988, -0.4986, -0.4996],\n           [-0.4962, -0.4962, -0.4990, -0.4999],\n           ...,\n           [-0.4998, -0.4994, -0.4994, -0.4991],\n           [-0.4962, -0.4978, -0.4969, -0.4977],\n           [-0.4983, -0.4976, -0.4986, -0.4996]],\n\n          [[-0.4989, -0.4992, -0.4993, -0.4973],\n           [-0.4997, -0.4972, -0.4976, -0.4963],\n           [-0.4985, -0.4998, -0.4982, -0.4971],\n           ...,\n           [-0.4977, -0.4962, -0.4972, -0.4981],\n           [-0.4995, -0.4974, -0.4974, -0.4978],\n           [-0.4992, -0.4970, -0.4962, -0.4980]],\n\n          [[-0.4974, -0.4980, -0.4971, -0.4995],\n           [-0.4980, -0.4962, -0.4980, -0.4976],\n           [-0.4997, -0.4991, -0.4991, -0.4986],\n           ...,\n           [-0.4989, -0.4969, -0.4968, -0.4987],\n           [-0.4980, -0.4985, -0.4963, -0.4972],\n           [-0.4980, -0.4965, -0.4970, -0.4967]]],\n\n\n         ...,\n\n\n         [[[-0.4998, -0.4994, -0.4976, -0.4988],\n           [-0.4963, -0.4980, -0.4998, -0.4983],\n           [-0.4989, -0.4962, -0.4992, -0.4975],\n           ...,\n           [-0.5000, -0.4967, -0.4997, -0.4989],\n           [-0.4966, -0.4985, -0.4975, -0.4976],\n           [-0.4992, -0.4984, -0.4994, -0.4983]],\n\n          [[-0.4966, -0.4965, -0.4980, -0.4982],\n           [-0.4966, -0.4963, -0.4990, -0.4998],\n           [-0.4996, -0.4981, -0.4984, -0.4993],\n           ...,\n           [-0.4989, -0.4993, -0.4990, -0.4991],\n           [-0.4973, -0.4981, -0.4985, -0.4993],\n           [-0.4967, -0.4962, -0.4969, -0.4963]],\n\n          [[-0.4974, -0.4993, -0.4971, -0.4980],\n           [-0.4990, -0.4990, -0.4975, -0.4999],\n           [-0.4985, -0.4977, -0.4976, -0.4969],\n           ...,\n           [-0.4998, -0.4977, -0.4964, -0.4996],\n           [-0.4977, -0.4970, -0.4989, -0.4989],\n           [-0.4992, -0.4967, -0.4978, -0.4964]],\n\n          ...,\n\n          [[-0.4982, -0.4992, -0.4999, -0.4970],\n           [-0.4975, -0.4988, -0.4999, -0.4974],\n           [-0.4997, -0.4967, -0.4994, -0.4996],\n           ...,\n           [-0.4964, -0.4995, -0.4991, -0.4999],\n           [-0.4992, -0.4972, -0.4964, -0.4975],\n           [-0.4962, -0.4972, -0.4983, -0.4981]],\n\n          [[-0.4974, -0.4995, -0.4983, -0.4980],\n           [-0.4981, -0.4996, -0.4977, -0.4961],\n           [-0.4996, -0.4966, -0.4994, -0.4976],\n           ...,\n           [-0.4985, -0.4977, -0.4968, -0.4992],\n           [-0.4999, -0.4989, -0.4992, -0.4991],\n           [-0.4993, -0.4994, -0.4987, -0.4990]],\n\n          [[-0.4963, -0.4983, -0.4975, -0.4985],\n           [-0.4992, -0.4999, -0.4967, -0.4978],\n           [-0.4981, -0.4996, -0.4998, -0.4967],\n           ...,\n           [-0.4984, -0.4962, -0.4993, -0.4970],\n           [-0.4970, -0.4986, -0.4978, -0.4996],\n           [-0.4964, -0.4983, -0.4991, -0.4973]]],\n\n\n         [[[-0.4973, -0.4978, -0.4971, -0.4986],\n           [-0.4973, -0.4982, -0.4992, -0.4966],\n           [-0.4977, -0.4993, -0.4996, -0.4967],\n           ...,\n           [-0.4999, -0.4992, -0.4981, -0.4966],\n           [-0.4968, -0.4981, -0.4966, -0.4979],\n           [-0.4969, -0.4978, -0.4988, -0.4980]],\n\n          [[-0.4986, -0.4982, -0.4979, -0.4990],\n           [-0.4986, -0.4965, -0.4985, -0.4977],\n           [-0.4964, -0.4987, -0.4980, -0.4965],\n           ...,\n           [-0.4973, -0.4969, -0.4977, -0.4985],\n           [-0.4971, -0.4962, -0.4961, -0.4993],\n           [-0.4982, -0.4993, -0.4993, -0.4972]],\n\n          [[-0.4979, -0.4967, -0.4978, -0.4995],\n           [-0.4982, -0.4997, -0.4972, -0.4986],\n           [-0.4962, -0.4982, -0.4986, -0.4980],\n           ...,\n           [-0.4992, -0.4976, -0.4971, -0.4997],\n           [-0.4994, -0.4991, -0.4963, -0.4995],\n           [-0.4968, -0.4995, -0.4977, -0.4988]],\n\n          ...,\n\n          [[-0.4984, -0.4968, -0.4981, -0.4993],\n           [-0.4988, -0.4991, -0.4967, -0.4969],\n           [-0.4976, -0.4987, -0.4975, -0.4974],\n           ...,\n           [-0.4967, -0.4964, -0.4968, -0.4990],\n           [-0.4974, -0.4981, -0.4999, -0.4976],\n           [-0.4993, -0.4983, -0.4969, -0.4976]],\n\n          [[-0.4991, -0.4973, -0.4982, -0.4984],\n           [-0.4992, -0.4972, -0.4994, -0.4976],\n           [-0.4989, -0.4998, -0.4995, -0.4968],\n           ...,\n           [-0.4967, -0.4985, -0.4980, -0.4962],\n           [-0.4984, -0.4998, -0.4989, -0.4999],\n           [-0.4981, -0.4961, -0.4986, -0.4999]],\n\n          [[-0.4993, -0.4980, -0.4980, -0.4990],\n           [-0.4988, -0.4962, -0.4997, -0.4970],\n           [-0.5000, -0.4988, -0.4965, -0.4985],\n           ...,\n           [-0.4984, -0.4969, -0.4988, -0.4991],\n           [-0.4983, -0.4994, -0.4962, -0.4988],\n           [-0.4994, -0.4987, -0.4996, -0.4968]]],\n\n\n         [[[-0.4968, -0.4978, -0.4995, -0.4966],\n           [-0.4982, -0.4971, -0.4967, -0.4964],\n           [-0.4974, -0.5000, -0.4966, -0.4978],\n           ...,\n           [-0.4977, -0.4964, -0.4988, -0.4976],\n           [-0.4965, -0.4988, -0.4976, -0.4985],\n           [-0.4963, -0.4983, -0.4963, -0.4976]],\n\n          [[-0.4999, -0.4964, -0.4972, -0.4998],\n           [-0.4968, -0.4974, -0.4991, -0.4962],\n           [-0.4987, -0.4983, -0.4986, -0.4971],\n           ...,\n           [-0.4962, -0.4992, -0.4966, -0.4985],\n           [-0.4979, -0.4966, -0.4967, -0.4977],\n           [-0.4976, -0.4971, -0.4996, -0.4977]],\n\n          [[-0.4969, -0.4984, -0.4971, -0.4968],\n           [-0.4977, -0.4974, -0.4966, -0.4992],\n           [-0.4996, -0.4987, -0.4990, -0.4975],\n           ...,\n           [-0.4976, -0.4985, -0.4977, -0.4961],\n           [-0.4987, -0.4984, -0.4964, -0.4988],\n           [-0.4968, -0.4965, -0.4975, -0.5000]],\n\n          ...,\n\n          [[-0.4978, -0.4962, -0.4989, -0.4964],\n           [-0.4980, -0.4999, -0.4980, -0.4981],\n           [-0.4967, -0.4991, -0.4970, -0.4967],\n           ...,\n           [-0.4974, -0.4985, -0.4989, -0.4968],\n           [-0.4996, -0.4991, -0.4973, -0.4994],\n           [-0.4964, -0.4977, -0.4988, -0.4981]],\n\n          [[-0.4980, -0.4970, -0.4995, -0.4966],\n           [-0.4976, -0.4978, -0.4980, -0.4984],\n           [-0.4995, -0.4986, -0.4999, -0.4992],\n           ...,\n           [-0.4993, -0.4993, -0.4973, -0.4978],\n           [-0.4991, -0.4989, -0.4964, -0.4975],\n           [-0.4998, -0.4998, -0.4988, -0.4991]],\n\n          [[-0.4998, -0.4989, -0.4984, -0.4972],\n           [-0.4998, -0.4984, -0.4988, -0.5000],\n           [-0.4999, -0.4978, -0.4983, -0.4984],\n           ...,\n           [-0.4976, -0.4977, -0.4975, -0.4998],\n           [-0.4968, -0.4987, -0.4980, -0.4978],\n           [-0.4977, -0.4990, -0.4985, -0.4963]]]],\n\n\n\n        [[[[-0.4990, -0.4998, -0.4978, -0.4963],\n           [-0.4995, -0.4985, -0.4984, -0.4990],\n           [-0.4968, -0.4993, -0.4995, -0.4962],\n           ...,\n           [-0.4992, -0.4963, -0.4984, -0.4991],\n           [-0.4969, -0.4975, -0.4991, -0.4987],\n           [-0.4991, -0.4978, -0.4964, -0.4971]],\n\n          [[-0.4995, -0.4988, -0.4961, -0.4994],\n           [-0.4983, -0.4970, -0.4991, -0.4979],\n           [-0.4962, -0.4962, -0.4985, -0.4968],\n           ...,\n           [-0.4966, -0.4975, -0.4978, -0.4990],\n           [-0.4974, -0.4982, -0.4980, -0.4977],\n           [-0.4976, -0.4985, -0.4971, -0.4972]],\n\n          [[-0.4995, -0.4977, -0.4999, -0.4986],\n           [-0.4990, -0.4996, -0.4992, -0.4988],\n           [-0.4963, -0.4973, -0.4967, -0.4990],\n           ...,\n           [-0.4971, -0.4971, -0.4973, -0.4967],\n           [-0.4978, -0.4992, -0.4967, -0.4973],\n           [-0.4988, -0.4997, -0.4970, -0.4981]],\n\n          ...,\n\n          [[-0.4978, -0.4994, -0.4973, -0.4980],\n           [-0.4963, -0.4965, -0.4993, -0.4980],\n           [-0.4991, -0.4997, -0.4968, -0.4979],\n           ...,\n           [-0.4963, -0.4997, -0.4996, -0.4975],\n           [-0.4979, -0.4999, -0.4996, -0.4963],\n           [-0.4978, -0.4999, -0.4961, -0.5000]],\n\n          [[-0.4989, -0.4973, -0.4980, -0.4963],\n           [-0.4968, -0.4975, -0.4977, -0.4980],\n           [-0.4992, -0.4971, -0.4966, -0.4964],\n           ...,\n           [-0.4981, -0.4974, -0.4964, -0.4998],\n           [-0.4979, -0.4975, -0.4964, -0.4996],\n           [-0.4965, -0.4986, -0.4962, -0.4963]],\n\n          [[-0.4971, -0.4963, -0.4988, -0.4965],\n           [-0.4992, -0.4972, -0.4997, -0.4997],\n           [-0.4962, -0.4990, -0.4990, -0.4999],\n           ...,\n           [-0.4997, -0.4966, -0.4993, -0.4964],\n           [-0.4975, -0.4973, -0.4983, -0.4981],\n           [-0.4967, -0.4975, -0.4993, -0.4997]]],\n\n\n         [[[-0.4999, -0.4984, -0.4963, -0.4969],\n           [-0.4993, -0.5000, -0.4986, -0.4985],\n           [-0.4998, -0.4984, -0.4990, -0.4961],\n           ...,\n           [-0.4977, -0.4994, -0.4970, -0.4984],\n           [-0.4982, -0.4973, -0.4979, -0.4983],\n           [-0.4986, -0.4982, -0.4986, -0.4983]],\n\n          [[-0.4971, -0.4962, -0.4997, -0.4985],\n           [-0.4970, -0.4963, -0.4978, -0.4963],\n           [-0.4980, -0.4973, -0.4975, -0.4994],\n           ...,\n           [-0.4967, -0.4988, -0.4963, -0.4994],\n           [-0.4964, -0.4966, -0.4976, -0.4988],\n           [-0.4968, -0.4983, -0.4962, -0.4984]],\n\n          [[-0.4977, -0.4968, -0.4974, -0.4981],\n           [-0.4978, -0.4965, -0.4990, -0.4991],\n           [-0.4982, -0.4996, -0.4995, -0.4984],\n           ...,\n           [-0.4991, -0.4978, -0.4998, -0.4964],\n           [-0.4962, -0.4975, -0.4976, -0.5000],\n           [-0.4976, -0.4970, -0.4996, -0.4978]],\n\n          ...,\n\n          [[-0.4974, -0.4983, -0.4965, -0.4994],\n           [-0.4976, -0.4991, -0.4974, -0.4974],\n           [-0.4978, -0.4979, -0.4978, -0.4977],\n           ...,\n           [-0.4988, -0.4994, -0.4977, -0.4997],\n           [-0.4991, -0.4965, -0.4979, -0.4986],\n           [-0.4987, -0.4984, -0.4978, -0.4965]],\n\n          [[-0.4964, -0.4963, -0.4984, -0.4990],\n           [-0.4964, -0.4963, -0.4993, -0.4997],\n           [-0.4987, -0.4995, -0.4961, -0.4997],\n           ...,\n           [-0.4978, -0.4975, -0.4962, -0.4988],\n           [-0.4984, -0.4998, -0.4989, -0.4969],\n           [-0.4980, -0.4990, -0.4981, -0.4989]],\n\n          [[-0.4994, -0.4979, -0.4995, -0.4968],\n           [-0.4974, -0.4977, -0.4990, -0.4962],\n           [-0.4977, -0.4994, -0.4976, -0.4990],\n           ...,\n           [-0.4965, -0.4991, -0.4992, -0.4964],\n           [-0.4986, -0.4991, -0.4969, -0.4977],\n           [-0.4971, -0.4978, -0.4964, -0.4964]]],\n\n\n         [[[-0.4969, -0.4968, -0.5000, -0.4990],\n           [-0.4997, -0.4987, -0.4974, -0.4994],\n           [-0.4984, -0.4978, -0.4990, -0.4986],\n           ...,\n           [-0.4981, -0.4998, -0.4989, -0.4963],\n           [-0.4981, -0.4996, -0.4978, -0.4971],\n           [-0.4999, -0.4988, -0.4987, -0.4964]],\n\n          [[-0.4976, -0.4982, -0.4969, -0.4975],\n           [-0.4987, -0.4997, -0.4970, -0.4979],\n           [-0.4962, -0.4976, -0.4994, -0.4979],\n           ...,\n           [-0.4990, -0.4973, -0.4981, -0.4964],\n           [-0.4971, -0.4964, -0.4993, -0.4965],\n           [-0.4979, -0.4986, -0.4965, -0.4971]],\n\n          [[-0.4999, -0.4995, -0.4989, -0.4977],\n           [-0.4984, -0.5000, -0.4971, -0.4966],\n           [-0.4991, -0.4985, -0.4966, -0.4993],\n           ...,\n           [-0.4980, -0.4982, -0.4967, -0.4989],\n           [-0.4973, -0.4987, -0.4993, -0.4963],\n           [-0.4984, -0.4991, -0.4983, -0.4973]],\n\n          ...,\n\n          [[-0.4965, -0.4963, -0.4976, -0.4977],\n           [-0.4983, -0.4966, -0.4963, -0.4999],\n           [-0.4978, -0.4970, -0.4984, -0.4967],\n           ...,\n           [-0.4977, -0.4990, -0.4977, -0.4985],\n           [-0.4987, -0.4973, -0.4997, -0.4993],\n           [-0.4982, -0.4991, -0.4978, -0.4967]],\n\n          [[-0.4964, -0.4983, -0.4984, -0.4971],\n           [-0.4996, -0.4970, -0.4978, -0.4998],\n           [-0.4977, -0.4995, -0.4987, -0.4988],\n           ...,\n           [-0.4966, -0.4971, -0.4973, -0.4991],\n           [-0.4962, -0.4982, -0.4981, -0.4986],\n           [-0.5000, -0.4969, -0.4979, -0.4996]],\n\n          [[-0.4983, -0.4997, -0.4979, -0.4965],\n           [-0.4991, -0.4971, -0.4964, -0.4981],\n           [-0.4991, -0.4972, -0.4968, -0.4999],\n           ...,\n           [-0.4961, -0.4985, -0.4978, -0.4977],\n           [-0.4987, -0.4962, -0.4978, -0.4961],\n           [-0.4971, -0.5000, -0.4965, -0.4993]]],\n\n\n         ...,\n\n\n         [[[-0.4976, -0.4970, -0.4964, -0.4986],\n           [-0.4970, -0.4990, -0.4995, -0.4990],\n           [-0.4989, -0.4981, -0.4978, -0.4989],\n           ...,\n           [-0.4984, -0.4992, -0.4976, -0.4984],\n           [-0.4970, -0.4975, -0.4968, -0.4987],\n           [-0.4990, -0.4979, -0.4973, -0.4991]],\n\n          [[-0.4987, -0.4989, -0.4965, -0.4983],\n           [-0.4989, -0.4974, -0.4964, -0.4961],\n           [-0.4981, -0.4974, -0.4983, -0.4977],\n           ...,\n           [-0.4996, -0.4963, -0.4961, -0.4974],\n           [-0.4977, -0.4995, -0.4975, -0.4980],\n           [-0.5000, -0.4985, -0.4998, -0.4968]],\n\n          [[-0.4984, -0.4988, -0.4962, -0.4977],\n           [-0.4976, -0.4979, -0.4974, -0.4991],\n           [-0.4982, -0.4970, -0.4966, -0.4967],\n           ...,\n           [-0.4991, -0.4987, -0.4984, -0.4965],\n           [-0.4966, -0.4989, -0.4969, -0.4997],\n           [-0.4978, -0.4985, -0.4978, -0.4970]],\n\n          ...,\n\n          [[-0.4967, -0.4991, -0.4967, -0.4996],\n           [-0.4992, -0.4975, -0.4965, -0.4982],\n           [-0.4961, -0.4987, -0.4997, -0.4980],\n           ...,\n           [-0.4981, -0.4972, -0.4970, -0.4987],\n           [-0.4973, -0.4969, -0.4996, -0.4977],\n           [-0.4995, -0.4962, -0.4967, -0.4979]],\n\n          [[-0.4997, -0.4971, -0.4978, -0.4965],\n           [-0.4993, -0.4996, -0.4967, -0.4971],\n           [-0.4991, -0.4979, -0.4967, -0.4969],\n           ...,\n           [-0.4983, -0.4981, -0.4969, -0.4987],\n           [-0.4970, -0.4962, -0.4987, -0.4989],\n           [-0.4976, -0.4964, -0.4979, -0.4972]],\n\n          [[-0.4969, -0.4985, -0.4999, -0.4981],\n           [-0.4965, -0.4988, -0.4992, -0.4997],\n           [-0.4993, -0.4963, -0.4982, -0.4997],\n           ...,\n           [-0.4995, -0.4983, -0.4989, -0.4964],\n           [-0.4974, -0.4967, -0.4979, -0.4987],\n           [-0.4991, -0.4980, -0.4995, -0.4967]]],\n\n\n         [[[-0.4991, -0.4983, -0.4994, -0.4993],\n           [-0.4980, -0.4963, -0.4982, -0.4972],\n           [-0.4969, -0.4996, -0.4975, -0.4962],\n           ...,\n           [-0.4998, -0.4974, -0.4983, -0.4985],\n           [-0.4994, -0.4980, -0.4985, -0.4993],\n           [-0.4976, -0.4979, -0.4997, -0.4962]],\n\n          [[-0.4986, -0.4966, -0.4983, -0.4997],\n           [-0.4963, -0.4982, -0.4974, -0.4981],\n           [-0.4989, -0.4988, -0.4998, -0.4994],\n           ...,\n           [-0.5000, -0.4982, -0.4982, -0.4971],\n           [-0.4985, -0.4976, -0.4989, -0.4989],\n           [-0.4974, -0.4983, -0.4985, -0.4989]],\n\n          [[-0.4993, -0.4985, -0.4970, -0.4969],\n           [-0.4976, -0.4990, -0.4998, -0.4971],\n           [-0.4996, -0.4987, -0.4985, -0.4984],\n           ...,\n           [-0.4989, -0.4970, -0.4983, -0.4968],\n           [-0.4963, -0.4995, -0.4974, -0.4980],\n           [-0.4999, -0.4987, -0.4980, -0.4969]],\n\n          ...,\n\n          [[-0.4995, -0.4984, -0.4977, -0.4993],\n           [-0.4990, -0.4993, -0.4996, -0.4964],\n           [-0.4965, -0.4988, -0.4970, -0.4965],\n           ...,\n           [-0.4965, -0.4967, -0.4988, -0.4980],\n           [-0.4981, -0.4966, -0.4977, -0.4982],\n           [-0.4983, -0.4983, -0.4984, -0.4975]],\n\n          [[-0.4968, -0.4986, -0.4991, -0.4999],\n           [-0.4985, -0.4979, -0.4984, -0.4983],\n           [-0.4988, -0.4978, -0.4975, -0.4979],\n           ...,\n           [-0.4967, -0.4997, -0.4998, -0.4982],\n           [-0.4975, -0.4996, -0.4997, -0.4961],\n           [-0.4962, -0.4989, -0.4979, -0.4970]],\n\n          [[-0.4976, -0.4983, -0.4968, -0.4991],\n           [-0.4967, -0.4984, -0.4963, -0.4965],\n           [-0.4988, -0.4975, -0.4962, -0.4965],\n           ...,\n           [-0.4986, -0.4991, -0.4993, -0.4978],\n           [-0.4974, -0.4963, -0.4992, -0.4979],\n           [-0.4997, -0.4994, -0.4997, -0.4966]]],\n\n\n         [[[-0.4976, -0.4997, -0.4983, -0.4998],\n           [-0.4983, -0.4988, -0.4976, -0.4968],\n           [-0.4972, -0.4972, -0.4998, -0.4999],\n           ...,\n           [-0.4966, -0.4973, -0.4990, -0.4969],\n           [-0.4967, -0.4991, -0.4979, -0.4977],\n           [-0.4998, -0.4973, -0.4992, -0.4992]],\n\n          [[-0.4991, -0.4977, -0.4979, -0.4976],\n           [-0.4967, -0.4969, -0.4977, -0.4967],\n           [-0.4970, -0.4967, -0.4981, -0.4962],\n           ...,\n           [-0.4999, -0.4967, -0.4972, -0.4993],\n           [-0.4990, -0.4965, -0.4967, -0.4968],\n           [-0.5000, -0.4965, -0.4973, -0.4990]],\n\n          [[-0.4971, -0.4973, -0.4982, -0.4981],\n           [-0.4992, -0.4983, -0.4983, -0.4969],\n           [-0.4987, -0.4993, -0.4977, -0.4999],\n           ...,\n           [-0.4974, -0.4968, -0.4995, -0.4966],\n           [-0.4973, -0.4985, -0.4993, -0.4992],\n           [-0.4968, -0.4992, -0.4985, -0.4991]],\n\n          ...,\n\n          [[-0.4979, -0.4963, -0.4979, -0.4978],\n           [-0.4982, -0.4986, -0.4988, -0.4968],\n           [-0.4970, -0.4964, -0.4996, -0.4973],\n           ...,\n           [-0.4993, -0.4984, -0.4965, -0.4978],\n           [-0.4990, -0.4984, -0.4970, -0.4985],\n           [-0.4999, -0.4980, -0.4966, -0.4965]],\n\n          [[-0.4994, -0.4974, -0.4974, -0.4975],\n           [-0.4996, -0.4989, -0.4991, -0.4969],\n           [-0.4974, -0.4965, -0.4992, -0.4986],\n           ...,\n           [-0.4993, -0.4965, -0.4999, -0.5000],\n           [-0.4991, -0.4982, -0.4964, -0.4991],\n           [-0.4968, -0.4967, -0.4988, -0.4996]],\n\n          [[-0.4990, -0.4972, -0.4970, -0.4993],\n           [-0.4981, -0.4991, -0.4989, -0.4968],\n           [-0.4974, -0.4961, -0.4970, -0.4962],\n           ...,\n           [-0.4982, -0.4992, -0.4971, -0.4971],\n           [-0.4982, -0.4970, -0.4988, -0.4999],\n           [-0.4977, -0.4990, -0.4992, -0.4981]]]],\n\n\n\n        [[[[-0.4996, -0.4981, -0.4973, -0.4983],\n           [-0.4999, -0.4968, -0.4977, -0.4990],\n           [-0.4990, -0.4983, -0.4990, -0.4995],\n           ...,\n           [-0.4963, -0.4964, -0.4975, -0.4967],\n           [-0.4987, -0.4997, -0.4974, -0.4982],\n           [-0.4982, -0.4977, -0.4975, -0.4985]],\n\n          [[-0.4986, -0.4972, -0.4990, -0.4976],\n           [-0.4962, -0.4991, -0.4989, -0.4994],\n           [-0.4994, -0.4962, -0.5000, -0.4984],\n           ...,\n           [-0.4988, -0.4964, -0.4977, -0.4963],\n           [-0.4986, -0.5000, -0.4996, -0.4966],\n           [-0.4981, -0.4976, -0.4974, -0.4963]],\n\n          [[-0.4967, -0.4970, -0.4977, -0.4964],\n           [-0.4985, -0.4962, -0.4973, -0.4991],\n           [-0.4999, -0.4962, -0.4999, -0.4973],\n           ...,\n           [-0.4971, -0.4962, -0.4991, -0.4968],\n           [-0.4964, -0.4991, -0.4993, -0.4968],\n           [-0.4966, -0.4971, -0.4976, -0.4994]],\n\n          ...,\n\n          [[-0.4990, -0.4994, -0.4971, -0.4997],\n           [-0.4982, -0.4966, -0.4993, -0.4969],\n           [-0.4986, -0.4983, -0.5000, -0.4980],\n           ...,\n           [-0.4981, -0.4968, -0.4984, -0.4998],\n           [-0.4999, -0.5000, -0.4983, -0.4979],\n           [-0.4984, -0.4965, -0.4991, -0.4994]],\n\n          [[-0.4965, -0.4993, -0.4962, -0.4983],\n           [-0.4986, -0.4980, -0.4974, -0.4994],\n           [-0.4975, -0.4995, -0.4965, -0.4988],\n           ...,\n           [-0.4992, -0.4994, -0.4964, -0.4992],\n           [-0.4981, -0.4981, -0.4998, -0.4964],\n           [-0.4975, -0.4972, -0.4961, -0.4978]],\n\n          [[-0.4994, -0.4971, -0.4998, -0.4979],\n           [-0.4991, -0.4969, -0.4998, -0.4993],\n           [-0.4963, -0.4980, -0.4988, -0.5000],\n           ...,\n           [-0.4977, -0.4976, -0.4970, -0.4994],\n           [-0.4980, -0.4973, -0.4998, -0.4997],\n           [-0.4982, -0.4978, -0.4996, -0.4982]]],\n\n\n         [[[-0.4964, -0.4990, -0.4975, -0.4967],\n           [-0.4981, -0.4972, -0.4995, -0.4990],\n           [-0.4962, -0.4996, -0.4999, -0.4962],\n           ...,\n           [-0.4981, -0.4988, -0.4978, -0.4968],\n           [-0.4969, -0.5000, -0.4965, -0.4991],\n           [-0.4972, -0.4975, -0.4976, -0.4983]],\n\n          [[-0.4961, -0.4967, -0.4988, -0.4994],\n           [-0.4996, -0.4965, -0.4982, -0.4981],\n           [-0.4974, -0.4979, -0.4985, -0.4984],\n           ...,\n           [-0.4986, -0.4996, -0.4971, -0.4996],\n           [-0.4967, -0.4984, -0.4990, -0.5000],\n           [-0.4989, -0.4996, -0.4973, -0.4993]],\n\n          [[-0.4969, -0.4987, -0.4982, -0.4977],\n           [-0.4977, -0.4962, -0.4980, -0.4980],\n           [-0.4967, -0.4967, -0.4981, -0.4995],\n           ...,\n           [-0.4965, -0.4963, -0.4975, -0.4984],\n           [-0.4981, -0.4991, -0.4985, -0.4992],\n           [-0.4962, -0.4980, -0.4982, -0.4984]],\n\n          ...,\n\n          [[-0.4974, -0.4987, -0.4981, -0.4983],\n           [-0.4977, -0.4980, -0.4984, -0.4969],\n           [-0.4965, -0.4998, -0.4963, -0.4988],\n           ...,\n           [-0.4988, -0.4992, -0.4979, -0.4961],\n           [-0.4974, -0.4990, -0.4985, -0.4985],\n           [-0.4971, -0.4997, -0.4977, -0.4971]],\n\n          [[-0.4973, -0.4984, -0.4986, -0.4978],\n           [-0.4983, -0.4992, -0.4995, -0.4969],\n           [-0.4985, -0.4992, -0.4996, -0.4969],\n           ...,\n           [-0.4982, -0.4976, -0.4977, -0.4977],\n           [-0.4998, -0.4976, -0.4999, -0.4975],\n           [-0.4980, -0.4990, -0.4980, -0.4997]],\n\n          [[-0.4973, -0.4979, -0.4999, -0.5000],\n           [-0.4992, -0.4986, -0.4968, -0.4990],\n           [-0.5000, -0.4989, -0.4993, -0.4970],\n           ...,\n           [-0.4973, -0.4967, -0.4961, -0.4980],\n           [-0.4985, -0.4997, -0.4975, -0.4968],\n           [-0.4983, -0.4993, -0.4971, -0.4975]]],\n\n\n         [[[-0.4970, -0.4999, -0.4980, -0.4977],\n           [-0.4996, -0.4981, -0.4999, -0.4993],\n           [-0.4981, -0.4964, -0.4974, -0.4980],\n           ...,\n           [-0.4979, -0.4966, -0.4982, -0.4978],\n           [-0.4989, -0.4966, -0.4964, -0.4988],\n           [-0.4972, -0.4976, -0.4988, -0.4973]],\n\n          [[-0.4972, -0.4976, -0.4986, -0.4994],\n           [-0.4985, -0.4970, -0.4995, -0.4971],\n           [-0.4971, -0.4997, -0.4977, -0.4975],\n           ...,\n           [-0.4962, -0.4998, -0.4994, -0.4993],\n           [-0.4975, -0.4997, -0.4971, -0.4987],\n           [-0.4967, -0.4966, -0.4990, -0.4998]],\n\n          [[-0.4986, -0.4990, -0.4964, -0.4993],\n           [-0.4973, -0.4992, -0.4983, -0.4967],\n           [-0.4964, -0.4974, -0.4984, -0.4996],\n           ...,\n           [-0.4991, -0.4988, -0.4961, -0.4994],\n           [-0.4990, -0.4989, -0.4996, -0.4996],\n           [-0.4989, -0.4979, -0.4994, -0.4984]],\n\n          ...,\n\n          [[-0.4968, -0.4962, -0.4987, -0.4977],\n           [-0.4984, -0.4969, -0.4962, -0.4994],\n           [-0.4996, -0.4996, -0.4980, -0.4970],\n           ...,\n           [-0.4971, -0.4980, -0.4976, -0.4981],\n           [-0.4993, -0.4989, -0.4976, -0.4985],\n           [-0.4990, -0.4976, -0.4975, -0.4970]],\n\n          [[-0.4978, -0.4977, -0.4997, -0.4986],\n           [-0.4995, -0.4978, -0.4986, -0.4972],\n           [-0.4995, -0.4985, -0.4968, -0.4999],\n           ...,\n           [-0.4983, -0.4972, -0.4981, -0.4968],\n           [-0.4977, -0.4967, -0.4996, -0.4992],\n           [-0.4962, -0.4996, -0.4973, -0.4996]],\n\n          [[-0.4990, -0.4969, -0.4997, -0.4965],\n           [-0.4991, -0.4992, -0.4995, -0.4995],\n           [-0.4973, -0.4980, -0.4972, -0.4991],\n           ...,\n           [-0.4964, -0.4990, -0.4993, -0.4994],\n           [-0.4967, -0.4969, -0.4965, -0.4981],\n           [-0.4967, -0.4979, -0.4993, -0.4965]]],\n\n\n         ...,\n\n\n         [[[-0.5000, -0.4965, -0.4993, -0.5000],\n           [-0.4973, -0.4982, -0.4965, -0.4999],\n           [-0.4967, -0.4964, -0.4998, -0.4967],\n           ...,\n           [-0.4981, -0.4992, -0.4984, -0.4976],\n           [-0.4964, -0.4996, -0.4967, -0.4979],\n           [-0.4980, -0.4984, -0.4995, -0.4978]],\n\n          [[-0.4998, -0.4966, -0.4998, -0.4988],\n           [-0.4989, -0.4976, -0.4976, -0.4996],\n           [-0.4978, -0.4997, -0.4994, -0.4991],\n           ...,\n           [-0.4963, -0.4962, -0.4965, -0.4974],\n           [-0.4985, -0.4965, -0.4980, -0.4985],\n           [-0.4989, -0.4996, -0.4977, -0.4967]],\n\n          [[-0.4996, -0.4995, -0.4992, -0.4993],\n           [-0.4984, -0.4964, -0.4964, -0.4973],\n           [-0.4977, -0.4978, -0.4983, -0.4987],\n           ...,\n           [-0.4995, -0.4964, -0.4992, -0.4996],\n           [-0.4965, -0.4991, -0.4991, -0.4998],\n           [-0.4967, -0.4981, -0.4964, -0.4989]],\n\n          ...,\n\n          [[-0.4992, -0.4971, -0.4974, -0.4985],\n           [-0.4992, -0.4967, -0.4999, -0.4996],\n           [-0.4969, -0.4977, -0.4974, -0.4995],\n           ...,\n           [-0.4978, -0.4987, -0.4980, -0.4984],\n           [-0.4987, -0.4984, -0.4968, -0.4980],\n           [-0.4971, -0.4996, -0.4988, -0.4995]],\n\n          [[-0.4984, -0.4988, -0.4999, -0.4991],\n           [-0.4984, -0.4979, -0.4966, -0.4975],\n           [-0.4964, -0.4992, -0.4973, -0.4962],\n           ...,\n           [-0.4981, -0.4980, -0.4990, -0.4987],\n           [-0.4966, -0.4968, -0.4962, -0.4994],\n           [-0.4972, -0.4976, -0.4990, -0.4969]],\n\n          [[-0.4979, -0.4967, -0.4972, -0.4994],\n           [-0.4961, -0.4988, -0.4985, -0.4978],\n           [-0.4969, -0.4984, -0.4962, -0.4967],\n           ...,\n           [-0.4981, -0.4989, -0.4968, -0.4989],\n           [-0.4989, -0.4962, -0.4995, -0.4988],\n           [-0.4985, -0.4971, -0.4986, -0.4993]]],\n\n\n         [[[-0.4994, -0.4996, -0.4981, -0.4974],\n           [-0.4986, -0.4971, -0.4981, -0.4997],\n           [-0.4973, -0.4978, -0.4992, -0.4965],\n           ...,\n           [-0.4988, -0.4997, -0.4965, -0.4998],\n           [-0.4995, -0.4998, -0.4967, -0.4997],\n           [-0.4980, -0.4985, -0.4975, -0.4963]],\n\n          [[-0.4999, -0.4989, -0.4993, -0.4970],\n           [-0.4981, -0.4969, -0.4961, -0.4973],\n           [-0.4987, -0.4964, -0.4994, -0.4979],\n           ...,\n           [-0.4989, -0.4967, -0.4989, -0.4975],\n           [-0.4988, -0.4986, -0.4984, -0.4997],\n           [-0.5000, -0.4980, -0.4985, -0.4963]],\n\n          [[-0.4987, -0.4967, -0.4989, -0.4985],\n           [-0.4982, -0.4965, -0.4975, -0.4999],\n           [-0.4966, -0.4961, -0.4974, -0.4982],\n           ...,\n           [-0.4971, -0.4973, -0.4990, -0.4983],\n           [-0.4971, -0.4970, -0.4978, -0.4978],\n           [-0.4998, -0.4981, -0.4967, -0.4987]],\n\n          ...,\n\n          [[-0.4984, -0.4973, -0.4977, -0.4965],\n           [-0.4991, -0.4992, -0.4964, -0.4965],\n           [-0.4973, -0.4978, -0.4969, -0.4963],\n           ...,\n           [-0.4965, -0.4994, -0.4978, -0.4997],\n           [-0.4986, -0.4966, -0.4979, -0.4978],\n           [-0.4961, -0.4981, -0.4995, -0.4966]],\n\n          [[-0.4994, -0.4972, -0.4963, -0.4995],\n           [-0.4966, -0.4981, -0.4973, -0.4968],\n           [-0.4963, -0.4966, -0.4966, -0.4998],\n           ...,\n           [-0.4961, -0.4977, -0.4985, -0.4974],\n           [-0.4999, -0.4992, -0.4974, -0.4970],\n           [-0.4973, -0.4996, -0.4991, -0.4965]],\n\n          [[-0.4976, -0.4973, -0.4993, -0.4991],\n           [-0.4996, -0.4991, -0.4965, -0.4975],\n           [-0.4962, -0.4963, -0.4990, -0.4992],\n           ...,\n           [-0.4999, -0.4967, -0.4971, -0.4989],\n           [-0.4963, -0.4985, -0.4978, -0.4988],\n           [-0.4979, -0.4984, -0.4988, -0.4997]]],\n\n\n         [[[-0.4962, -0.4970, -0.4962, -0.4996],\n           [-0.4985, -0.4963, -0.4981, -0.4966],\n           [-0.4969, -0.4989, -0.4970, -0.4975],\n           ...,\n           [-0.5000, -0.4995, -0.4974, -0.4982],\n           [-0.4965, -0.4962, -0.4993, -0.4962],\n           [-0.4975, -0.4974, -0.4980, -0.4962]],\n\n          [[-0.4984, -0.4985, -0.4998, -0.5000],\n           [-0.4971, -0.4991, -0.4963, -0.4980],\n           [-0.4999, -0.4988, -0.4999, -0.4963],\n           ...,\n           [-0.4976, -0.4994, -0.4984, -0.4988],\n           [-0.4997, -0.4990, -0.4986, -0.4987],\n           [-0.4966, -0.4989, -0.4985, -0.4975]],\n\n          [[-0.4976, -0.4975, -0.4996, -0.4973],\n           [-0.4987, -0.4978, -0.4996, -0.4988],\n           [-0.4972, -0.4986, -0.4971, -0.4974],\n           ...,\n           [-0.4971, -0.4978, -0.4963, -0.4981],\n           [-0.4969, -0.4987, -0.4970, -0.4964],\n           [-0.4974, -0.4989, -0.4991, -0.4973]],\n\n          ...,\n\n          [[-0.4982, -0.4996, -0.4967, -0.4970],\n           [-0.4984, -0.4991, -0.4988, -0.4965],\n           [-0.4993, -0.4966, -0.4986, -0.4975],\n           ...,\n           [-0.4975, -0.4992, -0.4964, -0.4967],\n           [-0.4978, -0.4987, -0.4997, -0.4971],\n           [-0.4986, -0.4973, -0.4983, -0.4961]],\n\n          [[-0.4978, -0.4994, -0.4971, -0.4973],\n           [-0.4983, -0.4988, -0.4992, -0.4983],\n           [-0.4988, -0.4999, -0.4961, -0.4982],\n           ...,\n           [-0.4967, -0.4991, -0.4985, -0.4982],\n           [-0.4967, -0.4993, -0.4983, -0.4963],\n           [-0.4966, -0.4993, -0.5000, -0.4980]],\n\n          [[-0.4994, -0.4992, -0.4977, -0.4985],\n           [-0.4976, -0.4973, -0.4969, -0.4996],\n           [-0.4971, -0.4989, -0.4961, -0.4991],\n           ...,\n           [-0.4981, -0.4976, -0.4981, -0.4987],\n           [-0.4983, -0.4978, -0.4977, -0.4996],\n           [-0.4972, -0.4966, -0.4971, -0.4968]]]]])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m batch_X, batch_y \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     67\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m     68\u001b[0m         \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m         logits \u001b[39m=\u001b[39m cnn(batch_X)\n\u001b[1;32m     70\u001b[0m         loss_value \u001b[39m=\u001b[39m loss_fn(batch_y, logits)\n\u001b[1;32m     72\u001b[0m     \u001b[39m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/conda3/envs/copy_env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[10], line 43\u001b[0m, in \u001b[0;36mCNN.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m---> 43\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(inputs)\n\u001b[1;32m     44\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool1(x)\n\u001b[1;32m     45\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Exception encountered when calling layer 'cnn_7' (type CNN).\n\n'torch.Size' object has no attribute 'rank'\n\nCall arguments received by layer 'cnn_7' (type CNN):\n  • inputs=tensor([[[[[-0.4976, -0.4994, -0.4994, -0.4962],\n           [-0.4993, -0.4994, -0.4965, -0.4983],\n           [-0.4992, -0.4986, -0.4972, -0.5000],\n           ...,\n           [-0.4983, -0.4969, -0.4986, -0.4962],\n           [-0.4977, -0.4991, -0.4973, -0.4964],\n           [-0.4991, -0.4979, -0.4993, -0.4977]],\n\n          [[-0.4987, -0.4995, -0.4986, -0.4986],\n           [-0.4982, -0.4987, -0.4991, -0.4973],\n           [-0.4967, -0.4962, -0.4962, -0.4966],\n           ...,\n           [-0.4991, -0.4996, -0.4978, -0.4990],\n           [-0.4985, -0.4998, -0.4968, -0.4984],\n           [-0.4976, -0.4997, -0.4989, -0.4978]],\n\n          [[-0.4983, -0.4987, -0.4984, -0.4997],\n           [-0.4990, -0.4962, -0.4994, -0.4986],\n           [-0.4965, -0.4979, -0.4978, -0.4965],\n           ...,\n           [-0.4962, -0.4990, -0.4991, -0.4963],\n           [-0.4963, -0.4985, -0.4983, -0.4979],\n           [-0.4961, -0.4987, -0.4991, -0.4991]],\n\n          ...,\n\n          [[-0.4964, -0.4983, -0.4965, -0.4967],\n           [-0.4976, -0.4964, -0.4963, -0.4981],\n           [-0.4997, -0.4980, -0.4972, -0.4982],\n           ...,\n           [-0.4977, -0.4987, -0.4982, -0.4987],\n           [-0.4961, -0.4977, -0.4973, -0.4987],\n           [-0.4983, -0.5000, -0.4999, -0.4990]],\n\n          [[-0.4993, -0.4980, -0.4984, -0.4980],\n           [-0.4984, -0.4985, -0.4986, -0.4970],\n           [-0.4976, -0.4966, -0.4964, -0.4995],\n           ...,\n           [-0.4993, -0.4995, -0.4999, -0.4998],\n           [-0.4964, -0.4981, -0.4974, -0.4962],\n           [-0.4994, -0.4977, -0.4989, -0.4979]],\n\n          [[-0.4967, -0.4985, -0.4991, -0.4996],\n           [-0.4973, -0.4992, -0.4972, -0.4992],\n           [-0.4978, -0.4967, -0.4964, -0.4993],\n           ...,\n           [-0.4990, -0.4963, -0.4990, -0.4979],\n           [-0.4973, -0.4994, -0.4965, -0.4999],\n           [-0.4987, -0.4969, -0.4963, -0.4968]]],\n\n\n         [[[-0.4974, -0.4991, -0.4971, -0.4972],\n           [-0.4997, -0.5000, -0.4974, -0.4999],\n           [-0.4962, -0.4982, -0.4962, -0.4969],\n           ...,\n           [-0.4966, -0.4988, -0.4996, -0.4976],\n           [-0.4976, -0.4984, -0.4969, -0.4987],\n           [-0.4984, -0.4990, -0.4981, -0.4979]],\n\n          [[-0.4987, -0.4970, -0.4983, -0.4991],\n           [-0.4976, -0.4993, -0.4984, -0.4973],\n           [-0.4995, -0.4982, -0.4986, -0.4966],\n           ...,\n           [-0.4969, -0.4994, -0.4962, -0.4988],\n           [-0.4966, -0.4981, -0.4994, -0.4972],\n           [-0.4962, -0.4980, -0.4971, -0.4964]],\n\n          [[-0.4981, -0.4970, -0.4969, -0.4987],\n           [-0.4986, -0.4996, -0.4976, -0.4963],\n           [-0.4984, -0.4982, -0.4971, -0.4980],\n           ...,\n           [-0.4992, -0.4968, -0.4962, -0.4981],\n           [-0.4980, -0.4981, -0.4982, -0.4999],\n           [-0.4989, -0.4983, -0.4996, -0.4999]],\n\n          ...,\n\n          [[-0.4961, -0.4986, -0.4994, -0.4968],\n           [-0.4987, -0.4988, -0.5000, -0.4990],\n           [-0.4968, -0.4988, -0.4976, -0.4965],\n           ...,\n           [-0.4997, -0.4979, -0.4978, -0.4982],\n           [-0.4987, -0.4991, -0.4966, -0.4981],\n           [-0.4972, -0.4980, -0.4966, -0.4992]],\n\n          [[-0.4996, -0.4964, -0.4983, -0.4974],\n           [-0.4987, -0.4978, -0.4963, -0.4999],\n           [-0.4977, -0.4984, -0.4972, -0.4999],\n           ...,\n           [-0.4968, -0.4962, -0.4977, -0.4977],\n           [-0.4980, -0.4995, -0.4994, -0.4982],\n           [-0.4981, -0.4973, -0.4997, -0.4976]],\n\n          [[-0.4988, -0.4971, -0.4999, -0.4976],\n           [-0.4982, -0.4984, -0.4973, -0.4979],\n           [-0.4991, -0.4988, -0.4974, -0.4981],\n           ...,\n           [-0.4963, -0.4999, -0.4986, -0.4997],\n           [-0.4994, -0.4970, -0.4975, -0.4984],\n           [-0.4995, -0.4966, -0.4991, -0.4967]]],\n\n\n         [[[-0.4996, -0.4999, -0.4985, -0.4992],\n           [-0.4988, -0.5000, -0.4993, -0.4994],\n           [-0.4972, -0.4996, -0.4997, -0.4967],\n           ...,\n           [-0.4980, -0.4981, -0.4971, -0.4967],\n           [-0.4968, -0.4962, -0.4973, -0.4984],\n           [-0.4980, -0.4998, -0.4996, -0.4993]],\n\n          [[-0.4991, -0.4964, -0.4998, -0.4987],\n           [-0.4991, -0.4995, -0.4998, -0.4964],\n           [-0.4980, -0.4975, -0.4979, -0.4999],\n           ...,\n           [-0.4985, -0.4986, -0.4973, -0.4991],\n           [-0.4999, -0.4962, -0.4964, -0.4973],\n           [-0.4984, -0.4967, -0.4982, -0.4979]],\n\n          [[-0.4991, -0.4981, -0.4963, -0.4986],\n           [-0.4984, -0.4989, -0.4972, -0.4964],\n           [-0.4995, -0.4996, -0.4993, -0.4995],\n           ...,\n           [-0.4968, -0.4982, -0.4967, -0.4981],\n           [-0.4992, -0.4983, -0.4983, -0.4976],\n           [-0.4980, -0.4969, -0.4983, -0.4982]],\n\n          ...,\n\n          [[-0.4987, -0.4969, -0.4979, -0.4969],\n           [-0.4984, -0.4974, -0.4980, -0.4992],\n           [-0.4966, -0.4986, -0.4962, -0.4996],\n           ...,\n           [-0.4994, -0.4978, -0.4973, -0.4989],\n           [-0.4990, -0.4980, -0.4999, -0.4973],\n           [-0.4992, -0.4962, -0.4989, -0.4962]],\n\n          [[-0.4978, -0.4987, -0.4967, -0.4967],\n           [-0.4979, -0.4982, -0.4967, -0.4982],\n           [-0.4964, -0.4973, -0.4992, -0.4970],\n           ...,\n           [-0.4973, -0.4964, -0.4963, -0.4987],\n           [-0.4983, -0.4984, -0.4969, -0.4992],\n           [-0.4999, -0.4986, -0.4995, -0.4990]],\n\n          [[-0.4976, -0.4997, -0.4965, -0.4975],\n           [-0.4975, -0.4982, -0.4968, -0.4988],\n           [-0.4999, -0.4992, -0.4986, -0.4972],\n           ...,\n           [-0.4990, -0.4980, -0.4971, -0.4994],\n           [-0.4985, -0.4982, -0.4968, -0.4989],\n           [-0.4980, -0.4962, -0.4980, -0.4968]]],\n\n\n         ...,\n\n\n         [[[-0.4972, -0.4963, -0.4986, -0.4967],\n           [-0.4985, -0.4966, -0.4974, -0.4964],\n           [-0.4983, -0.4988, -0.4968, -0.4986],\n           ...,\n           [-0.4963, -0.4998, -0.4972, -0.4969],\n           [-0.4980, -0.4967, -0.4975, -0.4977],\n           [-0.4995, -0.4979, -0.4984, -0.4986]],\n\n          [[-0.4970, -0.4979, -0.4982, -0.4988],\n           [-0.4982, -0.4969, -0.4972, -0.4961],\n           [-0.4965, -0.4987, -0.4962, -0.4981],\n           ...,\n           [-0.4986, -0.4996, -0.4981, -0.4982],\n           [-0.4989, -0.4998, -0.4998, -0.4982],\n           [-0.4970, -0.4972, -0.4965, -0.4996]],\n\n          [[-0.4963, -0.4970, -0.4965, -0.4993],\n           [-0.4968, -0.4991, -0.4989, -0.4987],\n           [-0.4987, -0.4973, -0.4975, -0.4996],\n           ...,\n           [-0.4978, -0.4996, -0.4996, -0.4980],\n           [-0.4966, -0.4973, -0.4990, -0.4976],\n           [-0.5000, -0.4993, -0.4988, -0.4978]],\n\n          ...,\n\n          [[-0.4980, -0.4964, -0.4983, -0.4999],\n           [-0.4993, -0.4994, -0.4991, -0.4975],\n           [-0.4969, -0.4991, -0.4991, -0.4968],\n           ...,\n           [-0.4970, -0.4968, -0.4978, -0.4997],\n           [-0.4961, -0.4973, -0.4995, -0.4999],\n           [-0.4984, -0.4983, -0.4969, -0.4991]],\n\n          [[-0.4984, -0.4965, -0.4996, -0.4985],\n           [-0.4964, -0.4986, -0.4967, -0.4971],\n           [-0.5000, -0.4961, -0.4971, -0.4992],\n           ...,\n           [-0.4983, -0.4966, -0.4984, -0.4963],\n           [-0.4986, -0.4997, -0.4988, -0.4969],\n           [-0.4998, -0.4998, -0.4975, -0.4971]],\n\n          [[-0.4978, -0.4981, -0.4985, -0.4981],\n           [-0.4966, -0.4967, -0.4984, -0.4981],\n           [-0.4982, -0.4995, -0.4997, -0.4989],\n           ...,\n           [-0.4998, -0.4979, -0.4983, -0.4973],\n           [-0.4968, -0.4973, -0.4986, -0.4964],\n           [-0.4964, -0.4969, -0.4986, -0.5000]]],\n\n\n         [[[-0.4990, -0.4987, -0.4986, -0.4967],\n           [-0.4974, -0.4971, -0.4981, -0.4971],\n           [-0.4987, -0.4976, -0.4977, -0.4994],\n           ...,\n           [-0.4980, -0.5000, -0.4979, -0.4964],\n           [-0.4992, -0.4988, -0.4976, -0.4965],\n           [-0.4989, -0.4998, -0.4986, -0.4995]],\n\n          [[-0.4976, -0.4976, -0.4987, -0.4961],\n           [-0.4962, -0.4965, -0.4977, -0.4980],\n           [-0.4987, -0.4987, -0.4970, -0.4993],\n           ...,\n           [-0.4972, -0.4981, -0.4973, -0.4970],\n           [-0.4979, -0.4970, -0.4972, -0.4975],\n           [-0.4990, -0.4975, -0.4973, -0.4987]],\n\n          [[-0.4963, -0.4966, -0.4974, -0.4979],\n           [-0.4984, -0.4982, -0.4978, -0.4987],\n           [-0.4993, -0.4979, -0.4963, -0.4997],\n           ...,\n           [-0.4999, -0.4992, -0.4990, -0.4965],\n           [-0.4976, -0.4981, -0.4980, -0.4989],\n           [-0.4969, -0.4995, -0.4998, -0.4964]],\n\n          ...,\n\n          [[-0.4984, -0.4981, -0.4981, -0.4999],\n           [-0.4966, -0.4963, -0.4997, -0.4998],\n           [-0.4989, -0.4997, -0.4982, -0.4993],\n           ...,\n           [-0.4977, -0.4971, -0.4999, -0.4980],\n           [-0.4979, -0.4999, -0.4986, -0.4963],\n           [-0.4971, -0.4986, -0.4978, -0.4992]],\n\n          [[-0.4964, -0.4981, -0.4987, -0.4972],\n           [-0.4985, -0.4994, -0.4985, -0.4974],\n           [-0.4964, -0.4984, -0.4963, -0.4995],\n           ...,\n           [-0.4983, -0.4989, -0.4961, -0.4965],\n           [-0.4979, -0.4982, -0.4999, -0.4987],\n           [-0.4993, -0.4965, -0.4968, -0.4971]],\n\n          [[-0.4996, -0.4998, -0.4973, -0.4994],\n           [-0.4985, -0.4982, -0.4968, -0.4983],\n           [-0.4992, -0.4983, -0.4976, -0.4993],\n           ...,\n           [-0.4992, -0.4984, -0.4963, -0.4988],\n           [-0.4980, -0.4986, -0.4976, -0.4990],\n           [-0.4979, -0.4995, -0.4966, -0.4997]]],\n\n\n         [[[-0.4962, -0.4964, -0.4967, -0.5000],\n           [-0.4962, -0.4976, -0.4996, -0.4965],\n           [-0.4968, -0.4972, -0.4973, -0.4970],\n           ...,\n           [-0.4997, -0.4963, -0.4985, -0.4963],\n           [-0.4967, -0.4990, -0.4985, -0.4987],\n           [-0.4997, -0.4998, -0.4978, -0.4976]],\n\n          [[-0.4997, -0.5000, -0.5000, -0.4976],\n           [-0.4985, -0.4961, -0.4982, -0.4971],\n           [-0.4996, -0.4993, -0.4963, -0.4981],\n           ...,\n           [-0.4988, -0.4987, -0.4971, -0.4975],\n           [-0.4993, -0.4982, -0.4977, -0.4967],\n           [-0.4964, -0.4972, -0.4995, -0.4986]],\n\n          [[-0.4977, -0.4991, -0.4972, -0.4966],\n           [-0.4993, -0.4996, -0.4985, -0.4971],\n           [-0.4991, -0.4978, -0.4962, -0.4999],\n           ...,\n           [-0.4986, -0.4972, -0.4979, -0.4965],\n           [-0.4979, -0.4966, -0.4972, -0.4967],\n           [-0.4970, -0.4972, -0.4962, -0.4990]],\n\n          ...,\n\n          [[-0.4971, -0.4979, -0.4988, -0.4981],\n           [-0.4969, -0.4975, -0.4991, -0.4962],\n           [-0.4966, -0.4983, -0.4984, -0.4966],\n           ...,\n           [-0.4978, -0.4968, -0.4999, -0.4997],\n           [-0.4979, -0.4993, -0.4978, -0.4987],\n           [-0.4982, -0.4972, -0.4974, -0.4993]],\n\n          [[-0.4963, -0.4996, -0.4964, -0.4986],\n           [-0.4991, -0.4975, -0.4965, -0.4967],\n           [-0.4978, -0.4991, -0.4990, -0.4963],\n           ...,\n           [-0.4966, -0.4978, -0.4996, -0.4979],\n           [-0.4990, -0.4964, -0.4974, -0.4972],\n           [-0.4982, -0.4991, -0.4980, -0.4972]],\n\n          [[-0.4977, -0.4978, -0.4979, -0.4969],\n           [-0.4999, -0.4988, -0.4965, -0.4983],\n           [-0.4986, -0.4982, -0.4969, -0.4995],\n           ...,\n           [-0.4964, -0.4968, -0.4989, -0.4975],\n           [-0.4995, -0.4976, -0.4998, -0.4979],\n           [-0.4985, -0.4974, -0.4978, -0.4998]]]],\n\n\n\n        [[[[-0.4978, -0.4995, -0.4982, -0.4983],\n           [-0.4976, -0.4986, -0.4999, -0.4983],\n           [-0.4977, -0.4994, -0.4976, -0.4966],\n           ...,\n           [-0.4987, -0.4991, -0.4979, -0.4963],\n           [-0.4988, -0.4995, -0.4995, -0.4996],\n           [-0.4994, -0.4969, -0.4982, -0.4969]],\n\n          [[-0.4974, -0.4982, -0.4982, -0.4997],\n           [-0.4987, -0.4965, -0.4979, -0.4980],\n           [-0.4974, -0.4964, -0.4997, -0.4971],\n           ...,\n           [-0.4980, -0.4976, -0.4968, -0.4968],\n           [-0.4990, -0.4987, -0.4973, -0.4981],\n           [-0.4982, -0.4962, -0.4976, -0.4968]],\n\n          [[-0.4975, -0.4990, -0.4962, -0.4970],\n           [-0.4966, -0.4975, -0.4979, -0.4977],\n           [-0.4965, -0.4965, -0.4967, -0.4963],\n           ...,\n           [-0.4991, -0.4988, -0.4993, -0.4978],\n           [-0.4969, -0.4962, -0.4999, -0.4987],\n           [-0.4973, -0.4969, -0.4979, -0.4981]],\n\n          ...,\n\n          [[-0.4987, -0.4969, -0.4967, -0.4991],\n           [-0.4992, -0.4979, -0.4972, -0.4987],\n           [-0.4991, -0.4971, -0.4995, -0.4975],\n           ...,\n           [-0.4993, -0.4999, -0.4991, -0.4964],\n           [-0.4970, -0.4970, -0.4984, -0.4967],\n           [-0.4963, -0.4983, -0.4974, -0.4990]],\n\n          [[-0.4980, -0.4993, -0.4965, -0.4973],\n           [-0.4961, -0.4967, -0.4971, -0.4981],\n           [-0.4991, -0.4964, -0.4975, -0.4982],\n           ...,\n           [-0.4961, -0.4999, -0.4991, -0.4988],\n           [-0.4966, -0.4982, -0.4964, -0.4975],\n           [-0.4999, -0.4988, -0.4969, -0.4973]],\n\n          [[-0.4973, -0.4982, -0.4992, -0.4980],\n           [-0.4967, -0.4968, -0.4970, -0.4986],\n           [-0.4965, -0.4988, -0.4977, -0.4984],\n           ...,\n           [-0.4972, -0.4985, -0.4993, -0.4988],\n           [-0.4971, -0.4987, -0.4973, -0.4971],\n           [-0.4999, -0.4994, -0.4973, -0.4962]]],\n\n\n         [[[-0.4977, -0.4985, -0.4997, -0.4976],\n           [-0.4981, -0.4995, -0.4976, -0.4974],\n           [-0.4994, -0.4982, -0.4992, -0.4976],\n           ...,\n           [-0.4970, -0.4978, -0.4969, -0.4965],\n           [-0.4970, -0.4973, -0.4991, -0.4971],\n           [-0.4998, -0.4979, -0.4990, -0.4990]],\n\n          [[-0.4973, -0.4978, -0.4981, -0.4984],\n           [-0.4978, -0.4982, -0.4965, -0.4964],\n           [-0.4992, -0.4996, -0.4982, -0.4975],\n           ...,\n           [-0.4983, -0.4973, -0.4986, -0.4974],\n           [-0.4977, -0.4979, -0.4984, -0.4962],\n           [-0.4981, -0.4968, -0.4984, -0.4982]],\n\n          [[-0.4968, -0.4988, -0.4988, -0.4967],\n           [-0.4995, -0.4999, -0.4994, -0.4963],\n           [-0.4974, -0.4985, -0.4990, -0.4986],\n           ...,\n           [-0.4998, -0.4989, -0.4999, -0.4969],\n           [-0.4989, -0.4964, -0.4964, -0.4965],\n           [-0.4991, -0.4990, -0.4973, -0.4985]],\n\n          ...,\n\n          [[-0.4971, -0.4983, -0.4961, -0.4963],\n           [-0.4978, -0.4961, -0.4975, -0.4975],\n           [-0.4988, -0.4974, -0.4987, -0.4970],\n           ...,\n           [-0.4970, -0.4986, -0.4980, -0.4994],\n           [-0.4962, -0.4977, -0.4966, -0.4967],\n           [-0.4977, -0.5000, -0.4974, -0.4987]],\n\n          [[-0.4964, -0.4973, -0.4990, -0.4979],\n           [-0.4964, -0.4973, -0.4991, -0.4982],\n           [-0.4997, -0.4993, -0.4962, -0.4994],\n           ...,\n           [-0.4976, -0.4975, -0.5000, -0.4979],\n           [-0.4976, -0.4992, -0.4977, -0.4962],\n           [-0.4992, -0.4998, -0.4989, -0.4963]],\n\n          [[-0.4990, -0.4969, -0.4980, -0.4986],\n           [-0.4991, -0.4992, -0.4983, -0.4996],\n           [-0.4973, -0.4972, -0.4980, -0.4983],\n           ...,\n           [-0.4999, -0.4963, -0.4974, -0.4986],\n           [-0.4962, -0.4987, -0.4990, -0.4987],\n           [-0.4975, -0.4984, -0.4989, -0.4975]]],\n\n\n         [[[-0.4974, -0.4991, -0.4972, -0.4965],\n           [-0.4989, -0.4993, -0.4972, -0.4972],\n           [-0.4963, -0.5000, -0.4971, -0.4961],\n           ...,\n           [-0.4995, -0.4992, -0.4996, -0.4974],\n           [-0.4998, -0.4984, -0.4980, -0.4978],\n           [-0.4987, -0.4979, -0.4974, -0.4963]],\n\n          [[-0.4989, -0.4961, -0.4974, -0.4994],\n           [-0.4987, -0.4967, -0.4975, -0.4973],\n           [-0.4985, -0.4999, -0.4988, -0.4965],\n           ...,\n           [-0.4963, -0.4989, -0.4963, -0.4961],\n           [-0.4977, -0.4971, -0.4965, -0.4966],\n           [-0.4966, -0.4969, -0.5000, -0.4961]],\n\n          [[-0.4990, -0.4970, -0.4968, -0.4981],\n           [-0.4991, -0.4977, -0.4974, -0.4991],\n           [-0.4999, -0.4962, -0.4992, -0.4964],\n           ...,\n           [-0.4965, -0.4975, -0.4997, -0.4988],\n           [-0.4976, -0.4992, -0.4990, -0.4968],\n           [-0.4999, -0.4979, -0.4971, -0.4970]],\n\n          ...,\n\n          [[-0.4974, -0.5000, -0.4987, -0.4967],\n           [-0.4961, -0.4968, -0.4976, -0.4968],\n           [-0.4970, -0.4975, -0.4982, -0.4963],\n           ...,\n           [-0.4980, -0.4983, -0.4965, -0.4979],\n           [-0.4976, -0.4980, -0.4991, -0.4977],\n           [-0.4969, -0.4981, -0.4965, -0.4989]],\n\n          [[-0.4970, -0.4986, -0.4986, -0.4966],\n           [-0.4998, -0.4998, -0.4996, -0.4964],\n           [-0.4964, -0.4988, -0.4990, -0.4975],\n           ...,\n           [-0.4992, -0.4997, -0.4983, -0.4985],\n           [-0.4989, -0.4980, -0.4985, -0.4984],\n           [-0.4998, -0.4963, -0.4979, -0.4980]],\n\n          [[-0.4983, -0.4983, -0.4971, -0.4982],\n           [-0.4997, -0.4988, -0.4976, -0.4978],\n           [-0.4994, -0.4990, -0.4999, -0.5000],\n           ...,\n           [-0.4979, -0.4985, -0.4992, -0.4977],\n           [-0.4971, -0.4994, -0.4968, -0.4999],\n           [-0.4989, -0.4975, -0.4964, -0.4977]]],\n\n\n         ...,\n\n\n         [[[-0.4964, -0.4984, -0.4992, -0.4979],\n           [-0.4980, -0.4970, -0.4970, -0.4979],\n           [-0.4994, -0.4971, -0.4999, -0.4982],\n           ...,\n           [-0.4981, -0.4974, -0.4981, -0.4980],\n           [-0.4999, -0.4975, -0.4990, -0.4984],\n           [-0.4971, -0.4972, -0.4991, -0.4970]],\n\n          [[-0.4962, -0.4963, -0.4984, -0.4965],\n           [-0.4973, -0.4986, -0.4961, -0.4995],\n           [-0.4980, -0.5000, -0.5000, -0.4995],\n           ...,\n           [-0.4982, -0.4968, -0.4963, -0.4974],\n           [-0.4977, -0.4986, -0.4999, -0.4975],\n           [-0.4969, -0.4967, -0.4964, -0.4989]],\n\n          [[-0.4995, -0.4983, -0.4970, -0.4993],\n           [-0.4988, -0.4993, -0.4972, -0.4964],\n           [-0.4982, -0.4963, -0.4972, -0.4995],\n           ...,\n           [-0.4981, -0.4993, -0.4964, -0.4988],\n           [-0.4989, -0.4965, -0.4997, -0.4964],\n           [-0.4986, -0.4995, -0.4982, -0.4982]],\n\n          ...,\n\n          [[-0.4969, -0.4965, -0.4964, -0.4964],\n           [-0.4964, -0.4992, -0.4998, -0.4961],\n           [-0.4978, -0.4996, -0.4979, -0.4983],\n           ...,\n           [-0.4971, -0.4971, -0.4976, -0.4972],\n           [-0.4974, -0.4976, -0.4961, -0.4964],\n           [-0.4966, -0.4983, -0.4963, -0.4996]],\n\n          [[-0.4985, -0.4978, -0.4983, -0.4998],\n           [-0.4993, -0.4984, -0.4963, -0.4975],\n           [-0.4967, -0.4970, -0.4997, -0.4996],\n           ...,\n           [-0.4971, -0.4969, -0.4970, -0.4962],\n           [-0.4980, -0.4988, -0.4991, -0.4980],\n           [-0.4983, -0.4992, -0.4968, -0.4997]],\n\n          [[-0.4973, -0.4972, -0.4971, -0.4990],\n           [-0.4975, -0.4991, -0.4965, -0.4983],\n           [-0.4989, -0.4983, -0.4965, -0.4961],\n           ...,\n           [-0.4962, -0.4991, -0.4963, -0.4982],\n           [-0.4978, -0.4998, -0.4967, -0.4997],\n           [-0.4995, -0.4963, -0.4994, -0.4970]]],\n\n\n         [[[-0.4981, -0.4977, -0.4995, -0.4985],\n           [-0.4972, -0.4972, -0.4968, -0.4984],\n           [-0.4978, -0.4998, -0.4983, -0.4964],\n           ...,\n           [-0.4999, -0.4978, -0.4994, -0.4980],\n           [-0.4964, -0.4997, -0.4997, -0.4972],\n           [-0.4993, -0.4984, -0.4997, -0.4974]],\n\n          [[-0.4972, -0.4999, -0.4997, -0.4977],\n           [-0.4971, -0.4965, -0.4964, -0.4981],\n           [-0.4979, -0.4978, -0.4964, -0.4972],\n           ...,\n           [-0.4986, -0.4974, -0.4997, -0.4987],\n           [-0.4964, -0.4994, -0.4982, -0.4967],\n           [-0.4979, -0.4978, -0.4985, -0.4981]],\n\n          [[-0.4994, -0.4972, -0.4978, -0.4978],\n           [-0.4989, -0.4978, -0.4969, -0.4974],\n           [-0.4999, -0.4967, -0.4999, -0.4975],\n           ...,\n           [-0.4990, -0.4978, -0.4990, -0.4982],\n           [-0.4987, -0.4972, -0.4983, -0.4963],\n           [-0.4973, -0.4964, -0.4992, -0.4966]],\n\n          ...,\n\n          [[-0.4971, -0.4973, -0.4966, -0.4965],\n           [-0.4997, -0.4987, -0.4985, -0.4981],\n           [-0.4974, -0.4980, -0.4981, -0.4994],\n           ...,\n           [-0.4980, -0.4990, -0.4998, -0.4998],\n           [-0.4981, -0.4985, -0.4985, -0.4978],\n           [-0.4979, -0.4993, -0.4993, -0.4995]],\n\n          [[-0.4996, -0.4999, -0.4990, -0.4977],\n           [-0.4973, -0.4964, -0.4973, -0.4964],\n           [-0.4988, -0.4979, -0.4967, -0.4977],\n           ...,\n           [-0.4996, -0.4981, -0.5000, -0.4997],\n           [-0.4988, -0.4977, -0.4978, -0.4968],\n           [-0.4971, -0.4995, -0.4964, -0.4990]],\n\n          [[-0.4971, -0.4974, -0.4974, -0.4975],\n           [-0.4982, -0.4993, -0.4995, -0.4972],\n           [-0.4996, -0.4993, -0.4974, -0.4998],\n           ...,\n           [-0.4993, -0.4971, -0.4984, -0.4982],\n           [-0.4978, -0.4962, -0.4998, -0.4987],\n           [-0.4971, -0.4966, -0.4969, -0.4976]]],\n\n\n         [[[-0.4995, -0.4992, -0.4963, -0.4979],\n           [-0.4962, -0.4971, -0.4970, -0.4985],\n           [-0.4979, -0.4981, -0.4968, -0.4965],\n           ...,\n           [-0.4976, -0.4973, -0.4996, -0.4994],\n           [-0.4983, -0.4989, -0.4978, -0.4968],\n           [-0.4990, -0.4987, -0.4992, -0.4981]],\n\n          [[-0.4993, -0.4998, -0.4976, -0.4965],\n           [-0.4991, -0.4991, -0.4980, -0.4975],\n           [-0.4984, -0.4984, -0.4966, -0.4999],\n           ...,\n           [-0.4993, -0.4988, -0.4991, -0.4990],\n           [-0.4980, -0.4983, -0.4961, -0.4997],\n           [-0.4988, -0.4987, -0.4993, -0.4996]],\n\n          [[-0.4978, -0.4984, -0.4969, -0.4992],\n           [-0.4980, -0.4984, -0.4992, -0.4994],\n           [-0.4976, -0.4982, -0.4984, -0.4976],\n           ...,\n           [-0.4997, -0.4998, -0.4999, -0.4987],\n           [-0.4974, -0.4966, -0.4979, -0.4982],\n           [-0.4999, -0.4982, -0.4981, -0.4986]],\n\n          ...,\n\n          [[-0.4980, -0.4978, -0.4982, -0.4991],\n           [-0.4978, -0.4978, -0.4975, -0.4966],\n           [-0.4968, -0.4989, -0.4971, -0.4997],\n           ...,\n           [-0.4967, -0.4979, -0.4996, -0.4967],\n           [-0.4978, -0.4972, -0.4976, -0.4986],\n           [-0.4973, -0.4994, -0.4999, -0.4988]],\n\n          [[-0.4993, -0.4975, -0.4982, -0.4968],\n           [-0.5000, -0.4992, -0.4961, -0.4972],\n           [-0.4977, -0.4972, -0.4965, -0.4970],\n           ...,\n           [-0.4980, -0.4988, -0.4973, -0.4968],\n           [-0.4977, -0.4968, -0.4995, -0.4976],\n           [-0.4965, -0.4997, -0.4974, -0.4996]],\n\n          [[-0.4986, -0.4978, -0.4991, -0.4991],\n           [-0.4997, -0.4995, -0.4990, -0.4982],\n           [-0.4976, -0.4987, -0.4968, -0.4990],\n           ...,\n           [-0.4966, -0.4969, -0.4976, -0.4999],\n           [-0.4984, -0.4983, -0.4993, -0.4968],\n           [-0.4989, -0.4987, -0.4978, -0.4964]]]],\n\n\n\n        [[[[-0.4976, -0.4966, -0.4968, -0.4978],\n           [-0.4971, -0.4972, -0.4989, -0.4984],\n           [-0.4972, -0.4992, -0.4967, -0.4968],\n           ...,\n           [-0.4986, -0.4972, -0.4991, -0.4974],\n           [-0.4990, -0.4993, -0.4989, -0.4973],\n           [-0.4990, -0.4980, -0.4980, -0.4991]],\n\n          [[-0.4998, -0.4977, -0.4964, -0.4998],\n           [-0.4983, -0.4971, -0.4999, -0.5000],\n           [-0.4977, -0.4975, -0.4983, -0.4995],\n           ...,\n           [-0.4965, -0.4989, -0.4961, -0.4986],\n           [-0.4967, -0.4970, -0.4991, -0.4991],\n           [-0.4967, -0.4970, -0.4968, -0.4993]],\n\n          [[-0.4992, -0.4976, -0.4982, -0.4970],\n           [-0.4978, -0.4995, -0.4976, -0.4989],\n           [-0.4993, -0.4965, -0.4985, -0.4970],\n           ...,\n           [-0.4986, -0.4971, -0.4965, -0.4982],\n           [-0.4983, -0.4981, -0.4984, -0.4991],\n           [-0.4980, -0.4995, -0.4967, -0.4989]],\n\n          ...,\n\n          [[-0.4963, -0.4963, -0.4989, -0.4999],\n           [-0.4994, -0.4962, -0.4995, -0.4984],\n           [-0.4982, -0.4994, -0.4966, -0.4973],\n           ...,\n           [-0.4974, -0.4963, -0.4972, -0.4982],\n           [-0.4962, -0.4961, -0.4972, -0.4978],\n           [-0.4979, -0.4985, -0.4992, -0.4991]],\n\n          [[-0.4998, -0.4989, -0.4989, -0.4983],\n           [-0.4968, -0.4977, -0.4966, -0.4987],\n           [-0.4982, -0.4994, -0.4980, -0.4994],\n           ...,\n           [-0.4971, -0.4998, -0.4964, -0.4972],\n           [-0.4971, -0.4980, -0.4968, -0.4997],\n           [-0.4996, -0.4983, -0.4994, -0.4995]],\n\n          [[-0.4996, -0.4984, -0.4967, -0.4991],\n           [-0.4964, -0.4991, -0.4972, -0.4979],\n           [-0.4991, -0.4978, -0.4998, -0.4997],\n           ...,\n           [-0.4982, -0.4995, -0.4976, -0.4963],\n           [-0.4995, -0.4964, -0.4979, -0.4987],\n           [-0.4988, -0.4980, -0.4974, -0.4966]]],\n\n\n         [[[-0.4984, -0.4986, -0.4993, -0.4977],\n           [-0.4999, -0.4975, -0.4993, -0.4962],\n           [-0.4967, -0.4981, -0.4965, -0.4994],\n           ...,\n           [-0.4992, -0.4961, -0.4991, -0.4977],\n           [-0.4990, -0.4983, -0.4974, -0.4977],\n           [-0.4972, -0.4967, -0.4981, -0.4975]],\n\n          [[-0.4992, -0.4996, -0.4991, -0.4961],\n           [-0.4977, -0.4996, -0.4972, -0.4992],\n           [-0.4983, -0.4991, -0.4975, -0.4972],\n           ...,\n           [-0.4979, -0.4976, -0.4967, -0.4978],\n           [-0.4982, -0.4983, -0.4987, -0.4995],\n           [-0.4967, -0.4975, -0.4978, -0.4984]],\n\n          [[-0.4963, -0.4987, -0.4984, -0.4966],\n           [-0.4964, -0.4961, -0.4966, -0.4967],\n           [-0.4966, -0.4985, -0.4994, -0.4996],\n           ...,\n           [-0.4997, -0.4979, -0.4975, -0.4972],\n           [-0.4971, -0.4982, -0.4982, -0.4996],\n           [-0.4977, -0.4987, -0.4992, -0.4970]],\n\n          ...,\n\n          [[-0.4966, -0.4963, -0.4995, -0.4968],\n           [-0.4970, -0.4984, -0.4988, -0.4995],\n           [-0.4990, -0.4993, -0.4963, -0.4961],\n           ...,\n           [-0.4999, -0.4976, -0.4972, -0.4969],\n           [-0.4979, -0.4961, -0.4987, -0.4981],\n           [-0.4986, -0.4997, -0.4988, -0.4991]],\n\n          [[-0.4977, -0.4967, -0.4999, -0.5000],\n           [-0.4977, -0.4983, -0.4991, -0.4981],\n           [-0.4980, -0.4962, -0.4966, -0.4973],\n           ...,\n           [-0.4991, -0.4968, -0.4966, -0.4965],\n           [-0.4975, -0.4987, -0.4998, -0.4992],\n           [-0.4986, -0.4994, -0.4993, -0.4982]],\n\n          [[-0.4988, -0.4990, -0.4972, -0.4981],\n           [-0.4973, -0.4976, -0.4961, -0.4966],\n           [-0.4979, -0.4972, -0.4995, -0.4978],\n           ...,\n           [-0.4987, -0.4979, -0.4993, -0.4988],\n           [-0.4977, -0.4974, -0.4989, -0.4991],\n           [-0.4984, -0.4998, -0.4976, -0.4963]]],\n\n\n         [[[-0.4968, -0.4991, -0.4998, -0.4989],\n           [-0.4989, -0.4976, -0.4962, -0.4996],\n           [-0.4972, -0.4983, -0.4986, -0.4962],\n           ...,\n           [-0.4996, -0.4995, -0.4996, -0.4982],\n           [-0.4996, -0.4987, -0.4976, -0.4982],\n           [-0.4990, -0.4982, -0.4975, -0.4999]],\n\n          [[-0.4996, -0.4965, -0.4997, -0.4969],\n           [-0.4983, -0.4976, -0.4979, -0.4982],\n           [-0.4992, -0.4988, -0.4982, -0.4994],\n           ...,\n           [-0.4984, -0.4999, -0.4990, -0.4997],\n           [-0.4984, -0.4992, -0.4992, -0.4986],\n           [-0.4994, -0.4975, -0.4997, -0.4977]],\n\n          [[-0.4976, -0.4964, -0.4981, -0.4992],\n           [-0.4990, -0.4965, -0.4991, -0.4995],\n           [-0.4964, -0.4990, -0.4963, -0.4963],\n           ...,\n           [-0.4963, -0.4990, -0.4991, -0.4980],\n           [-0.4998, -0.4961, -0.4969, -0.4968],\n           [-0.4993, -0.4986, -0.4969, -0.4974]],\n\n          ...,\n\n          [[-0.4981, -0.4963, -0.4983, -0.4985],\n           [-0.4979, -0.4992, -0.4981, -0.4974],\n           [-0.4969, -0.4993, -0.4973, -0.4993],\n           ...,\n           [-0.4978, -0.4983, -0.4988, -0.4993],\n           [-0.4987, -0.4962, -0.4976, -0.4966],\n           [-0.4979, -0.4973, -0.4969, -0.4961]],\n\n          [[-0.4972, -0.4962, -0.4986, -0.4978],\n           [-0.4975, -0.4980, -0.4973, -0.4964],\n           [-0.4969, -0.4972, -0.4970, -0.4987],\n           ...,\n           [-0.4966, -0.4980, -0.4985, -0.4962],\n           [-0.4989, -0.4964, -0.4989, -0.4994],\n           [-0.4974, -0.4979, -0.4979, -0.4991]],\n\n          [[-0.4980, -0.4989, -0.4963, -0.5000],\n           [-0.4970, -0.4988, -0.4991, -0.4983],\n           [-0.4977, -0.4973, -0.4988, -0.4962],\n           ...,\n           [-0.4975, -0.4981, -0.4975, -0.4990],\n           [-0.4969, -0.4967, -0.4964, -0.4982],\n           [-0.4991, -0.4984, -0.4966, -0.4964]]],\n\n\n         ...,\n\n\n         [[[-0.4971, -0.4997, -0.4996, -0.4990],\n           [-0.4967, -0.4987, -0.4975, -0.4984],\n           [-0.4994, -0.4988, -0.4961, -0.4968],\n           ...,\n           [-0.4971, -0.4965, -0.4994, -0.4970],\n           [-0.4981, -0.4999, -0.4977, -0.4985],\n           [-0.4985, -0.4992, -0.4993, -0.4996]],\n\n          [[-0.4963, -0.4983, -0.4990, -0.4991],\n           [-0.4986, -0.4970, -0.4962, -0.4973],\n           [-0.4971, -0.4966, -0.4974, -0.4998],\n           ...,\n           [-0.4967, -0.4962, -0.4966, -0.4995],\n           [-0.4996, -0.4970, -0.4968, -0.4964],\n           [-0.4996, -0.4981, -0.4973, -0.4986]],\n\n          [[-0.4978, -0.4997, -0.4997, -0.4967],\n           [-0.4970, -0.4977, -0.4968, -0.4964],\n           [-0.4972, -0.4980, -0.4983, -0.4968],\n           ...,\n           [-0.4977, -0.4968, -0.4979, -0.4974],\n           [-0.4970, -0.4996, -0.4997, -0.4984],\n           [-0.4978, -0.4961, -0.4977, -0.4964]],\n\n          ...,\n\n          [[-0.4996, -0.4983, -0.4981, -0.4980],\n           [-0.4991, -0.4971, -0.4970, -0.4986],\n           [-0.4990, -0.4979, -0.4985, -0.4998],\n           ...,\n           [-0.4978, -0.4988, -0.4999, -0.4962],\n           [-0.4984, -0.4993, -0.4979, -0.4987],\n           [-0.4973, -0.4972, -0.4989, -0.4981]],\n\n          [[-0.4998, -0.4990, -0.4967, -0.4997],\n           [-0.4964, -0.4972, -0.4993, -0.4968],\n           [-0.4990, -0.4984, -0.4992, -0.4974],\n           ...,\n           [-0.4986, -0.4993, -0.4966, -0.4990],\n           [-0.4974, -0.4982, -0.4986, -0.4973],\n           [-0.4993, -0.4972, -0.4989, -0.4970]],\n\n          [[-0.4984, -0.4981, -0.4978, -0.4988],\n           [-0.4993, -0.4986, -0.4996, -0.4979],\n           [-0.4964, -0.4962, -0.4988, -0.4981],\n           ...,\n           [-0.4968, -0.4979, -0.4965, -0.4986],\n           [-0.4974, -0.4969, -0.4992, -0.4988],\n           [-0.4988, -0.4987, -0.4974, -0.4980]]],\n\n\n         [[[-0.4992, -0.4981, -0.4967, -0.4999],\n           [-0.4986, -0.4980, -0.4970, -0.4997],\n           [-0.4964, -0.4993, -0.4969, -0.4995],\n           ...,\n           [-0.4984, -0.4991, -0.4974, -0.4987],\n           [-0.4972, -0.4983, -0.4967, -0.4999],\n           [-0.4996, -0.4990, -0.4968, -0.4980]],\n\n          [[-0.4987, -0.4992, -0.4987, -0.4972],\n           [-0.4982, -0.4996, -0.4990, -0.4973],\n           [-0.4963, -0.4989, -0.4961, -0.4985],\n           ...,\n           [-0.4972, -0.4993, -0.4993, -0.4992],\n           [-0.4968, -0.4968, -0.4976, -0.4964],\n           [-0.4992, -0.4966, -0.4995, -0.4976]],\n\n          [[-0.4971, -0.4980, -0.4963, -0.4997],\n           [-0.4962, -0.4973, -0.4998, -0.4985],\n           [-0.4999, -0.4963, -0.4991, -0.4979],\n           ...,\n           [-0.4991, -0.4980, -0.4992, -0.4981],\n           [-0.4989, -0.4963, -0.4988, -0.4988],\n           [-0.4986, -0.4999, -0.4962, -0.4976]],\n\n          ...,\n\n          [[-0.4973, -0.4986, -0.4971, -0.4981],\n           [-0.4983, -0.4981, -0.4977, -0.4982],\n           [-0.4976, -0.4993, -0.4999, -0.4972],\n           ...,\n           [-0.4978, -0.4979, -0.4971, -0.4994],\n           [-0.4976, -0.4974, -0.4997, -0.4974],\n           [-0.4989, -0.4961, -0.4967, -0.4963]],\n\n          [[-0.4985, -0.4988, -0.4977, -0.4988],\n           [-0.4978, -0.4975, -0.4990, -0.4962],\n           [-0.4987, -0.4966, -0.4973, -0.4994],\n           ...,\n           [-0.4988, -0.4962, -0.4972, -0.4986],\n           [-0.4996, -0.4982, -0.4963, -0.4994],\n           [-0.4977, -0.4970, -0.4979, -0.4998]],\n\n          [[-0.4996, -0.4994, -0.4970, -0.4982],\n           [-0.4973, -0.4991, -0.4989, -0.4992],\n           [-0.4998, -0.4995, -0.4976, -0.4999],\n           ...,\n           [-0.4985, -0.4985, -0.4993, -0.4991],\n           [-0.4961, -0.4992, -0.4984, -0.4983],\n           [-0.4996, -0.4965, -0.4982, -0.4985]]],\n\n\n         [[[-0.4980, -0.4967, -0.4973, -0.4987],\n           [-0.4975, -0.4977, -0.4966, -0.4997],\n           [-0.4976, -0.4978, -0.4964, -0.4970],\n           ...,\n           [-0.4975, -0.4996, -0.4966, -0.4962],\n           [-0.4984, -0.4983, -0.4981, -0.4973],\n           [-0.4983, -0.4995, -0.4996, -0.4963]],\n\n          [[-0.4969, -0.4984, -0.4971, -0.4969],\n           [-0.4967, -0.4985, -0.4993, -0.4993],\n           [-0.4967, -0.4984, -0.4964, -0.4974],\n           ...,\n           [-0.4967, -0.4976, -0.4961, -0.4972],\n           [-0.4981, -0.4962, -0.4988, -0.4992],\n           [-0.4972, -0.4970, -0.4986, -0.4972]],\n\n          [[-0.4982, -0.4994, -0.4984, -0.4989],\n           [-0.4973, -0.4994, -0.4990, -0.4969],\n           [-0.4966, -0.4968, -0.4968, -0.4962],\n           ...,\n           [-0.4999, -0.4984, -0.4984, -0.4991],\n           [-0.4997, -0.4965, -0.4991, -0.4985],\n           [-0.4987, -0.4978, -0.4964, -0.4972]],\n\n          ...,\n\n          [[-0.4984, -0.4996, -0.5000, -0.4987],\n           [-0.4963, -0.4963, -0.4994, -0.4994],\n           [-0.4974, -0.4991, -0.4971, -0.4982],\n           ...,\n           [-0.4968, -0.4975, -0.4976, -0.4995],\n           [-0.4989, -0.4983, -0.4998, -0.4998],\n           [-0.4981, -0.4985, -0.4964, -0.4970]],\n\n          [[-0.4996, -0.4966, -0.4993, -0.4985],\n           [-0.4989, -0.4990, -0.4975, -0.4985],\n           [-0.5000, -0.4985, -0.4977, -0.4967],\n           ...,\n           [-0.4968, -0.4973, -0.4984, -0.4997],\n           [-0.4972, -0.4999, -0.4966, -0.4991],\n           [-0.4995, -0.4984, -0.4979, -0.4968]],\n\n          [[-0.4999, -0.4978, -0.4988, -0.4994],\n           [-0.4980, -0.4979, -0.4982, -0.4969],\n           [-0.4993, -0.4981, -0.4992, -0.4979],\n           ...,\n           [-0.4966, -0.4963, -0.4977, -0.4966],\n           [-0.4973, -0.4980, -0.4990, -0.4997],\n           [-0.4992, -0.4976, -0.4991, -0.4970]]]],\n\n\n\n        ...,\n\n\n\n        [[[[-0.4961, -0.4993, -0.4964, -0.4962],\n           [-0.4966, -0.4987, -0.4990, -0.4982],\n           [-0.4976, -0.4973, -0.4999, -0.4985],\n           ...,\n           [-0.4979, -0.4983, -0.4991, -0.4997],\n           [-0.4964, -0.4999, -0.4964, -0.4968],\n           [-0.4969, -0.4966, -0.4987, -0.4987]],\n\n          [[-0.4967, -0.4979, -0.4995, -0.4987],\n           [-0.4988, -0.4961, -0.4997, -0.4972],\n           [-0.4980, -0.4991, -0.4964, -0.4996],\n           ...,\n           [-0.4997, -0.4965, -0.4973, -0.4980],\n           [-0.4995, -0.4999, -0.4990, -0.4969],\n           [-0.4986, -0.4984, -0.4971, -0.4973]],\n\n          [[-0.4966, -0.4982, -0.4977, -0.4983],\n           [-0.4988, -0.4964, -0.4969, -0.4990],\n           [-0.4980, -0.4972, -0.4962, -0.4974],\n           ...,\n           [-0.4967, -0.4973, -0.4991, -0.4975],\n           [-0.4965, -0.4966, -0.4995, -0.4965],\n           [-0.4965, -0.4966, -0.4965, -0.4996]],\n\n          ...,\n\n          [[-0.4968, -0.4991, -0.4963, -0.4970],\n           [-0.4970, -0.4964, -0.4992, -0.4970],\n           [-0.4973, -0.4974, -0.4981, -0.4979],\n           ...,\n           [-0.4999, -0.4965, -0.4993, -0.4988],\n           [-0.4964, -0.4999, -0.4978, -0.4993],\n           [-0.4963, -0.5000, -0.4979, -0.4993]],\n\n          [[-0.4967, -0.4967, -0.4995, -0.4981],\n           [-0.4995, -0.4973, -0.4979, -0.4970],\n           [-0.4968, -0.4999, -0.4998, -0.4973],\n           ...,\n           [-0.4968, -0.4966, -0.4971, -0.4962],\n           [-0.4963, -0.4974, -0.4985, -0.4998],\n           [-0.4964, -0.4983, -0.4971, -0.4999]],\n\n          [[-0.4995, -0.4972, -0.4969, -0.4963],\n           [-0.4970, -0.4985, -0.4966, -0.4996],\n           [-0.4961, -0.4965, -0.4981, -0.4962],\n           ...,\n           [-0.4982, -0.4979, -0.4977, -0.4985],\n           [-0.4969, -0.4988, -0.4968, -0.4993],\n           [-0.4985, -0.4968, -0.4983, -0.4969]]],\n\n\n         [[[-0.4964, -0.4973, -0.4995, -0.4974],\n           [-0.4968, -0.4995, -0.4976, -0.4988],\n           [-0.4966, -0.4984, -0.4997, -0.4983],\n           ...,\n           [-0.4997, -0.4966, -0.4969, -0.4989],\n           [-0.4972, -0.4980, -0.4966, -0.4962],\n           [-0.4984, -0.4970, -0.4964, -0.4963]],\n\n          [[-0.5000, -0.4978, -0.4989, -0.4977],\n           [-0.4979, -0.4979, -0.4963, -0.4973],\n           [-0.4962, -0.4970, -0.4996, -0.4965],\n           ...,\n           [-0.4993, -0.4963, -0.4966, -0.4985],\n           [-0.4994, -0.4979, -0.4971, -0.4988],\n           [-0.4987, -0.4987, -0.4993, -0.4963]],\n\n          [[-0.4993, -0.4995, -0.4968, -0.4981],\n           [-0.4993, -0.4987, -0.4979, -0.4997],\n           [-0.4988, -0.4988, -0.4964, -0.4975],\n           ...,\n           [-0.4974, -0.4974, -0.4965, -0.4974],\n           [-0.4968, -0.4964, -0.4970, -0.4994],\n           [-0.4965, -0.4971, -0.4998, -0.4977]],\n\n          ...,\n\n          [[-0.4963, -0.4972, -0.4966, -0.4984],\n           [-0.4968, -0.4964, -0.4975, -0.4980],\n           [-0.4966, -0.4971, -0.4967, -0.4979],\n           ...,\n           [-0.4985, -0.4988, -0.4962, -0.4982],\n           [-0.4989, -0.4985, -0.4975, -0.4961],\n           [-0.4988, -0.4969, -0.4964, -0.4969]],\n\n          [[-0.4979, -0.4987, -0.4967, -0.4986],\n           [-0.4979, -0.4972, -0.4971, -0.4984],\n           [-0.4984, -0.4963, -0.4978, -0.4979],\n           ...,\n           [-0.4986, -0.4980, -0.4965, -0.4992],\n           [-0.4969, -0.4982, -0.4979, -0.5000],\n           [-0.4992, -0.4986, -0.4987, -0.4994]],\n\n          [[-0.4996, -0.4988, -0.4994, -0.4981],\n           [-0.4977, -0.4993, -0.4986, -0.4991],\n           [-0.4964, -0.4997, -0.4995, -0.4976],\n           ...,\n           [-0.4967, -0.4984, -0.4979, -0.4970],\n           [-0.4973, -0.4996, -0.4984, -0.4976],\n           [-0.4980, -0.4976, -0.4972, -0.4984]]],\n\n\n         [[[-0.4969, -0.4975, -0.4968, -0.4987],\n           [-0.4983, -0.4998, -0.4985, -0.4989],\n           [-0.4986, -0.4985, -0.4965, -0.4969],\n           ...,\n           [-0.4994, -0.4996, -0.4984, -0.4981],\n           [-0.4988, -0.4992, -0.4997, -0.4999],\n           [-0.4988, -0.4974, -0.4997, -0.4993]],\n\n          [[-0.4979, -0.4975, -0.4984, -0.4980],\n           [-0.4989, -0.4974, -0.4978, -0.4971],\n           [-0.4969, -0.4996, -0.4972, -0.4975],\n           ...,\n           [-0.4999, -0.4999, -0.4965, -0.4981],\n           [-0.4982, -0.4988, -0.4968, -0.4962],\n           [-0.4976, -0.4980, -0.4974, -0.4964]],\n\n          [[-0.4987, -0.4975, -0.4964, -0.4975],\n           [-0.4977, -0.4972, -0.4988, -0.4992],\n           [-0.4969, -0.4967, -0.4963, -0.4975],\n           ...,\n           [-0.4994, -0.4968, -0.4970, -0.4964],\n           [-0.4969, -0.4965, -0.4964, -0.4997],\n           [-0.4970, -0.4967, -0.4991, -0.4977]],\n\n          ...,\n\n          [[-0.4983, -0.4988, -0.4998, -0.4989],\n           [-0.4968, -0.4988, -0.4986, -0.4996],\n           [-0.4962, -0.4962, -0.4990, -0.4999],\n           ...,\n           [-0.4998, -0.4994, -0.4994, -0.4991],\n           [-0.4962, -0.4978, -0.4969, -0.4977],\n           [-0.4983, -0.4976, -0.4986, -0.4996]],\n\n          [[-0.4989, -0.4992, -0.4993, -0.4973],\n           [-0.4997, -0.4972, -0.4976, -0.4963],\n           [-0.4985, -0.4998, -0.4982, -0.4971],\n           ...,\n           [-0.4977, -0.4962, -0.4972, -0.4981],\n           [-0.4995, -0.4974, -0.4974, -0.4978],\n           [-0.4992, -0.4970, -0.4962, -0.4980]],\n\n          [[-0.4974, -0.4980, -0.4971, -0.4995],\n           [-0.4980, -0.4962, -0.4980, -0.4976],\n           [-0.4997, -0.4991, -0.4991, -0.4986],\n           ...,\n           [-0.4989, -0.4969, -0.4968, -0.4987],\n           [-0.4980, -0.4985, -0.4963, -0.4972],\n           [-0.4980, -0.4965, -0.4970, -0.4967]]],\n\n\n         ...,\n\n\n         [[[-0.4998, -0.4994, -0.4976, -0.4988],\n           [-0.4963, -0.4980, -0.4998, -0.4983],\n           [-0.4989, -0.4962, -0.4992, -0.4975],\n           ...,\n           [-0.5000, -0.4967, -0.4997, -0.4989],\n           [-0.4966, -0.4985, -0.4975, -0.4976],\n           [-0.4992, -0.4984, -0.4994, -0.4983]],\n\n          [[-0.4966, -0.4965, -0.4980, -0.4982],\n           [-0.4966, -0.4963, -0.4990, -0.4998],\n           [-0.4996, -0.4981, -0.4984, -0.4993],\n           ...,\n           [-0.4989, -0.4993, -0.4990, -0.4991],\n           [-0.4973, -0.4981, -0.4985, -0.4993],\n           [-0.4967, -0.4962, -0.4969, -0.4963]],\n\n          [[-0.4974, -0.4993, -0.4971, -0.4980],\n           [-0.4990, -0.4990, -0.4975, -0.4999],\n           [-0.4985, -0.4977, -0.4976, -0.4969],\n           ...,\n           [-0.4998, -0.4977, -0.4964, -0.4996],\n           [-0.4977, -0.4970, -0.4989, -0.4989],\n           [-0.4992, -0.4967, -0.4978, -0.4964]],\n\n          ...,\n\n          [[-0.4982, -0.4992, -0.4999, -0.4970],\n           [-0.4975, -0.4988, -0.4999, -0.4974],\n           [-0.4997, -0.4967, -0.4994, -0.4996],\n           ...,\n           [-0.4964, -0.4995, -0.4991, -0.4999],\n           [-0.4992, -0.4972, -0.4964, -0.4975],\n           [-0.4962, -0.4972, -0.4983, -0.4981]],\n\n          [[-0.4974, -0.4995, -0.4983, -0.4980],\n           [-0.4981, -0.4996, -0.4977, -0.4961],\n           [-0.4996, -0.4966, -0.4994, -0.4976],\n           ...,\n           [-0.4985, -0.4977, -0.4968, -0.4992],\n           [-0.4999, -0.4989, -0.4992, -0.4991],\n           [-0.4993, -0.4994, -0.4987, -0.4990]],\n\n          [[-0.4963, -0.4983, -0.4975, -0.4985],\n           [-0.4992, -0.4999, -0.4967, -0.4978],\n           [-0.4981, -0.4996, -0.4998, -0.4967],\n           ...,\n           [-0.4984, -0.4962, -0.4993, -0.4970],\n           [-0.4970, -0.4986, -0.4978, -0.4996],\n           [-0.4964, -0.4983, -0.4991, -0.4973]]],\n\n\n         [[[-0.4973, -0.4978, -0.4971, -0.4986],\n           [-0.4973, -0.4982, -0.4992, -0.4966],\n           [-0.4977, -0.4993, -0.4996, -0.4967],\n           ...,\n           [-0.4999, -0.4992, -0.4981, -0.4966],\n           [-0.4968, -0.4981, -0.4966, -0.4979],\n           [-0.4969, -0.4978, -0.4988, -0.4980]],\n\n          [[-0.4986, -0.4982, -0.4979, -0.4990],\n           [-0.4986, -0.4965, -0.4985, -0.4977],\n           [-0.4964, -0.4987, -0.4980, -0.4965],\n           ...,\n           [-0.4973, -0.4969, -0.4977, -0.4985],\n           [-0.4971, -0.4962, -0.4961, -0.4993],\n           [-0.4982, -0.4993, -0.4993, -0.4972]],\n\n          [[-0.4979, -0.4967, -0.4978, -0.4995],\n           [-0.4982, -0.4997, -0.4972, -0.4986],\n           [-0.4962, -0.4982, -0.4986, -0.4980],\n           ...,\n           [-0.4992, -0.4976, -0.4971, -0.4997],\n           [-0.4994, -0.4991, -0.4963, -0.4995],\n           [-0.4968, -0.4995, -0.4977, -0.4988]],\n\n          ...,\n\n          [[-0.4984, -0.4968, -0.4981, -0.4993],\n           [-0.4988, -0.4991, -0.4967, -0.4969],\n           [-0.4976, -0.4987, -0.4975, -0.4974],\n           ...,\n           [-0.4967, -0.4964, -0.4968, -0.4990],\n           [-0.4974, -0.4981, -0.4999, -0.4976],\n           [-0.4993, -0.4983, -0.4969, -0.4976]],\n\n          [[-0.4991, -0.4973, -0.4982, -0.4984],\n           [-0.4992, -0.4972, -0.4994, -0.4976],\n           [-0.4989, -0.4998, -0.4995, -0.4968],\n           ...,\n           [-0.4967, -0.4985, -0.4980, -0.4962],\n           [-0.4984, -0.4998, -0.4989, -0.4999],\n           [-0.4981, -0.4961, -0.4986, -0.4999]],\n\n          [[-0.4993, -0.4980, -0.4980, -0.4990],\n           [-0.4988, -0.4962, -0.4997, -0.4970],\n           [-0.5000, -0.4988, -0.4965, -0.4985],\n           ...,\n           [-0.4984, -0.4969, -0.4988, -0.4991],\n           [-0.4983, -0.4994, -0.4962, -0.4988],\n           [-0.4994, -0.4987, -0.4996, -0.4968]]],\n\n\n         [[[-0.4968, -0.4978, -0.4995, -0.4966],\n           [-0.4982, -0.4971, -0.4967, -0.4964],\n           [-0.4974, -0.5000, -0.4966, -0.4978],\n           ...,\n           [-0.4977, -0.4964, -0.4988, -0.4976],\n           [-0.4965, -0.4988, -0.4976, -0.4985],\n           [-0.4963, -0.4983, -0.4963, -0.4976]],\n\n          [[-0.4999, -0.4964, -0.4972, -0.4998],\n           [-0.4968, -0.4974, -0.4991, -0.4962],\n           [-0.4987, -0.4983, -0.4986, -0.4971],\n           ...,\n           [-0.4962, -0.4992, -0.4966, -0.4985],\n           [-0.4979, -0.4966, -0.4967, -0.4977],\n           [-0.4976, -0.4971, -0.4996, -0.4977]],\n\n          [[-0.4969, -0.4984, -0.4971, -0.4968],\n           [-0.4977, -0.4974, -0.4966, -0.4992],\n           [-0.4996, -0.4987, -0.4990, -0.4975],\n           ...,\n           [-0.4976, -0.4985, -0.4977, -0.4961],\n           [-0.4987, -0.4984, -0.4964, -0.4988],\n           [-0.4968, -0.4965, -0.4975, -0.5000]],\n\n          ...,\n\n          [[-0.4978, -0.4962, -0.4989, -0.4964],\n           [-0.4980, -0.4999, -0.4980, -0.4981],\n           [-0.4967, -0.4991, -0.4970, -0.4967],\n           ...,\n           [-0.4974, -0.4985, -0.4989, -0.4968],\n           [-0.4996, -0.4991, -0.4973, -0.4994],\n           [-0.4964, -0.4977, -0.4988, -0.4981]],\n\n          [[-0.4980, -0.4970, -0.4995, -0.4966],\n           [-0.4976, -0.4978, -0.4980, -0.4984],\n           [-0.4995, -0.4986, -0.4999, -0.4992],\n           ...,\n           [-0.4993, -0.4993, -0.4973, -0.4978],\n           [-0.4991, -0.4989, -0.4964, -0.4975],\n           [-0.4998, -0.4998, -0.4988, -0.4991]],\n\n          [[-0.4998, -0.4989, -0.4984, -0.4972],\n           [-0.4998, -0.4984, -0.4988, -0.5000],\n           [-0.4999, -0.4978, -0.4983, -0.4984],\n           ...,\n           [-0.4976, -0.4977, -0.4975, -0.4998],\n           [-0.4968, -0.4987, -0.4980, -0.4978],\n           [-0.4977, -0.4990, -0.4985, -0.4963]]]],\n\n\n\n        [[[[-0.4990, -0.4998, -0.4978, -0.4963],\n           [-0.4995, -0.4985, -0.4984, -0.4990],\n           [-0.4968, -0.4993, -0.4995, -0.4962],\n           ...,\n           [-0.4992, -0.4963, -0.4984, -0.4991],\n           [-0.4969, -0.4975, -0.4991, -0.4987],\n           [-0.4991, -0.4978, -0.4964, -0.4971]],\n\n          [[-0.4995, -0.4988, -0.4961, -0.4994],\n           [-0.4983, -0.4970, -0.4991, -0.4979],\n           [-0.4962, -0.4962, -0.4985, -0.4968],\n           ...,\n           [-0.4966, -0.4975, -0.4978, -0.4990],\n           [-0.4974, -0.4982, -0.4980, -0.4977],\n           [-0.4976, -0.4985, -0.4971, -0.4972]],\n\n          [[-0.4995, -0.4977, -0.4999, -0.4986],\n           [-0.4990, -0.4996, -0.4992, -0.4988],\n           [-0.4963, -0.4973, -0.4967, -0.4990],\n           ...,\n           [-0.4971, -0.4971, -0.4973, -0.4967],\n           [-0.4978, -0.4992, -0.4967, -0.4973],\n           [-0.4988, -0.4997, -0.4970, -0.4981]],\n\n          ...,\n\n          [[-0.4978, -0.4994, -0.4973, -0.4980],\n           [-0.4963, -0.4965, -0.4993, -0.4980],\n           [-0.4991, -0.4997, -0.4968, -0.4979],\n           ...,\n           [-0.4963, -0.4997, -0.4996, -0.4975],\n           [-0.4979, -0.4999, -0.4996, -0.4963],\n           [-0.4978, -0.4999, -0.4961, -0.5000]],\n\n          [[-0.4989, -0.4973, -0.4980, -0.4963],\n           [-0.4968, -0.4975, -0.4977, -0.4980],\n           [-0.4992, -0.4971, -0.4966, -0.4964],\n           ...,\n           [-0.4981, -0.4974, -0.4964, -0.4998],\n           [-0.4979, -0.4975, -0.4964, -0.4996],\n           [-0.4965, -0.4986, -0.4962, -0.4963]],\n\n          [[-0.4971, -0.4963, -0.4988, -0.4965],\n           [-0.4992, -0.4972, -0.4997, -0.4997],\n           [-0.4962, -0.4990, -0.4990, -0.4999],\n           ...,\n           [-0.4997, -0.4966, -0.4993, -0.4964],\n           [-0.4975, -0.4973, -0.4983, -0.4981],\n           [-0.4967, -0.4975, -0.4993, -0.4997]]],\n\n\n         [[[-0.4999, -0.4984, -0.4963, -0.4969],\n           [-0.4993, -0.5000, -0.4986, -0.4985],\n           [-0.4998, -0.4984, -0.4990, -0.4961],\n           ...,\n           [-0.4977, -0.4994, -0.4970, -0.4984],\n           [-0.4982, -0.4973, -0.4979, -0.4983],\n           [-0.4986, -0.4982, -0.4986, -0.4983]],\n\n          [[-0.4971, -0.4962, -0.4997, -0.4985],\n           [-0.4970, -0.4963, -0.4978, -0.4963],\n           [-0.4980, -0.4973, -0.4975, -0.4994],\n           ...,\n           [-0.4967, -0.4988, -0.4963, -0.4994],\n           [-0.4964, -0.4966, -0.4976, -0.4988],\n           [-0.4968, -0.4983, -0.4962, -0.4984]],\n\n          [[-0.4977, -0.4968, -0.4974, -0.4981],\n           [-0.4978, -0.4965, -0.4990, -0.4991],\n           [-0.4982, -0.4996, -0.4995, -0.4984],\n           ...,\n           [-0.4991, -0.4978, -0.4998, -0.4964],\n           [-0.4962, -0.4975, -0.4976, -0.5000],\n           [-0.4976, -0.4970, -0.4996, -0.4978]],\n\n          ...,\n\n          [[-0.4974, -0.4983, -0.4965, -0.4994],\n           [-0.4976, -0.4991, -0.4974, -0.4974],\n           [-0.4978, -0.4979, -0.4978, -0.4977],\n           ...,\n           [-0.4988, -0.4994, -0.4977, -0.4997],\n           [-0.4991, -0.4965, -0.4979, -0.4986],\n           [-0.4987, -0.4984, -0.4978, -0.4965]],\n\n          [[-0.4964, -0.4963, -0.4984, -0.4990],\n           [-0.4964, -0.4963, -0.4993, -0.4997],\n           [-0.4987, -0.4995, -0.4961, -0.4997],\n           ...,\n           [-0.4978, -0.4975, -0.4962, -0.4988],\n           [-0.4984, -0.4998, -0.4989, -0.4969],\n           [-0.4980, -0.4990, -0.4981, -0.4989]],\n\n          [[-0.4994, -0.4979, -0.4995, -0.4968],\n           [-0.4974, -0.4977, -0.4990, -0.4962],\n           [-0.4977, -0.4994, -0.4976, -0.4990],\n           ...,\n           [-0.4965, -0.4991, -0.4992, -0.4964],\n           [-0.4986, -0.4991, -0.4969, -0.4977],\n           [-0.4971, -0.4978, -0.4964, -0.4964]]],\n\n\n         [[[-0.4969, -0.4968, -0.5000, -0.4990],\n           [-0.4997, -0.4987, -0.4974, -0.4994],\n           [-0.4984, -0.4978, -0.4990, -0.4986],\n           ...,\n           [-0.4981, -0.4998, -0.4989, -0.4963],\n           [-0.4981, -0.4996, -0.4978, -0.4971],\n           [-0.4999, -0.4988, -0.4987, -0.4964]],\n\n          [[-0.4976, -0.4982, -0.4969, -0.4975],\n           [-0.4987, -0.4997, -0.4970, -0.4979],\n           [-0.4962, -0.4976, -0.4994, -0.4979],\n           ...,\n           [-0.4990, -0.4973, -0.4981, -0.4964],\n           [-0.4971, -0.4964, -0.4993, -0.4965],\n           [-0.4979, -0.4986, -0.4965, -0.4971]],\n\n          [[-0.4999, -0.4995, -0.4989, -0.4977],\n           [-0.4984, -0.5000, -0.4971, -0.4966],\n           [-0.4991, -0.4985, -0.4966, -0.4993],\n           ...,\n           [-0.4980, -0.4982, -0.4967, -0.4989],\n           [-0.4973, -0.4987, -0.4993, -0.4963],\n           [-0.4984, -0.4991, -0.4983, -0.4973]],\n\n          ...,\n\n          [[-0.4965, -0.4963, -0.4976, -0.4977],\n           [-0.4983, -0.4966, -0.4963, -0.4999],\n           [-0.4978, -0.4970, -0.4984, -0.4967],\n           ...,\n           [-0.4977, -0.4990, -0.4977, -0.4985],\n           [-0.4987, -0.4973, -0.4997, -0.4993],\n           [-0.4982, -0.4991, -0.4978, -0.4967]],\n\n          [[-0.4964, -0.4983, -0.4984, -0.4971],\n           [-0.4996, -0.4970, -0.4978, -0.4998],\n           [-0.4977, -0.4995, -0.4987, -0.4988],\n           ...,\n           [-0.4966, -0.4971, -0.4973, -0.4991],\n           [-0.4962, -0.4982, -0.4981, -0.4986],\n           [-0.5000, -0.4969, -0.4979, -0.4996]],\n\n          [[-0.4983, -0.4997, -0.4979, -0.4965],\n           [-0.4991, -0.4971, -0.4964, -0.4981],\n           [-0.4991, -0.4972, -0.4968, -0.4999],\n           ...,\n           [-0.4961, -0.4985, -0.4978, -0.4977],\n           [-0.4987, -0.4962, -0.4978, -0.4961],\n           [-0.4971, -0.5000, -0.4965, -0.4993]]],\n\n\n         ...,\n\n\n         [[[-0.4976, -0.4970, -0.4964, -0.4986],\n           [-0.4970, -0.4990, -0.4995, -0.4990],\n           [-0.4989, -0.4981, -0.4978, -0.4989],\n           ...,\n           [-0.4984, -0.4992, -0.4976, -0.4984],\n           [-0.4970, -0.4975, -0.4968, -0.4987],\n           [-0.4990, -0.4979, -0.4973, -0.4991]],\n\n          [[-0.4987, -0.4989, -0.4965, -0.4983],\n           [-0.4989, -0.4974, -0.4964, -0.4961],\n           [-0.4981, -0.4974, -0.4983, -0.4977],\n           ...,\n           [-0.4996, -0.4963, -0.4961, -0.4974],\n           [-0.4977, -0.4995, -0.4975, -0.4980],\n           [-0.5000, -0.4985, -0.4998, -0.4968]],\n\n          [[-0.4984, -0.4988, -0.4962, -0.4977],\n           [-0.4976, -0.4979, -0.4974, -0.4991],\n           [-0.4982, -0.4970, -0.4966, -0.4967],\n           ...,\n           [-0.4991, -0.4987, -0.4984, -0.4965],\n           [-0.4966, -0.4989, -0.4969, -0.4997],\n           [-0.4978, -0.4985, -0.4978, -0.4970]],\n\n          ...,\n\n          [[-0.4967, -0.4991, -0.4967, -0.4996],\n           [-0.4992, -0.4975, -0.4965, -0.4982],\n           [-0.4961, -0.4987, -0.4997, -0.4980],\n           ...,\n           [-0.4981, -0.4972, -0.4970, -0.4987],\n           [-0.4973, -0.4969, -0.4996, -0.4977],\n           [-0.4995, -0.4962, -0.4967, -0.4979]],\n\n          [[-0.4997, -0.4971, -0.4978, -0.4965],\n           [-0.4993, -0.4996, -0.4967, -0.4971],\n           [-0.4991, -0.4979, -0.4967, -0.4969],\n           ...,\n           [-0.4983, -0.4981, -0.4969, -0.4987],\n           [-0.4970, -0.4962, -0.4987, -0.4989],\n           [-0.4976, -0.4964, -0.4979, -0.4972]],\n\n          [[-0.4969, -0.4985, -0.4999, -0.4981],\n           [-0.4965, -0.4988, -0.4992, -0.4997],\n           [-0.4993, -0.4963, -0.4982, -0.4997],\n           ...,\n           [-0.4995, -0.4983, -0.4989, -0.4964],\n           [-0.4974, -0.4967, -0.4979, -0.4987],\n           [-0.4991, -0.4980, -0.4995, -0.4967]]],\n\n\n         [[[-0.4991, -0.4983, -0.4994, -0.4993],\n           [-0.4980, -0.4963, -0.4982, -0.4972],\n           [-0.4969, -0.4996, -0.4975, -0.4962],\n           ...,\n           [-0.4998, -0.4974, -0.4983, -0.4985],\n           [-0.4994, -0.4980, -0.4985, -0.4993],\n           [-0.4976, -0.4979, -0.4997, -0.4962]],\n\n          [[-0.4986, -0.4966, -0.4983, -0.4997],\n           [-0.4963, -0.4982, -0.4974, -0.4981],\n           [-0.4989, -0.4988, -0.4998, -0.4994],\n           ...,\n           [-0.5000, -0.4982, -0.4982, -0.4971],\n           [-0.4985, -0.4976, -0.4989, -0.4989],\n           [-0.4974, -0.4983, -0.4985, -0.4989]],\n\n          [[-0.4993, -0.4985, -0.4970, -0.4969],\n           [-0.4976, -0.4990, -0.4998, -0.4971],\n           [-0.4996, -0.4987, -0.4985, -0.4984],\n           ...,\n           [-0.4989, -0.4970, -0.4983, -0.4968],\n           [-0.4963, -0.4995, -0.4974, -0.4980],\n           [-0.4999, -0.4987, -0.4980, -0.4969]],\n\n          ...,\n\n          [[-0.4995, -0.4984, -0.4977, -0.4993],\n           [-0.4990, -0.4993, -0.4996, -0.4964],\n           [-0.4965, -0.4988, -0.4970, -0.4965],\n           ...,\n           [-0.4965, -0.4967, -0.4988, -0.4980],\n           [-0.4981, -0.4966, -0.4977, -0.4982],\n           [-0.4983, -0.4983, -0.4984, -0.4975]],\n\n          [[-0.4968, -0.4986, -0.4991, -0.4999],\n           [-0.4985, -0.4979, -0.4984, -0.4983],\n           [-0.4988, -0.4978, -0.4975, -0.4979],\n           ...,\n           [-0.4967, -0.4997, -0.4998, -0.4982],\n           [-0.4975, -0.4996, -0.4997, -0.4961],\n           [-0.4962, -0.4989, -0.4979, -0.4970]],\n\n          [[-0.4976, -0.4983, -0.4968, -0.4991],\n           [-0.4967, -0.4984, -0.4963, -0.4965],\n           [-0.4988, -0.4975, -0.4962, -0.4965],\n           ...,\n           [-0.4986, -0.4991, -0.4993, -0.4978],\n           [-0.4974, -0.4963, -0.4992, -0.4979],\n           [-0.4997, -0.4994, -0.4997, -0.4966]]],\n\n\n         [[[-0.4976, -0.4997, -0.4983, -0.4998],\n           [-0.4983, -0.4988, -0.4976, -0.4968],\n           [-0.4972, -0.4972, -0.4998, -0.4999],\n           ...,\n           [-0.4966, -0.4973, -0.4990, -0.4969],\n           [-0.4967, -0.4991, -0.4979, -0.4977],\n           [-0.4998, -0.4973, -0.4992, -0.4992]],\n\n          [[-0.4991, -0.4977, -0.4979, -0.4976],\n           [-0.4967, -0.4969, -0.4977, -0.4967],\n           [-0.4970, -0.4967, -0.4981, -0.4962],\n           ...,\n           [-0.4999, -0.4967, -0.4972, -0.4993],\n           [-0.4990, -0.4965, -0.4967, -0.4968],\n           [-0.5000, -0.4965, -0.4973, -0.4990]],\n\n          [[-0.4971, -0.4973, -0.4982, -0.4981],\n           [-0.4992, -0.4983, -0.4983, -0.4969],\n           [-0.4987, -0.4993, -0.4977, -0.4999],\n           ...,\n           [-0.4974, -0.4968, -0.4995, -0.4966],\n           [-0.4973, -0.4985, -0.4993, -0.4992],\n           [-0.4968, -0.4992, -0.4985, -0.4991]],\n\n          ...,\n\n          [[-0.4979, -0.4963, -0.4979, -0.4978],\n           [-0.4982, -0.4986, -0.4988, -0.4968],\n           [-0.4970, -0.4964, -0.4996, -0.4973],\n           ...,\n           [-0.4993, -0.4984, -0.4965, -0.4978],\n           [-0.4990, -0.4984, -0.4970, -0.4985],\n           [-0.4999, -0.4980, -0.4966, -0.4965]],\n\n          [[-0.4994, -0.4974, -0.4974, -0.4975],\n           [-0.4996, -0.4989, -0.4991, -0.4969],\n           [-0.4974, -0.4965, -0.4992, -0.4986],\n           ...,\n           [-0.4993, -0.4965, -0.4999, -0.5000],\n           [-0.4991, -0.4982, -0.4964, -0.4991],\n           [-0.4968, -0.4967, -0.4988, -0.4996]],\n\n          [[-0.4990, -0.4972, -0.4970, -0.4993],\n           [-0.4981, -0.4991, -0.4989, -0.4968],\n           [-0.4974, -0.4961, -0.4970, -0.4962],\n           ...,\n           [-0.4982, -0.4992, -0.4971, -0.4971],\n           [-0.4982, -0.4970, -0.4988, -0.4999],\n           [-0.4977, -0.4990, -0.4992, -0.4981]]]],\n\n\n\n        [[[[-0.4996, -0.4981, -0.4973, -0.4983],\n           [-0.4999, -0.4968, -0.4977, -0.4990],\n           [-0.4990, -0.4983, -0.4990, -0.4995],\n           ...,\n           [-0.4963, -0.4964, -0.4975, -0.4967],\n           [-0.4987, -0.4997, -0.4974, -0.4982],\n           [-0.4982, -0.4977, -0.4975, -0.4985]],\n\n          [[-0.4986, -0.4972, -0.4990, -0.4976],\n           [-0.4962, -0.4991, -0.4989, -0.4994],\n           [-0.4994, -0.4962, -0.5000, -0.4984],\n           ...,\n           [-0.4988, -0.4964, -0.4977, -0.4963],\n           [-0.4986, -0.5000, -0.4996, -0.4966],\n           [-0.4981, -0.4976, -0.4974, -0.4963]],\n\n          [[-0.4967, -0.4970, -0.4977, -0.4964],\n           [-0.4985, -0.4962, -0.4973, -0.4991],\n           [-0.4999, -0.4962, -0.4999, -0.4973],\n           ...,\n           [-0.4971, -0.4962, -0.4991, -0.4968],\n           [-0.4964, -0.4991, -0.4993, -0.4968],\n           [-0.4966, -0.4971, -0.4976, -0.4994]],\n\n          ...,\n\n          [[-0.4990, -0.4994, -0.4971, -0.4997],\n           [-0.4982, -0.4966, -0.4993, -0.4969],\n           [-0.4986, -0.4983, -0.5000, -0.4980],\n           ...,\n           [-0.4981, -0.4968, -0.4984, -0.4998],\n           [-0.4999, -0.5000, -0.4983, -0.4979],\n           [-0.4984, -0.4965, -0.4991, -0.4994]],\n\n          [[-0.4965, -0.4993, -0.4962, -0.4983],\n           [-0.4986, -0.4980, -0.4974, -0.4994],\n           [-0.4975, -0.4995, -0.4965, -0.4988],\n           ...,\n           [-0.4992, -0.4994, -0.4964, -0.4992],\n           [-0.4981, -0.4981, -0.4998, -0.4964],\n           [-0.4975, -0.4972, -0.4961, -0.4978]],\n\n          [[-0.4994, -0.4971, -0.4998, -0.4979],\n           [-0.4991, -0.4969, -0.4998, -0.4993],\n           [-0.4963, -0.4980, -0.4988, -0.5000],\n           ...,\n           [-0.4977, -0.4976, -0.4970, -0.4994],\n           [-0.4980, -0.4973, -0.4998, -0.4997],\n           [-0.4982, -0.4978, -0.4996, -0.4982]]],\n\n\n         [[[-0.4964, -0.4990, -0.4975, -0.4967],\n           [-0.4981, -0.4972, -0.4995, -0.4990],\n           [-0.4962, -0.4996, -0.4999, -0.4962],\n           ...,\n           [-0.4981, -0.4988, -0.4978, -0.4968],\n           [-0.4969, -0.5000, -0.4965, -0.4991],\n           [-0.4972, -0.4975, -0.4976, -0.4983]],\n\n          [[-0.4961, -0.4967, -0.4988, -0.4994],\n           [-0.4996, -0.4965, -0.4982, -0.4981],\n           [-0.4974, -0.4979, -0.4985, -0.4984],\n           ...,\n           [-0.4986, -0.4996, -0.4971, -0.4996],\n           [-0.4967, -0.4984, -0.4990, -0.5000],\n           [-0.4989, -0.4996, -0.4973, -0.4993]],\n\n          [[-0.4969, -0.4987, -0.4982, -0.4977],\n           [-0.4977, -0.4962, -0.4980, -0.4980],\n           [-0.4967, -0.4967, -0.4981, -0.4995],\n           ...,\n           [-0.4965, -0.4963, -0.4975, -0.4984],\n           [-0.4981, -0.4991, -0.4985, -0.4992],\n           [-0.4962, -0.4980, -0.4982, -0.4984]],\n\n          ...,\n\n          [[-0.4974, -0.4987, -0.4981, -0.4983],\n           [-0.4977, -0.4980, -0.4984, -0.4969],\n           [-0.4965, -0.4998, -0.4963, -0.4988],\n           ...,\n           [-0.4988, -0.4992, -0.4979, -0.4961],\n           [-0.4974, -0.4990, -0.4985, -0.4985],\n           [-0.4971, -0.4997, -0.4977, -0.4971]],\n\n          [[-0.4973, -0.4984, -0.4986, -0.4978],\n           [-0.4983, -0.4992, -0.4995, -0.4969],\n           [-0.4985, -0.4992, -0.4996, -0.4969],\n           ...,\n           [-0.4982, -0.4976, -0.4977, -0.4977],\n           [-0.4998, -0.4976, -0.4999, -0.4975],\n           [-0.4980, -0.4990, -0.4980, -0.4997]],\n\n          [[-0.4973, -0.4979, -0.4999, -0.5000],\n           [-0.4992, -0.4986, -0.4968, -0.4990],\n           [-0.5000, -0.4989, -0.4993, -0.4970],\n           ...,\n           [-0.4973, -0.4967, -0.4961, -0.4980],\n           [-0.4985, -0.4997, -0.4975, -0.4968],\n           [-0.4983, -0.4993, -0.4971, -0.4975]]],\n\n\n         [[[-0.4970, -0.4999, -0.4980, -0.4977],\n           [-0.4996, -0.4981, -0.4999, -0.4993],\n           [-0.4981, -0.4964, -0.4974, -0.4980],\n           ...,\n           [-0.4979, -0.4966, -0.4982, -0.4978],\n           [-0.4989, -0.4966, -0.4964, -0.4988],\n           [-0.4972, -0.4976, -0.4988, -0.4973]],\n\n          [[-0.4972, -0.4976, -0.4986, -0.4994],\n           [-0.4985, -0.4970, -0.4995, -0.4971],\n           [-0.4971, -0.4997, -0.4977, -0.4975],\n           ...,\n           [-0.4962, -0.4998, -0.4994, -0.4993],\n           [-0.4975, -0.4997, -0.4971, -0.4987],\n           [-0.4967, -0.4966, -0.4990, -0.4998]],\n\n          [[-0.4986, -0.4990, -0.4964, -0.4993],\n           [-0.4973, -0.4992, -0.4983, -0.4967],\n           [-0.4964, -0.4974, -0.4984, -0.4996],\n           ...,\n           [-0.4991, -0.4988, -0.4961, -0.4994],\n           [-0.4990, -0.4989, -0.4996, -0.4996],\n           [-0.4989, -0.4979, -0.4994, -0.4984]],\n\n          ...,\n\n          [[-0.4968, -0.4962, -0.4987, -0.4977],\n           [-0.4984, -0.4969, -0.4962, -0.4994],\n           [-0.4996, -0.4996, -0.4980, -0.4970],\n           ...,\n           [-0.4971, -0.4980, -0.4976, -0.4981],\n           [-0.4993, -0.4989, -0.4976, -0.4985],\n           [-0.4990, -0.4976, -0.4975, -0.4970]],\n\n          [[-0.4978, -0.4977, -0.4997, -0.4986],\n           [-0.4995, -0.4978, -0.4986, -0.4972],\n           [-0.4995, -0.4985, -0.4968, -0.4999],\n           ...,\n           [-0.4983, -0.4972, -0.4981, -0.4968],\n           [-0.4977, -0.4967, -0.4996, -0.4992],\n           [-0.4962, -0.4996, -0.4973, -0.4996]],\n\n          [[-0.4990, -0.4969, -0.4997, -0.4965],\n           [-0.4991, -0.4992, -0.4995, -0.4995],\n           [-0.4973, -0.4980, -0.4972, -0.4991],\n           ...,\n           [-0.4964, -0.4990, -0.4993, -0.4994],\n           [-0.4967, -0.4969, -0.4965, -0.4981],\n           [-0.4967, -0.4979, -0.4993, -0.4965]]],\n\n\n         ...,\n\n\n         [[[-0.5000, -0.4965, -0.4993, -0.5000],\n           [-0.4973, -0.4982, -0.4965, -0.4999],\n           [-0.4967, -0.4964, -0.4998, -0.4967],\n           ...,\n           [-0.4981, -0.4992, -0.4984, -0.4976],\n           [-0.4964, -0.4996, -0.4967, -0.4979],\n           [-0.4980, -0.4984, -0.4995, -0.4978]],\n\n          [[-0.4998, -0.4966, -0.4998, -0.4988],\n           [-0.4989, -0.4976, -0.4976, -0.4996],\n           [-0.4978, -0.4997, -0.4994, -0.4991],\n           ...,\n           [-0.4963, -0.4962, -0.4965, -0.4974],\n           [-0.4985, -0.4965, -0.4980, -0.4985],\n           [-0.4989, -0.4996, -0.4977, -0.4967]],\n\n          [[-0.4996, -0.4995, -0.4992, -0.4993],\n           [-0.4984, -0.4964, -0.4964, -0.4973],\n           [-0.4977, -0.4978, -0.4983, -0.4987],\n           ...,\n           [-0.4995, -0.4964, -0.4992, -0.4996],\n           [-0.4965, -0.4991, -0.4991, -0.4998],\n           [-0.4967, -0.4981, -0.4964, -0.4989]],\n\n          ...,\n\n          [[-0.4992, -0.4971, -0.4974, -0.4985],\n           [-0.4992, -0.4967, -0.4999, -0.4996],\n           [-0.4969, -0.4977, -0.4974, -0.4995],\n           ...,\n           [-0.4978, -0.4987, -0.4980, -0.4984],\n           [-0.4987, -0.4984, -0.4968, -0.4980],\n           [-0.4971, -0.4996, -0.4988, -0.4995]],\n\n          [[-0.4984, -0.4988, -0.4999, -0.4991],\n           [-0.4984, -0.4979, -0.4966, -0.4975],\n           [-0.4964, -0.4992, -0.4973, -0.4962],\n           ...,\n           [-0.4981, -0.4980, -0.4990, -0.4987],\n           [-0.4966, -0.4968, -0.4962, -0.4994],\n           [-0.4972, -0.4976, -0.4990, -0.4969]],\n\n          [[-0.4979, -0.4967, -0.4972, -0.4994],\n           [-0.4961, -0.4988, -0.4985, -0.4978],\n           [-0.4969, -0.4984, -0.4962, -0.4967],\n           ...,\n           [-0.4981, -0.4989, -0.4968, -0.4989],\n           [-0.4989, -0.4962, -0.4995, -0.4988],\n           [-0.4985, -0.4971, -0.4986, -0.4993]]],\n\n\n         [[[-0.4994, -0.4996, -0.4981, -0.4974],\n           [-0.4986, -0.4971, -0.4981, -0.4997],\n           [-0.4973, -0.4978, -0.4992, -0.4965],\n           ...,\n           [-0.4988, -0.4997, -0.4965, -0.4998],\n           [-0.4995, -0.4998, -0.4967, -0.4997],\n           [-0.4980, -0.4985, -0.4975, -0.4963]],\n\n          [[-0.4999, -0.4989, -0.4993, -0.4970],\n           [-0.4981, -0.4969, -0.4961, -0.4973],\n           [-0.4987, -0.4964, -0.4994, -0.4979],\n           ...,\n           [-0.4989, -0.4967, -0.4989, -0.4975],\n           [-0.4988, -0.4986, -0.4984, -0.4997],\n           [-0.5000, -0.4980, -0.4985, -0.4963]],\n\n          [[-0.4987, -0.4967, -0.4989, -0.4985],\n           [-0.4982, -0.4965, -0.4975, -0.4999],\n           [-0.4966, -0.4961, -0.4974, -0.4982],\n           ...,\n           [-0.4971, -0.4973, -0.4990, -0.4983],\n           [-0.4971, -0.4970, -0.4978, -0.4978],\n           [-0.4998, -0.4981, -0.4967, -0.4987]],\n\n          ...,\n\n          [[-0.4984, -0.4973, -0.4977, -0.4965],\n           [-0.4991, -0.4992, -0.4964, -0.4965],\n           [-0.4973, -0.4978, -0.4969, -0.4963],\n           ...,\n           [-0.4965, -0.4994, -0.4978, -0.4997],\n           [-0.4986, -0.4966, -0.4979, -0.4978],\n           [-0.4961, -0.4981, -0.4995, -0.4966]],\n\n          [[-0.4994, -0.4972, -0.4963, -0.4995],\n           [-0.4966, -0.4981, -0.4973, -0.4968],\n           [-0.4963, -0.4966, -0.4966, -0.4998],\n           ...,\n           [-0.4961, -0.4977, -0.4985, -0.4974],\n           [-0.4999, -0.4992, -0.4974, -0.4970],\n           [-0.4973, -0.4996, -0.4991, -0.4965]],\n\n          [[-0.4976, -0.4973, -0.4993, -0.4991],\n           [-0.4996, -0.4991, -0.4965, -0.4975],\n           [-0.4962, -0.4963, -0.4990, -0.4992],\n           ...,\n           [-0.4999, -0.4967, -0.4971, -0.4989],\n           [-0.4963, -0.4985, -0.4978, -0.4988],\n           [-0.4979, -0.4984, -0.4988, -0.4997]]],\n\n\n         [[[-0.4962, -0.4970, -0.4962, -0.4996],\n           [-0.4985, -0.4963, -0.4981, -0.4966],\n           [-0.4969, -0.4989, -0.4970, -0.4975],\n           ...,\n           [-0.5000, -0.4995, -0.4974, -0.4982],\n           [-0.4965, -0.4962, -0.4993, -0.4962],\n           [-0.4975, -0.4974, -0.4980, -0.4962]],\n\n          [[-0.4984, -0.4985, -0.4998, -0.5000],\n           [-0.4971, -0.4991, -0.4963, -0.4980],\n           [-0.4999, -0.4988, -0.4999, -0.4963],\n           ...,\n           [-0.4976, -0.4994, -0.4984, -0.4988],\n           [-0.4997, -0.4990, -0.4986, -0.4987],\n           [-0.4966, -0.4989, -0.4985, -0.4975]],\n\n          [[-0.4976, -0.4975, -0.4996, -0.4973],\n           [-0.4987, -0.4978, -0.4996, -0.4988],\n           [-0.4972, -0.4986, -0.4971, -0.4974],\n           ...,\n           [-0.4971, -0.4978, -0.4963, -0.4981],\n           [-0.4969, -0.4987, -0.4970, -0.4964],\n           [-0.4974, -0.4989, -0.4991, -0.4973]],\n\n          ...,\n\n          [[-0.4982, -0.4996, -0.4967, -0.4970],\n           [-0.4984, -0.4991, -0.4988, -0.4965],\n           [-0.4993, -0.4966, -0.4986, -0.4975],\n           ...,\n           [-0.4975, -0.4992, -0.4964, -0.4967],\n           [-0.4978, -0.4987, -0.4997, -0.4971],\n           [-0.4986, -0.4973, -0.4983, -0.4961]],\n\n          [[-0.4978, -0.4994, -0.4971, -0.4973],\n           [-0.4983, -0.4988, -0.4992, -0.4983],\n           [-0.4988, -0.4999, -0.4961, -0.4982],\n           ...,\n           [-0.4967, -0.4991, -0.4985, -0.4982],\n           [-0.4967, -0.4993, -0.4983, -0.4963],\n           [-0.4966, -0.4993, -0.5000, -0.4980]],\n\n          [[-0.4994, -0.4992, -0.4977, -0.4985],\n           [-0.4976, -0.4973, -0.4969, -0.4996],\n           [-0.4971, -0.4989, -0.4961, -0.4991],\n           ...,\n           [-0.4981, -0.4976, -0.4981, -0.4987],\n           [-0.4983, -0.4978, -0.4977, -0.4996],\n           [-0.4972, -0.4966, -0.4971, -0.4968]]]]])"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "# Load input data and output labels\n",
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/data_gp/slices_64_new\"\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "X = []\n",
    "for f in input_files:\n",
    "    data = np.load(os.path.join(input_dir, f))  # Shape (155, 64, 64, 4)\n",
    "    X.append(data)\n",
    "X = np.stack(X, axis=0)  # Shape (120, 155, 64, 64, 4)\n",
    "\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, output_data, test_size=0.2, random_state=10)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=0.25, random_state=10)\n",
    "\n",
    "# Create data loaders for training, validation, and test sets\n",
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(train_X).float(), torch.from_numpy(train_y).float()), batch_size=10, shuffle=True) # shape (72, 155, 64, 64, 4)\n",
    "valid_loader = DataLoader(TensorDataset(torch.from_numpy(valid_X).float(), torch.from_numpy(valid_y).float()), batch_size=10)\n",
    "test_loader = DataLoader(TensorDataset(torch.from_numpy(test_X).float(), torch.from_numpy(test_y).float()), batch_size=10)\n",
    "\n",
    "# Define CNN model\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv3D(32, kernel_size=(3,3,3), activation='relu', input_shape=(155, 64, 64, 4))\n",
    "        self.max_pool1 = tf.keras.layers.MaxPooling3D(pool_size=(2,2,2))\n",
    "        self.conv2 = tf.keras.layers.Conv3D(64, kernel_size=(3,3,3), activation='relu')\n",
    "        self.max_pool2 = tf.keras.layers.MaxPooling3D(pool_size=(2,2,2))\n",
    "        self.conv2 = tf.keras.layers.Conv3D(128, kernel_size=(3,3,3), activation='relu')\n",
    "        self.max_pool2 = tf.keras.layers.MaxPooling3D(pool_size=(2,2,2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        output = self.dense2(x)\n",
    "        return output\n",
    "\n",
    "# Instantiate the CNN model\n",
    "cnn = CNN()\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss = tf.keras.metrics.Mean()\n",
    "    valid_loss = tf.keras.metrics.Mean()\n",
    "\n",
    "    # Train loop\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            logits = cnn(batch_X)\n",
    "            loss_value = loss_fn(batch_y, logits)\n",
    "\n",
    "        # Backward pass\n",
    "        grads = tape.gradient(loss_value, cnn.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, cnn.trainable_variables))\n",
    "\n",
    "        # Update train loss metric\n",
    "        train_loss.update_state(loss_value)\n",
    "\n",
    "    for batch_X, batch_y in valid_loader:\n",
    "        # Forward pass\n",
    "        logits = cnn(batch_X)\n",
    "        loss_value = loss_fn(batch_y, logits)\n",
    "\n",
    "        # Compute and update metrics\n",
    "        valid_loss(loss_value)\n",
    "\n",
    "    # Print epoch results\n",
    "    print(\"Epoch {:03d}: Train Loss: {:.3f}, Valid Loss: {:.3f}\".format(epoch, train_loss.result(), valid_loss.result()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dhc/home/youngbin.ko/conda3/envs/copy_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 2539520)\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/data_gp/slices_64_new\"\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "result = []\n",
    "\n",
    "for f in input_files:\n",
    "  data = np.load(os.path.join(input_dir, f))\n",
    "  result.append(data.reshape(1, -1))\n",
    "\n",
    "result = np.concatenate(result, axis=0)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 0, Loss 109497.21875\n",
      "Epoch 10, Loss 58947.0546875\n",
      "Epoch 20, Loss 39493.4609375\n",
      "Epoch 30, Loss 30814.509765625\n",
      "Epoch 40, Loss 26195.822265625\n",
      "Epoch 50, Loss 23339.302734375\n",
      "Epoch 60, Loss 21354.359375\n",
      "Epoch 70, Loss 19854.412109375\n",
      "Epoch 80, Loss 18654.419921875\n",
      "Epoch 90, Loss 17657.18359375\n",
      "Epoch 100, Loss 16806.71875\n",
      "Epoch 110, Loss 16067.9443359375\n",
      "Epoch 120, Loss 15417.25390625\n",
      "Epoch 130, Loss 14837.8486328125\n",
      "Epoch 140, Loss 14317.279296875\n",
      "Epoch 150, Loss 13846.013671875\n",
      "Epoch 160, Loss 13416.5986328125\n",
      "Epoch 170, Loss 13023.0830078125\n",
      "Epoch 180, Loss 12660.64453125\n",
      "Epoch 190, Loss 12325.3369140625\n",
      "Validation Loss: 30017.16796875\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# input_dir = \"/dhc/home/youngbin.ko/glow_brain/model/output\" # Shape (1,2539520 ) npy\n",
    "# input_files = sorted(os.listdir(input_dir))\n",
    "# input_data = np.concatenate([np.load(os.path.join(input_dir, f)) for f in input_files], axis=0)\n",
    "# print(input_data.shape) #(120,2539520)\n",
    "input_data = result\n",
    "\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "train_x, test_x, train_y, test_y = train_test_split(input_data, output_data, test_size=0.2, random_state=10)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.25, random_state=10)\n",
    "\n",
    "train_x = torch.from_numpy(train_x).float().to(device)\n",
    "train_y = torch.from_numpy(train_y).float().to(device)\n",
    "valid_x = torch.from_numpy(valid_x).float().to(device)\n",
    "valid_y = torch.from_numpy(valid_y).float().to(device)\n",
    "test_x = torch.from_numpy(test_x).float().to(device)\n",
    "test_y = torch.from_numpy(test_y).float().to(device)\n",
    "\n",
    "# Define Gaussian Process Regression model\n",
    "class GaussianProcessRegression(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Initialize model and likelihood\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "model = GaussianProcessRegression(train_x, train_y, likelihood).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss {loss.item()}\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(valid_x)\n",
    "    loss = -mll(output, valid_y)\n",
    "    print(f\"Validation Loss: {loss.item()}\")\n",
    "\n",
    "    f_preds = model(valid_x)\n",
    "    y_preds = likelihood(f_preds)\n",
    "    f_mean = f_preds.mean\n",
    "    f_var = f_preds.variance\n",
    "    f_covar = f_preds.covariance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 303609.6875\n",
      "Epoch 10, Loss 302756.53125\n",
      "Epoch 20, Loss 301905.75\n",
      "Epoch 30, Loss 301057.5\n",
      "Epoch 40, Loss 300212.125\n",
      "Epoch 50, Loss 299369.5625\n",
      "Epoch 60, Loss 298529.90625\n",
      "Epoch 70, Loss 297693.1875\n",
      "Epoch 80, Loss 296859.4375\n",
      "Epoch 90, Loss 296028.625\n",
      "Epoch 100, Loss 295200.75\n",
      "Epoch 110, Loss 294375.75\n",
      "Epoch 120, Loss 293553.6875\n",
      "Epoch 130, Loss 292734.5625\n",
      "Epoch 140, Loss 291918.28125\n",
      "Epoch 150, Loss 291104.90625\n",
      "Epoch 160, Loss 290294.40625\n",
      "Epoch 170, Loss 289486.75\n",
      "Epoch 180, Loss 288681.9375\n",
      "Epoch 190, Loss 287880.0625\n",
      "Validation Loss: 410864.9375\n"
     ]
    }
   ],
   "source": [
    "#using MSE\n",
    "input_data = result\n",
    "\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "train_x, test_x, train_y, test_y = train_test_split(input_data, output_data, test_size=0.2, random_state=10)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.25, random_state=10)\n",
    "\n",
    "train_x = torch.from_numpy(train_x).float().to(device)\n",
    "train_y = torch.from_numpy(train_y).float().to(device)\n",
    "valid_x = torch.from_numpy(valid_x).float().to(device)\n",
    "valid_y = torch.from_numpy(valid_y).float().to(device)\n",
    "test_x = torch.from_numpy(test_x).float().to(device)\n",
    "test_y = torch.from_numpy(test_y).float().to(device)\n",
    "\n",
    "# Define Gaussian Process Regression model\n",
    "class GaussianProcessRegression(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y):\n",
    "        super().__init__(train_x, train_y, gpytorch.likelihoods.GaussianLikelihood())\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Initialize model\n",
    "model = GaussianProcessRegression(train_x, train_y).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = loss_fn(output.mean, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss {loss.item()}\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(valid_x)\n",
    "    loss = loss_fn(output.mean, valid_y)\n",
    "    print(f\"Validation Loss: {loss.item()}\")\n",
    "\n",
    "    y_preds = output.mean\n",
    "    y_var = output.variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 109497.21875\n",
      "Validation Loss: 253505.546875\n",
      "Validation Loss: 236439.53125\n",
      "Validation Loss: 220969.234375\n",
      "Validation Loss: 206955.8125\n",
      "Validation Loss: 194267.484375\n",
      "Validation Loss: 182780.125\n",
      "Validation Loss: 172378.234375\n",
      "Validation Loss: 162954.890625\n",
      "Validation Loss: 154412.125\n",
      "Validation Loss: 146660.59375\n",
      "Epoch 10, Loss 58947.0546875\n",
      "Validation Loss: 139619.203125\n",
      "Validation Loss: 133214.75\n",
      "Validation Loss: 127381.453125\n",
      "Validation Loss: 122060.28125\n",
      "Validation Loss: 117198.40625\n",
      "Validation Loss: 112748.6796875\n",
      "Validation Loss: 108669.078125\n",
      "Validation Loss: 104922.0859375\n",
      "Validation Loss: 101474.3984375\n",
      "Validation Loss: 98296.2421875\n",
      "Epoch 20, Loss 39493.4609375\n",
      "Validation Loss: 95361.2421875\n",
      "Validation Loss: 92645.734375\n",
      "Validation Loss: 90128.7421875\n",
      "Validation Loss: 87791.5\n",
      "Validation Loss: 85617.296875\n",
      "Validation Loss: 83591.125\n",
      "Validation Loss: 81699.59375\n",
      "Validation Loss: 79930.7109375\n",
      "Validation Loss: 78273.6953125\n",
      "Validation Loss: 76718.8828125\n",
      "Epoch 30, Loss 30814.509765625\n",
      "Validation Loss: 75257.546875\n",
      "Validation Loss: 73881.84375\n",
      "Validation Loss: 72584.7265625\n",
      "Validation Loss: 71359.78125\n",
      "Validation Loss: 70201.234375\n",
      "Validation Loss: 69103.8515625\n",
      "Validation Loss: 68062.890625\n",
      "Validation Loss: 67074.03125\n",
      "Validation Loss: 66133.375\n",
      "Validation Loss: 65237.35546875\n",
      "Epoch 40, Loss 26195.822265625\n",
      "Validation Loss: 64382.6875\n",
      "Validation Loss: 63566.43359375\n",
      "Validation Loss: 62785.890625\n",
      "Validation Loss: 62038.57421875\n",
      "Validation Loss: 61322.21875\n",
      "Validation Loss: 60634.73046875\n",
      "Validation Loss: 59974.21875\n",
      "Validation Loss: 59338.93359375\n",
      "Validation Loss: 58727.25\n",
      "Validation Loss: 58137.71484375\n",
      "Epoch 50, Loss 23339.302734375\n",
      "Validation Loss: 57568.93359375\n",
      "Validation Loss: 57019.65625\n",
      "Validation Loss: 56488.734375\n",
      "Validation Loss: 55975.10546875\n",
      "Validation Loss: 55477.74609375\n",
      "Validation Loss: 54995.75\n",
      "Validation Loss: 54528.27734375\n",
      "Validation Loss: 54074.53125\n",
      "Validation Loss: 53633.7890625\n",
      "Validation Loss: 53205.359375\n",
      "Epoch 60, Loss 21354.359375\n",
      "Validation Loss: 52788.6171875\n",
      "Validation Loss: 52382.9765625\n",
      "Validation Loss: 51987.90625\n",
      "Validation Loss: 51602.85546875\n",
      "Validation Loss: 51227.390625\n",
      "Validation Loss: 50861.03125\n",
      "Validation Loss: 50503.38671875\n",
      "Validation Loss: 50154.0703125\n",
      "Validation Loss: 49812.6875\n",
      "Validation Loss: 49478.9375\n",
      "Epoch 70, Loss 19854.412109375\n",
      "Validation Loss: 49152.46875\n",
      "Validation Loss: 48832.99609375\n",
      "Validation Loss: 48520.2421875\n",
      "Validation Loss: 48213.91796875\n",
      "Validation Loss: 47913.79296875\n",
      "Validation Loss: 47619.625\n",
      "Validation Loss: 47331.1953125\n",
      "Validation Loss: 47048.29296875\n",
      "Validation Loss: 46770.71484375\n",
      "Validation Loss: 46498.28125\n",
      "Epoch 80, Loss 18654.419921875\n",
      "Validation Loss: 46230.8046875\n",
      "Validation Loss: 45968.1328125\n",
      "Validation Loss: 45710.078125\n",
      "Validation Loss: 45456.51171875\n",
      "Validation Loss: 45207.29296875\n",
      "Validation Loss: 44962.265625\n",
      "Validation Loss: 44721.3125\n",
      "Validation Loss: 44484.32421875\n",
      "Validation Loss: 44251.140625\n",
      "Validation Loss: 44021.68359375\n",
      "Epoch 90, Loss 17657.18359375\n",
      "Validation Loss: 43795.82421875\n",
      "Validation Loss: 43573.484375\n",
      "Validation Loss: 43354.546875\n",
      "Validation Loss: 43138.91796875\n",
      "Validation Loss: 42926.515625\n",
      "Validation Loss: 42717.25390625\n",
      "Validation Loss: 42511.03125\n",
      "Validation Loss: 42307.79296875\n",
      "Validation Loss: 42107.4453125\n",
      "Validation Loss: 41909.9375\n",
      "Epoch 100, Loss 16806.71875\n",
      "Validation Loss: 41715.1640625\n",
      "Validation Loss: 41523.08984375\n",
      "Validation Loss: 41333.625\n",
      "Validation Loss: 41146.72265625\n",
      "Validation Loss: 40962.3203125\n",
      "Validation Loss: 40780.359375\n",
      "Validation Loss: 40600.77734375\n",
      "Validation Loss: 40423.52734375\n",
      "Validation Loss: 40248.55078125\n",
      "Validation Loss: 40075.796875\n",
      "Epoch 110, Loss 16067.9443359375\n",
      "Validation Loss: 39905.22265625\n",
      "Validation Loss: 39736.7734375\n",
      "Validation Loss: 39570.40625\n",
      "Validation Loss: 39406.078125\n",
      "Validation Loss: 39243.74609375\n",
      "Validation Loss: 39083.36328125\n",
      "Validation Loss: 38924.8984375\n",
      "Validation Loss: 38768.30078125\n",
      "Validation Loss: 38613.5390625\n",
      "Validation Loss: 38460.5859375\n",
      "Epoch 120, Loss 15417.25390625\n",
      "Validation Loss: 38309.390625\n",
      "Validation Loss: 38159.9140625\n",
      "Validation Loss: 38012.1484375\n",
      "Validation Loss: 37866.03515625\n",
      "Validation Loss: 37721.5546875\n",
      "Validation Loss: 37578.671875\n",
      "Validation Loss: 37437.35546875\n",
      "Validation Loss: 37297.578125\n",
      "Validation Loss: 37159.31640625\n",
      "Validation Loss: 37022.5390625\n",
      "Epoch 130, Loss 14837.8486328125\n",
      "Validation Loss: 36887.20703125\n",
      "Validation Loss: 36753.3125\n",
      "Validation Loss: 36620.828125\n",
      "Validation Loss: 36489.71875\n",
      "Validation Loss: 36359.95703125\n",
      "Validation Loss: 36231.53125\n",
      "Validation Loss: 36104.421875\n",
      "Validation Loss: 35978.5859375\n",
      "Validation Loss: 35854.01953125\n",
      "Validation Loss: 35730.6953125\n",
      "Epoch 140, Loss 14317.279296875\n",
      "Validation Loss: 35608.58984375\n",
      "Validation Loss: 35487.6875\n",
      "Validation Loss: 35367.9609375\n",
      "Validation Loss: 35249.3984375\n",
      "Validation Loss: 35131.9765625\n",
      "Validation Loss: 35015.6796875\n",
      "Validation Loss: 34900.484375\n",
      "Validation Loss: 34786.38671875\n",
      "Validation Loss: 34673.34765625\n",
      "Validation Loss: 34561.37109375\n",
      "Epoch 150, Loss 13846.013671875\n",
      "Validation Loss: 34450.4296875\n",
      "Validation Loss: 34340.5078125\n",
      "Validation Loss: 34231.58984375\n",
      "Validation Loss: 34123.6640625\n",
      "Validation Loss: 34016.703125\n",
      "Validation Loss: 33910.703125\n",
      "Validation Loss: 33805.65625\n",
      "Validation Loss: 33701.546875\n",
      "Validation Loss: 33598.3359375\n",
      "Validation Loss: 33496.0390625\n",
      "Epoch 160, Loss 13416.5986328125\n",
      "Validation Loss: 33394.625\n",
      "Validation Loss: 33294.09375\n",
      "Validation Loss: 33194.41796875\n",
      "Validation Loss: 33095.6015625\n",
      "Validation Loss: 32997.62109375\n",
      "Validation Loss: 32900.4765625\n",
      "Validation Loss: 32804.1328125\n",
      "Validation Loss: 32708.59375\n",
      "Validation Loss: 32613.859375\n",
      "Validation Loss: 32519.89453125\n",
      "Epoch 170, Loss 13023.0830078125\n",
      "Validation Loss: 32426.708984375\n",
      "Validation Loss: 32334.271484375\n",
      "Validation Loss: 32242.59765625\n",
      "Validation Loss: 32151.6484375\n",
      "Validation Loss: 32061.4375\n",
      "Validation Loss: 31971.943359375\n",
      "Validation Loss: 31883.162109375\n",
      "Validation Loss: 31795.076171875\n",
      "Validation Loss: 31707.68359375\n",
      "Validation Loss: 31620.97265625\n",
      "Epoch 180, Loss 12660.64453125\n",
      "Validation Loss: 31534.935546875\n",
      "Validation Loss: 31449.5546875\n",
      "Validation Loss: 31364.83984375\n",
      "Validation Loss: 31280.76171875\n",
      "Validation Loss: 31197.33203125\n",
      "Validation Loss: 31114.5234375\n",
      "Validation Loss: 31032.33984375\n",
      "Validation Loss: 30950.7734375\n",
      "Validation Loss: 30869.8125\n",
      "Validation Loss: 30789.4453125\n",
      "Epoch 190, Loss 12325.3369140625\n",
      "Validation Loss: 30709.67578125\n",
      "Validation Loss: 30630.48828125\n",
      "Validation Loss: 30551.875\n",
      "Validation Loss: 30473.8359375\n",
      "Validation Loss: 30396.359375\n",
      "Validation Loss: 30319.43359375\n",
      "Validation Loss: 30243.05859375\n",
      "Validation Loss: 30167.2265625\n",
      "Validation Loss: 30091.9296875\n",
      "Validation Loss: 30017.16796875\n",
      "Epoch 200, Loss 12013.876953125\n",
      "Validation Loss: 29942.92578125\n",
      "Validation Loss: 29869.201171875\n",
      "Validation Loss: 29795.984375\n",
      "Validation Loss: 29723.279296875\n",
      "Validation Loss: 29651.06640625\n",
      "Validation Loss: 29579.3515625\n",
      "Validation Loss: 29508.123046875\n",
      "Validation Loss: 29437.37890625\n",
      "Validation Loss: 29367.109375\n",
      "Validation Loss: 29297.310546875\n",
      "Epoch 210, Loss 11723.5224609375\n",
      "Validation Loss: 29227.982421875\n",
      "Validation Loss: 29159.10546875\n",
      "Validation Loss: 29090.685546875\n",
      "Validation Loss: 29022.72265625\n",
      "Validation Loss: 28955.193359375\n",
      "Validation Loss: 28888.12109375\n",
      "Validation Loss: 28821.4765625\n",
      "Validation Loss: 28755.255859375\n",
      "Validation Loss: 28689.46875\n",
      "Validation Loss: 28624.1015625\n",
      "Epoch 220, Loss 11451.947265625\n",
      "Validation Loss: 28559.1484375\n",
      "Validation Loss: 28494.61328125\n",
      "Validation Loss: 28430.48046875\n",
      "Validation Loss: 28366.755859375\n",
      "Validation Loss: 28303.42578125\n",
      "Validation Loss: 28240.4921875\n",
      "Validation Loss: 28177.951171875\n",
      "Validation Loss: 28115.796875\n",
      "Validation Loss: 28054.021484375\n",
      "Validation Loss: 27992.62890625\n",
      "Epoch 230, Loss 11197.1796875\n",
      "Validation Loss: 27931.607421875\n",
      "Validation Loss: 27870.958984375\n",
      "Validation Loss: 27810.68359375\n",
      "Validation Loss: 27750.76171875\n",
      "Validation Loss: 27691.20703125\n",
      "Validation Loss: 27632.0\n",
      "Validation Loss: 27573.146484375\n",
      "Validation Loss: 27514.6484375\n",
      "Validation Loss: 27456.48828125\n",
      "Validation Loss: 27398.67578125\n",
      "Epoch 240, Loss 10957.51953125\n",
      "Validation Loss: 27341.1953125\n",
      "Validation Loss: 27284.0546875\n",
      "Validation Loss: 27227.248046875\n",
      "Validation Loss: 27170.765625\n",
      "Validation Loss: 27114.609375\n",
      "Validation Loss: 27058.7734375\n",
      "Validation Loss: 27003.2578125\n",
      "Validation Loss: 26948.0625\n",
      "Validation Loss: 26893.17578125\n",
      "Validation Loss: 26838.59375\n",
      "Epoch 250, Loss 10731.498046875\n",
      "Validation Loss: 26784.33203125\n",
      "Validation Loss: 26730.365234375\n",
      "Validation Loss: 26676.6953125\n",
      "Validation Loss: 26623.33203125\n",
      "Validation Loss: 26570.255859375\n",
      "Validation Loss: 26517.484375\n",
      "Validation Loss: 26464.9921875\n",
      "Validation Loss: 26412.79296875\n",
      "Validation Loss: 26360.875\n",
      "Validation Loss: 26309.2421875\n",
      "Epoch 260, Loss 10517.84765625\n",
      "Validation Loss: 26257.88671875\n",
      "Validation Loss: 26206.8046875\n",
      "Validation Loss: 26156.00390625\n",
      "Validation Loss: 26105.47265625\n",
      "Validation Loss: 26055.208984375\n",
      "Validation Loss: 26005.208984375\n",
      "Validation Loss: 25955.4765625\n",
      "Validation Loss: 25906.0078125\n",
      "Validation Loss: 25856.794921875\n",
      "Validation Loss: 25807.841796875\n",
      "Epoch 270, Loss 10315.453125\n",
      "Validation Loss: 25759.14453125\n",
      "Validation Loss: 25710.69921875\n",
      "Validation Loss: 25662.5\n",
      "Validation Loss: 25614.5546875\n",
      "Validation Loss: 25566.857421875\n",
      "Validation Loss: 25519.3984375\n",
      "Validation Loss: 25472.18359375\n",
      "Validation Loss: 25425.2109375\n",
      "Validation Loss: 25378.46875\n",
      "Validation Loss: 25331.974609375\n",
      "Epoch 280, Loss 10123.34765625\n",
      "Validation Loss: 25285.70703125\n",
      "Validation Loss: 25239.669921875\n",
      "Validation Loss: 25193.8671875\n",
      "Validation Loss: 25148.2890625\n",
      "Validation Loss: 25102.93359375\n",
      "Validation Loss: 25057.810546875\n",
      "Validation Loss: 25012.904296875\n",
      "Validation Loss: 24968.216796875\n",
      "Validation Loss: 24923.75390625\n",
      "Validation Loss: 24879.5\n",
      "Epoch 290, Loss 9940.6591796875\n",
      "Validation Loss: 24835.46484375\n",
      "Validation Loss: 24791.64453125\n",
      "Validation Loss: 24748.02734375\n",
      "Validation Loss: 24704.62890625\n",
      "Validation Loss: 24661.435546875\n",
      "Validation Loss: 24618.44921875\n",
      "Validation Loss: 24575.662109375\n",
      "Validation Loss: 24533.083984375\n",
      "Validation Loss: 24490.70703125\n",
      "Validation Loss: 24448.5234375\n",
      "Epoch 300, Loss 9766.63671875\n",
      "Validation Loss: 24406.54296875\n",
      "Validation Loss: 24364.755859375\n",
      "Validation Loss: 24323.16796875\n",
      "Validation Loss: 24281.771484375\n",
      "Validation Loss: 24240.56640625\n",
      "Validation Loss: 24199.55078125\n",
      "Validation Loss: 24158.724609375\n",
      "Validation Loss: 24118.08984375\n",
      "Validation Loss: 24077.6328125\n",
      "Validation Loss: 24037.365234375\n",
      "Epoch 310, Loss 9600.591796875\n",
      "Validation Loss: 23997.279296875\n",
      "Validation Loss: 23957.373046875\n",
      "Validation Loss: 23917.6484375\n",
      "Validation Loss: 23878.1015625\n",
      "Validation Loss: 23838.734375\n",
      "Validation Loss: 23799.54296875\n",
      "Validation Loss: 23760.5234375\n",
      "Validation Loss: 23721.6796875\n",
      "Validation Loss: 23683.0078125\n",
      "Validation Loss: 23644.505859375\n",
      "Epoch 320, Loss 9441.9228515625\n",
      "Validation Loss: 23606.177734375\n",
      "Validation Loss: 23568.013671875\n",
      "Validation Loss: 23530.01953125\n",
      "Validation Loss: 23492.18359375\n",
      "Validation Loss: 23454.51953125\n",
      "Validation Loss: 23417.013671875\n",
      "Validation Loss: 23379.669921875\n",
      "Validation Loss: 23342.4921875\n",
      "Validation Loss: 23305.474609375\n",
      "Validation Loss: 23268.61328125\n",
      "Epoch 330, Loss 9290.083984375\n",
      "Validation Loss: 23231.904296875\n",
      "Validation Loss: 23195.357421875\n",
      "Validation Loss: 23158.96484375\n",
      "Validation Loss: 23122.72265625\n",
      "Validation Loss: 23086.640625\n",
      "Validation Loss: 23050.703125\n",
      "Validation Loss: 23014.921875\n",
      "Validation Loss: 22979.28125\n",
      "Validation Loss: 22943.80078125\n",
      "Validation Loss: 22908.458984375\n",
      "Epoch 340, Loss 9144.5908203125\n",
      "Validation Loss: 22873.26953125\n",
      "Validation Loss: 22838.216796875\n",
      "Validation Loss: 22803.318359375\n",
      "Validation Loss: 22768.55859375\n",
      "Validation Loss: 22733.943359375\n",
      "Validation Loss: 22699.466796875\n",
      "Validation Loss: 22665.130859375\n",
      "Validation Loss: 22630.935546875\n",
      "Validation Loss: 22596.87890625\n",
      "Validation Loss: 22562.9609375\n",
      "Epoch 350, Loss 9005.0048828125\n",
      "Validation Loss: 22529.1796875\n",
      "Validation Loss: 22495.53125\n",
      "Validation Loss: 22462.015625\n",
      "Validation Loss: 22428.638671875\n",
      "Validation Loss: 22395.39453125\n",
      "Validation Loss: 22362.279296875\n",
      "Validation Loss: 22329.294921875\n",
      "Validation Loss: 22296.4375\n",
      "Validation Loss: 22263.716796875\n",
      "Validation Loss: 22231.12109375\n",
      "Epoch 360, Loss 8870.9189453125\n",
      "Validation Loss: 22198.65625\n",
      "Validation Loss: 22166.3125\n",
      "Validation Loss: 22134.09765625\n",
      "Validation Loss: 22102.0078125\n",
      "Validation Loss: 22070.04296875\n",
      "Validation Loss: 22038.19921875\n",
      "Validation Loss: 22006.48046875\n",
      "Validation Loss: 21974.880859375\n",
      "Validation Loss: 21943.40234375\n",
      "Validation Loss: 21912.044921875\n",
      "Epoch 370, Loss 8741.9765625\n",
      "Validation Loss: 21880.8046875\n",
      "Validation Loss: 21849.685546875\n",
      "Validation Loss: 21818.6875\n",
      "Validation Loss: 21787.80078125\n",
      "Validation Loss: 21757.03515625\n",
      "Validation Loss: 21726.380859375\n",
      "Validation Loss: 21695.841796875\n",
      "Validation Loss: 21665.41796875\n",
      "Validation Loss: 21635.111328125\n",
      "Validation Loss: 21604.916015625\n",
      "Epoch 380, Loss 8617.8515625\n",
      "Validation Loss: 21574.828125\n",
      "Validation Loss: 21544.85546875\n",
      "Validation Loss: 21514.990234375\n",
      "Validation Loss: 21485.23828125\n",
      "Validation Loss: 21455.595703125\n",
      "Validation Loss: 21426.05859375\n",
      "Validation Loss: 21396.630859375\n",
      "Validation Loss: 21367.3125\n",
      "Validation Loss: 21338.08984375\n",
      "Validation Loss: 21308.982421875\n",
      "Epoch 390, Loss 8498.236328125\n",
      "Validation Loss: 21279.9765625\n",
      "Validation Loss: 21251.078125\n",
      "Validation Loss: 21222.28515625\n",
      "Validation Loss: 21193.591796875\n",
      "Validation Loss: 21165.00390625\n",
      "Validation Loss: 21136.513671875\n",
      "Validation Loss: 21108.12890625\n",
      "Validation Loss: 21079.83984375\n",
      "Validation Loss: 21051.658203125\n",
      "Validation Loss: 21023.568359375\n",
      "Epoch 400, Loss 8382.8642578125\n",
      "Validation Loss: 20995.583984375\n",
      "Validation Loss: 20967.6953125\n",
      "Validation Loss: 20939.90234375\n",
      "Validation Loss: 20912.20703125\n",
      "Validation Loss: 20884.609375\n",
      "Validation Loss: 20857.109375\n",
      "Validation Loss: 20829.703125\n",
      "Validation Loss: 20802.388671875\n",
      "Validation Loss: 20775.17578125\n",
      "Validation Loss: 20748.05078125\n",
      "Epoch 410, Loss 8271.4775390625\n",
      "Validation Loss: 20721.021484375\n",
      "Validation Loss: 20694.08203125\n",
      "Validation Loss: 20667.23046875\n",
      "Validation Loss: 20640.478515625\n",
      "Validation Loss: 20613.814453125\n",
      "Validation Loss: 20587.2421875\n",
      "Validation Loss: 20560.7578125\n",
      "Validation Loss: 20534.3671875\n",
      "Validation Loss: 20508.0625\n",
      "Validation Loss: 20481.849609375\n",
      "Epoch 420, Loss 8163.84619140625\n",
      "Validation Loss: 20455.71875\n",
      "Validation Loss: 20429.6796875\n",
      "Validation Loss: 20403.72265625\n",
      "Validation Loss: 20377.85546875\n",
      "Validation Loss: 20352.07421875\n",
      "Validation Loss: 20326.376953125\n",
      "Validation Loss: 20300.767578125\n",
      "Validation Loss: 20275.2421875\n",
      "Validation Loss: 20249.80078125\n",
      "Validation Loss: 20224.4375\n",
      "Epoch 430, Loss 8059.76220703125\n",
      "Validation Loss: 20199.1640625\n",
      "Validation Loss: 20173.96875\n",
      "Validation Loss: 20148.859375\n",
      "Validation Loss: 20123.828125\n",
      "Validation Loss: 20098.87890625\n",
      "Validation Loss: 20074.01171875\n",
      "Validation Loss: 20049.2265625\n",
      "Validation Loss: 20024.521484375\n",
      "Validation Loss: 19999.890625\n",
      "Validation Loss: 19975.34375\n",
      "Epoch 440, Loss 7959.02978515625\n",
      "Validation Loss: 19950.869140625\n",
      "Validation Loss: 19926.4765625\n",
      "Validation Loss: 19902.1640625\n",
      "Validation Loss: 19877.921875\n",
      "Validation Loss: 19853.763671875\n",
      "Validation Loss: 19829.677734375\n",
      "Validation Loss: 19805.66796875\n",
      "Validation Loss: 19781.736328125\n",
      "Validation Loss: 19757.875\n",
      "Validation Loss: 19734.095703125\n",
      "Epoch 450, Loss 7861.45947265625\n",
      "Validation Loss: 19710.390625\n",
      "Validation Loss: 19686.75\n",
      "Validation Loss: 19663.19140625\n",
      "Validation Loss: 19639.69921875\n",
      "Validation Loss: 19616.2890625\n",
      "Validation Loss: 19592.94140625\n",
      "Validation Loss: 19569.673828125\n",
      "Validation Loss: 19546.474609375\n",
      "Validation Loss: 19523.349609375\n",
      "Validation Loss: 19500.29296875\n",
      "Epoch 460, Loss 7766.890625\n",
      "Validation Loss: 19477.30859375\n",
      "Validation Loss: 19454.388671875\n",
      "Validation Loss: 19431.541015625\n",
      "Validation Loss: 19408.765625\n",
      "Validation Loss: 19386.056640625\n",
      "Validation Loss: 19363.416015625\n",
      "Validation Loss: 19340.84375\n",
      "Validation Loss: 19318.33984375\n",
      "Validation Loss: 19295.908203125\n",
      "Validation Loss: 19273.5390625\n",
      "Epoch 470, Loss 7675.16748046875\n",
      "Validation Loss: 19251.236328125\n",
      "Validation Loss: 19229.00390625\n",
      "Validation Loss: 19206.8359375\n",
      "Validation Loss: 19184.734375\n",
      "Validation Loss: 19162.6953125\n",
      "Validation Loss: 19140.7265625\n",
      "Validation Loss: 19118.81640625\n",
      "Validation Loss: 19096.978515625\n",
      "Validation Loss: 19075.203125\n",
      "Validation Loss: 19053.48828125\n",
      "Epoch 480, Loss 7586.14501953125\n",
      "Validation Loss: 19031.8359375\n",
      "Validation Loss: 19010.25\n",
      "Validation Loss: 18988.7265625\n",
      "Validation Loss: 18967.265625\n",
      "Validation Loss: 18945.8671875\n",
      "Validation Loss: 18924.53125\n",
      "Validation Loss: 18903.2578125\n",
      "Validation Loss: 18882.04296875\n",
      "Validation Loss: 18860.890625\n",
      "Validation Loss: 18839.798828125\n",
      "Epoch 490, Loss 7499.68994140625\n",
      "Validation Loss: 18818.76953125\n",
      "Validation Loss: 18797.798828125\n",
      "Validation Loss: 18776.88671875\n",
      "Validation Loss: 18756.03515625\n",
      "Validation Loss: 18735.248046875\n",
      "Validation Loss: 18714.51171875\n",
      "Validation Loss: 18693.83984375\n",
      "Validation Loss: 18673.22265625\n",
      "Validation Loss: 18652.66796875\n",
      "Validation Loss: 18632.1640625\n",
      "Epoch 500, Loss 7415.67822265625\n",
      "Validation Loss: 18611.72265625\n",
      "Validation Loss: 18591.33984375\n",
      "Validation Loss: 18571.015625\n",
      "Validation Loss: 18550.740234375\n",
      "Validation Loss: 18530.529296875\n",
      "Validation Loss: 18510.37109375\n",
      "Validation Loss: 18490.267578125\n",
      "Validation Loss: 18470.22265625\n",
      "Validation Loss: 18450.23046875\n",
      "Validation Loss: 18430.296875\n",
      "Epoch 510, Loss 7333.98974609375\n",
      "Validation Loss: 18410.4140625\n",
      "Validation Loss: 18390.587890625\n",
      "Validation Loss: 18370.818359375\n",
      "Validation Loss: 18351.09765625\n",
      "Validation Loss: 18331.4375\n",
      "Validation Loss: 18311.828125\n",
      "Validation Loss: 18292.26953125\n",
      "Validation Loss: 18272.76953125\n",
      "Validation Loss: 18253.318359375\n",
      "Validation Loss: 18233.921875\n",
      "Epoch 520, Loss 7254.51708984375\n",
      "Validation Loss: 18214.576171875\n",
      "Validation Loss: 18195.28125\n",
      "Validation Loss: 18176.04296875\n",
      "Validation Loss: 18156.8515625\n",
      "Validation Loss: 18137.712890625\n",
      "Validation Loss: 18118.625\n",
      "Validation Loss: 18099.591796875\n",
      "Validation Loss: 18080.60546875\n",
      "Validation Loss: 18061.671875\n",
      "Validation Loss: 18042.7890625\n",
      "Epoch 530, Loss 7177.16015625\n",
      "Validation Loss: 18023.953125\n",
      "Validation Loss: 18005.16796875\n",
      "Validation Loss: 17986.43359375\n",
      "Validation Loss: 17967.75\n",
      "Validation Loss: 17949.11328125\n",
      "Validation Loss: 17930.525390625\n",
      "Validation Loss: 17911.986328125\n",
      "Validation Loss: 17893.49609375\n",
      "Validation Loss: 17875.0546875\n",
      "Validation Loss: 17856.662109375\n",
      "Epoch 540, Loss 7101.8232421875\n",
      "Validation Loss: 17838.314453125\n",
      "Validation Loss: 17820.015625\n",
      "Validation Loss: 17801.767578125\n",
      "Validation Loss: 17783.5625\n",
      "Validation Loss: 17765.408203125\n",
      "Validation Loss: 17747.296875\n",
      "Validation Loss: 17729.234375\n",
      "Validation Loss: 17711.21484375\n",
      "Validation Loss: 17693.24609375\n",
      "Validation Loss: 17675.322265625\n",
      "Epoch 550, Loss 7028.416015625\n",
      "Validation Loss: 17657.44140625\n",
      "Validation Loss: 17639.609375\n",
      "Validation Loss: 17621.8203125\n",
      "Validation Loss: 17604.076171875\n",
      "Validation Loss: 17586.380859375\n",
      "Validation Loss: 17568.724609375\n",
      "Validation Loss: 17551.12109375\n",
      "Validation Loss: 17533.552734375\n",
      "Validation Loss: 17516.03125\n",
      "Validation Loss: 17498.556640625\n",
      "Epoch 560, Loss 6956.85595703125\n",
      "Validation Loss: 17481.125\n",
      "Validation Loss: 17463.734375\n",
      "Validation Loss: 17446.390625\n",
      "Validation Loss: 17429.0859375\n",
      "Validation Loss: 17411.828125\n",
      "Validation Loss: 17394.61328125\n",
      "Validation Loss: 17377.4375\n",
      "Validation Loss: 17360.306640625\n",
      "Validation Loss: 17343.21875\n",
      "Validation Loss: 17326.171875\n",
      "Epoch 570, Loss 6887.0634765625\n",
      "Validation Loss: 17309.16796875\n",
      "Validation Loss: 17292.203125\n",
      "Validation Loss: 17275.283203125\n",
      "Validation Loss: 17258.404296875\n",
      "Validation Loss: 17241.56640625\n",
      "Validation Loss: 17224.767578125\n",
      "Validation Loss: 17208.01171875\n",
      "Validation Loss: 17191.29296875\n",
      "Validation Loss: 17174.62109375\n",
      "Validation Loss: 17157.986328125\n",
      "Epoch 580, Loss 6818.9658203125\n",
      "Validation Loss: 17141.390625\n",
      "Validation Loss: 17124.833984375\n",
      "Validation Loss: 17108.322265625\n",
      "Validation Loss: 17091.845703125\n",
      "Validation Loss: 17075.41015625\n",
      "Validation Loss: 17059.015625\n",
      "Validation Loss: 17042.658203125\n",
      "Validation Loss: 17026.33984375\n",
      "Validation Loss: 17010.060546875\n",
      "Validation Loss: 16993.8203125\n",
      "Epoch 590, Loss 6752.490234375\n",
      "Validation Loss: 16977.619140625\n",
      "Validation Loss: 16961.455078125\n",
      "Validation Loss: 16945.333984375\n",
      "Validation Loss: 16929.24609375\n",
      "Validation Loss: 16913.197265625\n",
      "Validation Loss: 16897.1875\n",
      "Validation Loss: 16881.212890625\n",
      "Validation Loss: 16865.27734375\n",
      "Validation Loss: 16849.3828125\n",
      "Validation Loss: 16833.51953125\n",
      "Epoch 600, Loss 6687.57373046875\n",
      "Validation Loss: 16817.693359375\n",
      "Validation Loss: 16801.908203125\n",
      "Validation Loss: 16786.158203125\n",
      "Validation Loss: 16770.443359375\n",
      "Validation Loss: 16754.765625\n",
      "Validation Loss: 16739.125\n",
      "Validation Loss: 16723.51953125\n",
      "Validation Loss: 16707.953125\n",
      "Validation Loss: 16692.419921875\n",
      "Validation Loss: 16676.921875\n",
      "Epoch 610, Loss 6624.15185546875\n",
      "Validation Loss: 16661.4609375\n",
      "Validation Loss: 16646.033203125\n",
      "Validation Loss: 16630.64453125\n",
      "Validation Loss: 16615.287109375\n",
      "Validation Loss: 16599.96875\n",
      "Validation Loss: 16584.681640625\n",
      "Validation Loss: 16569.431640625\n",
      "Validation Loss: 16554.21484375\n",
      "Validation Loss: 16539.03125\n",
      "Validation Loss: 16523.8828125\n",
      "Epoch 620, Loss 6562.16943359375\n",
      "Validation Loss: 16508.7734375\n",
      "Validation Loss: 16493.6953125\n",
      "Validation Loss: 16478.6484375\n",
      "Validation Loss: 16463.63671875\n",
      "Validation Loss: 16448.65625\n",
      "Validation Loss: 16433.71484375\n",
      "Validation Loss: 16418.8046875\n",
      "Validation Loss: 16403.92578125\n",
      "Validation Loss: 16389.08203125\n",
      "Validation Loss: 16374.2724609375\n",
      "Epoch 630, Loss 6501.56591796875\n",
      "Validation Loss: 16359.4921875\n",
      "Validation Loss: 16344.748046875\n",
      "Validation Loss: 16330.0341796875\n",
      "Validation Loss: 16315.35546875\n",
      "Validation Loss: 16300.708984375\n",
      "Validation Loss: 16286.08984375\n",
      "Validation Loss: 16271.509765625\n",
      "Validation Loss: 16256.958984375\n",
      "Validation Loss: 16242.4404296875\n",
      "Validation Loss: 16227.9521484375\n",
      "Epoch 640, Loss 6442.29638671875\n",
      "Validation Loss: 16213.49609375\n",
      "Validation Loss: 16199.0703125\n",
      "Validation Loss: 16184.67578125\n",
      "Validation Loss: 16170.31640625\n",
      "Validation Loss: 16155.984375\n",
      "Validation Loss: 16141.685546875\n",
      "Validation Loss: 16127.4169921875\n",
      "Validation Loss: 16113.181640625\n",
      "Validation Loss: 16098.974609375\n",
      "Validation Loss: 16084.798828125\n",
      "Epoch 650, Loss 6384.30224609375\n",
      "Validation Loss: 16070.654296875\n",
      "Validation Loss: 16056.5380859375\n",
      "Validation Loss: 16042.453125\n",
      "Validation Loss: 16028.3974609375\n",
      "Validation Loss: 16014.3740234375\n",
      "Validation Loss: 16000.380859375\n",
      "Validation Loss: 15986.4140625\n",
      "Validation Loss: 15972.482421875\n",
      "Validation Loss: 15958.576171875\n",
      "Validation Loss: 15944.6982421875\n",
      "Epoch 660, Loss 6327.54052734375\n",
      "Validation Loss: 15930.853515625\n",
      "Validation Loss: 15917.037109375\n",
      "Validation Loss: 15903.2490234375\n",
      "Validation Loss: 15889.490234375\n",
      "Validation Loss: 15875.7607421875\n",
      "Validation Loss: 15862.0576171875\n",
      "Validation Loss: 15848.3857421875\n",
      "Validation Loss: 15834.7451171875\n",
      "Validation Loss: 15821.12890625\n",
      "Validation Loss: 15807.54296875\n",
      "Epoch 670, Loss 6271.96923828125\n",
      "Validation Loss: 15793.986328125\n",
      "Validation Loss: 15780.4560546875\n",
      "Validation Loss: 15766.953125\n",
      "Validation Loss: 15753.478515625\n",
      "Validation Loss: 15740.03515625\n",
      "Validation Loss: 15726.6171875\n",
      "Validation Loss: 15713.2255859375\n",
      "Validation Loss: 15699.8623046875\n",
      "Validation Loss: 15686.529296875\n",
      "Validation Loss: 15673.220703125\n",
      "Epoch 680, Loss 6217.541015625\n",
      "Validation Loss: 15659.9375\n",
      "Validation Loss: 15646.689453125\n",
      "Validation Loss: 15633.462890625\n",
      "Validation Loss: 15620.263671875\n",
      "Validation Loss: 15607.09375\n",
      "Validation Loss: 15593.9482421875\n",
      "Validation Loss: 15580.830078125\n",
      "Validation Loss: 15567.73828125\n",
      "Validation Loss: 15554.673828125\n",
      "Validation Loss: 15541.634765625\n",
      "Epoch 690, Loss 6164.21923828125\n",
      "Validation Loss: 15528.6240234375\n",
      "Validation Loss: 15515.638671875\n",
      "Validation Loss: 15502.681640625\n",
      "Validation Loss: 15489.748046875\n",
      "Validation Loss: 15476.841796875\n",
      "Validation Loss: 15463.9599609375\n",
      "Validation Loss: 15451.103515625\n",
      "Validation Loss: 15438.275390625\n",
      "Validation Loss: 15425.4716796875\n",
      "Validation Loss: 15412.693359375\n",
      "Epoch 700, Loss 6111.96337890625\n",
      "Validation Loss: 15399.9404296875\n",
      "Validation Loss: 15387.212890625\n",
      "Validation Loss: 15374.51171875\n",
      "Validation Loss: 15361.8349609375\n",
      "Validation Loss: 15349.1826171875\n",
      "Validation Loss: 15336.5546875\n",
      "Validation Loss: 15323.9560546875\n",
      "Validation Loss: 15311.3779296875\n",
      "Validation Loss: 15298.8271484375\n",
      "Validation Loss: 15286.298828125\n",
      "Epoch 710, Loss 6060.73779296875\n",
      "Validation Loss: 15273.798828125\n",
      "Validation Loss: 15261.318359375\n",
      "Validation Loss: 15248.8671875\n",
      "Validation Loss: 15236.439453125\n",
      "Validation Loss: 15224.0341796875\n",
      "Validation Loss: 15211.654296875\n",
      "Validation Loss: 15199.296875\n",
      "Validation Loss: 15186.9638671875\n",
      "Validation Loss: 15174.658203125\n",
      "Validation Loss: 15162.373046875\n",
      "Epoch 720, Loss 6010.509765625\n",
      "Validation Loss: 15150.111328125\n",
      "Validation Loss: 15137.876953125\n",
      "Validation Loss: 15125.6630859375\n",
      "Validation Loss: 15113.47265625\n",
      "Validation Loss: 15101.30859375\n",
      "Validation Loss: 15089.1640625\n",
      "Validation Loss: 15077.0458984375\n",
      "Validation Loss: 15064.94921875\n",
      "Validation Loss: 15052.8779296875\n",
      "Validation Loss: 15040.828125\n",
      "Epoch 730, Loss 5961.2412109375\n",
      "Validation Loss: 15028.7998046875\n",
      "Validation Loss: 15016.798828125\n",
      "Validation Loss: 15004.814453125\n",
      "Validation Loss: 14992.8583984375\n",
      "Validation Loss: 14980.923828125\n",
      "Validation Loss: 14969.013671875\n",
      "Validation Loss: 14957.1201171875\n",
      "Validation Loss: 14945.2529296875\n",
      "Validation Loss: 14933.4091796875\n",
      "Validation Loss: 14921.587890625\n",
      "Epoch 740, Loss 5912.9052734375\n",
      "Validation Loss: 14909.78515625\n",
      "Validation Loss: 14898.00390625\n",
      "Validation Loss: 14886.25\n",
      "Validation Loss: 14874.5185546875\n",
      "Validation Loss: 14862.8046875\n",
      "Validation Loss: 14851.1123046875\n",
      "Validation Loss: 14839.4453125\n",
      "Validation Loss: 14827.798828125\n",
      "Validation Loss: 14816.173828125\n",
      "Validation Loss: 14804.572265625\n",
      "Epoch 750, Loss 5865.47119140625\n",
      "Validation Loss: 14792.990234375\n",
      "Validation Loss: 14781.4296875\n",
      "Validation Loss: 14769.8896484375\n",
      "Validation Loss: 14758.3740234375\n",
      "Validation Loss: 14746.8779296875\n",
      "Validation Loss: 14735.40234375\n",
      "Validation Loss: 14723.951171875\n",
      "Validation Loss: 14712.517578125\n",
      "Validation Loss: 14701.107421875\n",
      "Validation Loss: 14689.712890625\n",
      "Epoch 760, Loss 5818.9052734375\n",
      "Validation Loss: 14678.34375\n",
      "Validation Loss: 14666.99609375\n",
      "Validation Loss: 14655.669921875\n",
      "Validation Loss: 14644.361328125\n",
      "Validation Loss: 14633.076171875\n",
      "Validation Loss: 14621.810546875\n",
      "Validation Loss: 14610.564453125\n",
      "Validation Loss: 14599.3388671875\n",
      "Validation Loss: 14588.1318359375\n",
      "Validation Loss: 14576.94921875\n",
      "Epoch 770, Loss 5773.185546875\n",
      "Validation Loss: 14565.78515625\n",
      "Validation Loss: 14554.6396484375\n",
      "Validation Loss: 14543.5146484375\n",
      "Validation Loss: 14532.412109375\n",
      "Validation Loss: 14521.328125\n",
      "Validation Loss: 14510.263671875\n",
      "Validation Loss: 14499.21875\n",
      "Validation Loss: 14488.1953125\n",
      "Validation Loss: 14477.1904296875\n",
      "Validation Loss: 14466.2060546875\n",
      "Epoch 780, Loss 5728.28271484375\n",
      "Validation Loss: 14455.2412109375\n",
      "Validation Loss: 14444.294921875\n",
      "Validation Loss: 14433.3701171875\n",
      "Validation Loss: 14422.4638671875\n",
      "Validation Loss: 14411.576171875\n",
      "Validation Loss: 14400.70703125\n",
      "Validation Loss: 14389.859375\n",
      "Validation Loss: 14379.0302734375\n",
      "Validation Loss: 14368.2177734375\n",
      "Validation Loss: 14357.42578125\n",
      "Epoch 790, Loss 5684.17431640625\n",
      "Validation Loss: 14346.6552734375\n",
      "Validation Loss: 14335.9013671875\n",
      "Validation Loss: 14325.1640625\n",
      "Validation Loss: 14314.44921875\n",
      "Validation Loss: 14303.75390625\n",
      "Validation Loss: 14293.076171875\n",
      "Validation Loss: 14282.4130859375\n",
      "Validation Loss: 14271.7763671875\n",
      "Validation Loss: 14261.1513671875\n",
      "Validation Loss: 14250.546875\n",
      "Epoch 800, Loss 5640.83544921875\n",
      "Validation Loss: 14239.962890625\n",
      "Validation Loss: 14229.396484375\n",
      "Validation Loss: 14218.84765625\n",
      "Validation Loss: 14208.31640625\n",
      "Validation Loss: 14197.8037109375\n",
      "Validation Loss: 14187.310546875\n",
      "Validation Loss: 14176.8349609375\n",
      "Validation Loss: 14166.376953125\n",
      "Validation Loss: 14155.9365234375\n",
      "Validation Loss: 14145.5146484375\n",
      "Epoch 810, Loss 5598.23974609375\n",
      "Validation Loss: 14135.1123046875\n",
      "Validation Loss: 14124.724609375\n",
      "Validation Loss: 14114.357421875\n",
      "Validation Loss: 14104.0068359375\n",
      "Validation Loss: 14093.671875\n",
      "Validation Loss: 14083.357421875\n",
      "Validation Loss: 14073.060546875\n",
      "Validation Loss: 14062.779296875\n",
      "Validation Loss: 14052.5185546875\n",
      "Validation Loss: 14042.2724609375\n",
      "Epoch 820, Loss 5556.369140625\n",
      "Validation Loss: 14032.0458984375\n",
      "Validation Loss: 14021.8349609375\n",
      "Validation Loss: 14011.6435546875\n",
      "Validation Loss: 14001.466796875\n",
      "Validation Loss: 13991.3076171875\n",
      "Validation Loss: 13981.1640625\n",
      "Validation Loss: 13971.0419921875\n",
      "Validation Loss: 13960.935546875\n",
      "Validation Loss: 13950.8427734375\n",
      "Validation Loss: 13940.76953125\n",
      "Epoch 830, Loss 5515.2021484375\n",
      "Validation Loss: 13930.712890625\n",
      "Validation Loss: 13920.67578125\n",
      "Validation Loss: 13910.6513671875\n",
      "Validation Loss: 13900.646484375\n",
      "Validation Loss: 13890.65625\n",
      "Validation Loss: 13880.681640625\n",
      "Validation Loss: 13870.724609375\n",
      "Validation Loss: 13860.787109375\n",
      "Validation Loss: 13850.8623046875\n",
      "Validation Loss: 13840.9560546875\n",
      "Epoch 840, Loss 5474.716796875\n",
      "Validation Loss: 13831.064453125\n",
      "Validation Loss: 13821.189453125\n",
      "Validation Loss: 13811.3310546875\n",
      "Validation Loss: 13801.4912109375\n",
      "Validation Loss: 13791.666015625\n",
      "Validation Loss: 13781.85546875\n",
      "Validation Loss: 13772.0615234375\n",
      "Validation Loss: 13762.28515625\n",
      "Validation Loss: 13752.5234375\n",
      "Validation Loss: 13742.77734375\n",
      "Epoch 850, Loss 5434.89453125\n",
      "Validation Loss: 13733.046875\n",
      "Validation Loss: 13723.3349609375\n",
      "Validation Loss: 13713.638671875\n",
      "Validation Loss: 13703.95703125\n",
      "Validation Loss: 13694.2890625\n",
      "Validation Loss: 13684.638671875\n",
      "Validation Loss: 13675.00390625\n",
      "Validation Loss: 13665.38671875\n",
      "Validation Loss: 13655.7841796875\n",
      "Validation Loss: 13646.1953125\n",
      "Epoch 860, Loss 5395.716796875\n",
      "Validation Loss: 13636.623046875\n",
      "Validation Loss: 13627.0654296875\n",
      "Validation Loss: 13617.5234375\n",
      "Validation Loss: 13607.99609375\n",
      "Validation Loss: 13598.4873046875\n",
      "Validation Loss: 13588.9921875\n",
      "Validation Loss: 13579.5107421875\n",
      "Validation Loss: 13570.044921875\n",
      "Validation Loss: 13560.59375\n",
      "Validation Loss: 13551.16015625\n",
      "Epoch 870, Loss 5357.16455078125\n",
      "Validation Loss: 13541.7412109375\n",
      "Validation Loss: 13532.3349609375\n",
      "Validation Loss: 13522.947265625\n",
      "Validation Loss: 13513.5693359375\n",
      "Validation Loss: 13504.2099609375\n",
      "Validation Loss: 13494.86328125\n",
      "Validation Loss: 13485.5341796875\n",
      "Validation Loss: 13476.2177734375\n",
      "Validation Loss: 13466.91796875\n",
      "Validation Loss: 13457.6318359375\n",
      "Epoch 880, Loss 5319.22119140625\n",
      "Validation Loss: 13448.3583984375\n",
      "Validation Loss: 13439.1005859375\n",
      "Validation Loss: 13429.8583984375\n",
      "Validation Loss: 13420.6328125\n",
      "Validation Loss: 13411.4169921875\n",
      "Validation Loss: 13402.2177734375\n",
      "Validation Loss: 13393.03125\n",
      "Validation Loss: 13383.86328125\n",
      "Validation Loss: 13374.70703125\n",
      "Validation Loss: 13365.56640625\n",
      "Epoch 890, Loss 5281.86962890625\n",
      "Validation Loss: 13356.4375\n",
      "Validation Loss: 13347.32421875\n",
      "Validation Loss: 13338.2255859375\n",
      "Validation Loss: 13329.1396484375\n",
      "Validation Loss: 13320.068359375\n",
      "Validation Loss: 13311.0107421875\n",
      "Validation Loss: 13301.970703125\n",
      "Validation Loss: 13292.9404296875\n",
      "Validation Loss: 13283.923828125\n",
      "Validation Loss: 13274.923828125\n",
      "Epoch 900, Loss 5245.0947265625\n",
      "Validation Loss: 13265.9365234375\n",
      "Validation Loss: 13256.9638671875\n",
      "Validation Loss: 13248.0029296875\n",
      "Validation Loss: 13239.056640625\n",
      "Validation Loss: 13230.125\n",
      "Validation Loss: 13221.208984375\n",
      "Validation Loss: 13212.3037109375\n",
      "Validation Loss: 13203.4130859375\n",
      "Validation Loss: 13194.53515625\n",
      "Validation Loss: 13185.671875\n",
      "Epoch 910, Loss 5208.88134765625\n",
      "Validation Loss: 13176.8203125\n",
      "Validation Loss: 13167.982421875\n",
      "Validation Loss: 13159.1591796875\n",
      "Validation Loss: 13150.3505859375\n",
      "Validation Loss: 13141.5546875\n",
      "Validation Loss: 13132.771484375\n",
      "Validation Loss: 13123.9990234375\n",
      "Validation Loss: 13115.2421875\n",
      "Validation Loss: 13106.49609375\n",
      "Validation Loss: 13097.7685546875\n",
      "Epoch 920, Loss 5173.21337890625\n",
      "Validation Loss: 13089.05078125\n",
      "Validation Loss: 13080.3466796875\n",
      "Validation Loss: 13071.654296875\n",
      "Validation Loss: 13062.978515625\n",
      "Validation Loss: 13054.3125\n",
      "Validation Loss: 13045.66015625\n",
      "Validation Loss: 13037.01953125\n",
      "Validation Loss: 13028.39453125\n",
      "Validation Loss: 13019.78125\n",
      "Validation Loss: 13011.1826171875\n",
      "Epoch 930, Loss 5138.0791015625\n",
      "Validation Loss: 13002.5927734375\n",
      "Validation Loss: 12994.017578125\n",
      "Validation Loss: 12985.4560546875\n",
      "Validation Loss: 12976.908203125\n",
      "Validation Loss: 12968.369140625\n",
      "Validation Loss: 12959.845703125\n",
      "Validation Loss: 12951.3349609375\n",
      "Validation Loss: 12942.837890625\n",
      "Validation Loss: 12934.3505859375\n",
      "Validation Loss: 12925.875\n",
      "Epoch 940, Loss 5103.4619140625\n",
      "Validation Loss: 12917.4140625\n",
      "Validation Loss: 12908.9638671875\n",
      "Validation Loss: 12900.52734375\n",
      "Validation Loss: 12892.10546875\n",
      "Validation Loss: 12883.693359375\n",
      "Validation Loss: 12875.2919921875\n",
      "Validation Loss: 12866.9052734375\n",
      "Validation Loss: 12858.53125\n",
      "Validation Loss: 12850.16796875\n",
      "Validation Loss: 12841.81640625\n",
      "Epoch 950, Loss 5069.35009765625\n",
      "Validation Loss: 12833.478515625\n",
      "Validation Loss: 12825.15234375\n",
      "Validation Loss: 12816.8388671875\n",
      "Validation Loss: 12808.53515625\n",
      "Validation Loss: 12800.24609375\n",
      "Validation Loss: 12791.96875\n",
      "Validation Loss: 12783.703125\n",
      "Validation Loss: 12775.4482421875\n",
      "Validation Loss: 12767.2060546875\n",
      "Validation Loss: 12758.9765625\n",
      "Epoch 960, Loss 5035.73046875\n",
      "Validation Loss: 12750.7568359375\n",
      "Validation Loss: 12742.552734375\n",
      "Validation Loss: 12734.3583984375\n",
      "Validation Loss: 12726.1748046875\n",
      "Validation Loss: 12718.00390625\n",
      "Validation Loss: 12709.84375\n",
      "Validation Loss: 12701.697265625\n",
      "Validation Loss: 12693.560546875\n",
      "Validation Loss: 12685.435546875\n",
      "Validation Loss: 12677.3232421875\n",
      "Epoch 970, Loss 5002.59228515625\n",
      "Validation Loss: 12669.21875\n",
      "Validation Loss: 12661.130859375\n",
      "Validation Loss: 12653.0537109375\n",
      "Validation Loss: 12644.986328125\n",
      "Validation Loss: 12636.927734375\n",
      "Validation Loss: 12628.884765625\n",
      "Validation Loss: 12620.8515625\n",
      "Validation Loss: 12612.8310546875\n",
      "Validation Loss: 12604.8203125\n",
      "Validation Loss: 12596.822265625\n",
      "Epoch 980, Loss 4969.9189453125\n",
      "Validation Loss: 12588.8349609375\n",
      "Validation Loss: 12580.8583984375\n",
      "Validation Loss: 12572.8935546875\n",
      "Validation Loss: 12564.939453125\n",
      "Validation Loss: 12556.99609375\n",
      "Validation Loss: 12549.064453125\n",
      "Validation Loss: 12541.14453125\n",
      "Validation Loss: 12533.234375\n",
      "Validation Loss: 12525.3359375\n",
      "Validation Loss: 12517.451171875\n",
      "Epoch 990, Loss 4937.70361328125\n",
      "Validation Loss: 12509.57421875\n",
      "Validation Loss: 12501.7099609375\n",
      "Validation Loss: 12493.85546875\n",
      "Validation Loss: 12486.0107421875\n",
      "Validation Loss: 12478.1787109375\n",
      "Validation Loss: 12470.3583984375\n",
      "Validation Loss: 12462.5498046875\n",
      "Validation Loss: 12454.75\n",
      "Validation Loss: 12446.9609375\n",
      "Validation Loss: 12439.181640625\n",
      "Test Loss: 13928.9208984375\n"
     ]
    }
   ],
   "source": [
    "#early stopping\n",
    "input_data = result\n",
    "\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "train_x, test_x, train_y, test_y = train_test_split(input_data, output_data, test_size=0.2, random_state=10)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.25, random_state=10)\n",
    "\n",
    "train_x = torch.from_numpy(train_x).float().to(device)\n",
    "train_y = torch.from_numpy(train_y).float().to(device)\n",
    "valid_x = torch.from_numpy(valid_x).float().to(device)\n",
    "valid_y = torch.from_numpy(valid_y).float().to(device)\n",
    "test_x = torch.from_numpy(test_x).float().to(device)\n",
    "test_y = torch.from_numpy(test_y).float().to(device)\n",
    "\n",
    "# Define Gaussian Process Regression model\n",
    "class GaussianProcessRegression(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Initialize model and likelihood\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "model = GaussianProcessRegression(train_x, train_y, likelihood).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.train()\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "best_loss = float(\"inf\")\n",
    "patience = 10  # Stop training if validation loss does not improve for 10 epochs\n",
    "counter = 0\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss {loss.item()}\")\n",
    "\n",
    "    # Evaluate the model on the validation set and check for improvement\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(valid_x)\n",
    "        loss = -mll(output, valid_y)\n",
    "        print(f\"Validation Loss: {loss.item()}\")\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"No improvement in {patience} epochs, stopping early.\")\n",
    "            break\n",
    "    model.train()\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(test_x)\n",
    "    loss = -mll(output, test_y)\n",
    "    print(f\"Test Loss: {loss.item()}\")\n",
    "\n",
    "    f_preds = model(test_x)\n",
    "    y_preds = likelihood(f_preds)\n",
    "    f_mean = f_preds.mean\n",
    "    f_var = f_preds.variance\n",
    "    f_covar = f_preds.covariance_matrix\n",
    "\n",
    "#original data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 2539520)\n",
      "Epoch 0, Loss 109497.21875\n",
      "Validation Loss: 182665.671875\n",
      "Validation Loss: 170368.875\n",
      "Validation Loss: 159221.875\n",
      "Validation Loss: 149124.640625\n",
      "Validation Loss: 139982.21875\n",
      "Validation Loss: 131705.140625\n",
      "Validation Loss: 124210.171875\n",
      "Validation Loss: 117420.3359375\n",
      "Validation Loss: 111264.9609375\n",
      "Validation Loss: 105679.703125\n",
      "Epoch 10, Loss 58947.0546875\n",
      "Validation Loss: 100606.140625\n",
      "Validation Loss: 95991.5\n",
      "Validation Loss: 91788.4375\n",
      "Validation Loss: 87954.359375\n",
      "Validation Loss: 84451.203125\n",
      "Validation Loss: 81245.03125\n",
      "Validation Loss: 78305.546875\n",
      "Validation Loss: 75605.7421875\n",
      "Validation Loss: 73121.578125\n",
      "Validation Loss: 70831.625\n",
      "Epoch 20, Loss 39493.4609375\n",
      "Validation Loss: 68716.859375\n",
      "Validation Loss: 66760.2734375\n",
      "Validation Loss: 64946.69921875\n",
      "Validation Loss: 63262.671875\n",
      "Validation Loss: 61696.08984375\n",
      "Validation Loss: 60236.1796875\n",
      "Validation Loss: 58873.296875\n",
      "Validation Loss: 57598.78125\n",
      "Validation Loss: 56404.859375\n",
      "Validation Loss: 55284.5859375\n",
      "Epoch 30, Loss 30814.509765625\n",
      "Validation Loss: 54231.6640625\n",
      "Validation Loss: 53240.4453125\n",
      "Validation Loss: 52305.8515625\n",
      "Validation Loss: 51423.25\n",
      "Validation Loss: 50588.5\n",
      "Validation Loss: 49797.8359375\n",
      "Validation Loss: 49047.8046875\n",
      "Validation Loss: 48335.32421875\n",
      "Validation Loss: 47657.57421875\n",
      "Validation Loss: 47011.9765625\n",
      "Epoch 40, Loss 26195.822265625\n",
      "Validation Loss: 46396.18359375\n",
      "Validation Loss: 45808.07421875\n",
      "Validation Loss: 45245.6953125\n",
      "Validation Loss: 44707.25\n",
      "Validation Loss: 44191.109375\n",
      "Validation Loss: 43695.77734375\n",
      "Validation Loss: 43219.875\n",
      "Validation Loss: 42762.15625\n",
      "Validation Loss: 42321.4453125\n",
      "Validation Loss: 41896.6875\n",
      "Epoch 50, Loss 23339.302734375\n",
      "Validation Loss: 41486.8828125\n",
      "Validation Loss: 41091.1484375\n",
      "Validation Loss: 40708.6171875\n",
      "Validation Loss: 40338.55078125\n",
      "Validation Loss: 39980.21484375\n",
      "Validation Loss: 39632.9375\n",
      "Validation Loss: 39296.1328125\n",
      "Validation Loss: 38969.22265625\n",
      "Validation Loss: 38651.671875\n",
      "Validation Loss: 38343.0\n",
      "Epoch 60, Loss 21354.359375\n",
      "Validation Loss: 38042.7421875\n",
      "Validation Loss: 37750.50390625\n",
      "Validation Loss: 37465.8515625\n",
      "Validation Loss: 37188.4375\n",
      "Validation Loss: 36917.9140625\n",
      "Validation Loss: 36653.9765625\n",
      "Validation Loss: 36396.30078125\n",
      "Validation Loss: 36144.62890625\n",
      "Validation Loss: 35898.67578125\n",
      "Validation Loss: 35658.21484375\n",
      "Epoch 70, Loss 19854.412109375\n",
      "Validation Loss: 35423.01171875\n",
      "Validation Loss: 35192.84375\n",
      "Validation Loss: 34967.5078125\n",
      "Validation Loss: 34746.8203125\n",
      "Validation Loss: 34530.59375\n",
      "Validation Loss: 34318.66015625\n",
      "Validation Loss: 34110.859375\n",
      "Validation Loss: 33907.03515625\n",
      "Validation Loss: 33707.05859375\n",
      "Validation Loss: 33510.78125\n",
      "Epoch 80, Loss 18654.419921875\n",
      "Validation Loss: 33318.07421875\n",
      "Validation Loss: 33128.828125\n",
      "Validation Loss: 32942.91796875\n",
      "Validation Loss: 32760.23828125\n",
      "Validation Loss: 32580.68359375\n",
      "Validation Loss: 32404.16015625\n",
      "Validation Loss: 32230.568359375\n",
      "Validation Loss: 32059.8203125\n",
      "Validation Loss: 31891.833984375\n",
      "Validation Loss: 31726.515625\n",
      "Epoch 90, Loss 17657.18359375\n",
      "Validation Loss: 31563.802734375\n",
      "Validation Loss: 31403.6171875\n",
      "Validation Loss: 31245.88671875\n",
      "Validation Loss: 31090.5390625\n",
      "Validation Loss: 30937.51953125\n",
      "Validation Loss: 30786.7578125\n",
      "Validation Loss: 30638.1875\n",
      "Validation Loss: 30491.76953125\n",
      "Validation Loss: 30347.44140625\n",
      "Validation Loss: 30205.140625\n",
      "Epoch 100, Loss 16806.71875\n",
      "Validation Loss: 30064.818359375\n",
      "Validation Loss: 29926.44140625\n",
      "Validation Loss: 29789.951171875\n",
      "Validation Loss: 29655.302734375\n",
      "Validation Loss: 29522.451171875\n",
      "Validation Loss: 29391.359375\n",
      "Validation Loss: 29261.984375\n",
      "Validation Loss: 29134.28515625\n",
      "Validation Loss: 29008.232421875\n",
      "Validation Loss: 28883.779296875\n",
      "Epoch 110, Loss 16067.9443359375\n",
      "Validation Loss: 28760.896484375\n",
      "Validation Loss: 28639.53515625\n",
      "Validation Loss: 28519.6796875\n",
      "Validation Loss: 28401.294921875\n",
      "Validation Loss: 28284.35546875\n",
      "Validation Loss: 28168.810546875\n",
      "Validation Loss: 28054.646484375\n",
      "Validation Loss: 27941.8359375\n",
      "Validation Loss: 27830.349609375\n",
      "Validation Loss: 27720.15234375\n",
      "Epoch 120, Loss 15417.25390625\n",
      "Validation Loss: 27611.2265625\n",
      "Validation Loss: 27503.546875\n",
      "Validation Loss: 27397.09765625\n",
      "Validation Loss: 27291.83203125\n",
      "Validation Loss: 27187.75390625\n",
      "Validation Loss: 27084.8125\n",
      "Validation Loss: 26983.013671875\n",
      "Validation Loss: 26882.3125\n",
      "Validation Loss: 26782.7109375\n",
      "Validation Loss: 26684.177734375\n",
      "Epoch 130, Loss 14837.8486328125\n",
      "Validation Loss: 26586.685546875\n",
      "Validation Loss: 26490.23046875\n",
      "Validation Loss: 26394.78125\n",
      "Validation Loss: 26300.33203125\n",
      "Validation Loss: 26206.85546875\n",
      "Validation Loss: 26114.33984375\n",
      "Validation Loss: 26022.763671875\n",
      "Validation Loss: 25932.1171875\n",
      "Validation Loss: 25842.375\n",
      "Validation Loss: 25753.537109375\n",
      "Epoch 140, Loss 14317.279296875\n",
      "Validation Loss: 25665.5703125\n",
      "Validation Loss: 25578.474609375\n",
      "Validation Loss: 25492.224609375\n",
      "Validation Loss: 25406.8125\n",
      "Validation Loss: 25322.224609375\n",
      "Validation Loss: 25238.44921875\n",
      "Validation Loss: 25155.4609375\n",
      "Validation Loss: 25073.263671875\n",
      "Validation Loss: 24991.83984375\n",
      "Validation Loss: 24911.169921875\n",
      "Epoch 150, Loss 13846.013671875\n",
      "Validation Loss: 24831.248046875\n",
      "Validation Loss: 24752.06640625\n",
      "Validation Loss: 24673.599609375\n",
      "Validation Loss: 24595.8515625\n",
      "Validation Loss: 24518.8046875\n",
      "Validation Loss: 24442.4453125\n",
      "Validation Loss: 24366.7734375\n",
      "Validation Loss: 24291.76953125\n",
      "Validation Loss: 24217.421875\n",
      "Validation Loss: 24143.73046875\n",
      "Epoch 160, Loss 13416.5986328125\n",
      "Validation Loss: 24070.67578125\n",
      "Validation Loss: 23998.25390625\n",
      "Validation Loss: 23926.44921875\n",
      "Validation Loss: 23855.26171875\n",
      "Validation Loss: 23784.685546875\n",
      "Validation Loss: 23714.701171875\n",
      "Validation Loss: 23645.30078125\n",
      "Validation Loss: 23576.4765625\n",
      "Validation Loss: 23508.234375\n",
      "Validation Loss: 23440.55078125\n",
      "Epoch 170, Loss 13023.0830078125\n",
      "Validation Loss: 23373.41796875\n",
      "Validation Loss: 23306.833984375\n",
      "Validation Loss: 23240.78515625\n",
      "Validation Loss: 23175.27734375\n",
      "Validation Loss: 23110.294921875\n",
      "Validation Loss: 23045.82421875\n",
      "Validation Loss: 22981.8671875\n",
      "Validation Loss: 22918.4140625\n",
      "Validation Loss: 22855.4609375\n",
      "Validation Loss: 22792.998046875\n",
      "Epoch 180, Loss 12660.64453125\n",
      "Validation Loss: 22731.015625\n",
      "Validation Loss: 22669.521484375\n",
      "Validation Loss: 22608.490234375\n",
      "Validation Loss: 22547.92578125\n",
      "Validation Loss: 22487.82421875\n",
      "Validation Loss: 22428.18359375\n",
      "Validation Loss: 22368.974609375\n",
      "Validation Loss: 22310.22265625\n",
      "Validation Loss: 22251.8984375\n",
      "Validation Loss: 22194.0078125\n",
      "Epoch 190, Loss 12325.3369140625\n",
      "Validation Loss: 22136.546875\n",
      "Validation Loss: 22079.50390625\n",
      "Validation Loss: 22022.873046875\n",
      "Validation Loss: 21966.66015625\n",
      "Validation Loss: 21910.84375\n",
      "Validation Loss: 21855.44140625\n",
      "Validation Loss: 21800.42578125\n",
      "Validation Loss: 21745.80078125\n",
      "Validation Loss: 21691.55859375\n",
      "Validation Loss: 21637.703125\n",
      "Epoch 200, Loss 12013.876953125\n",
      "Validation Loss: 21584.224609375\n",
      "Validation Loss: 21531.1171875\n",
      "Validation Loss: 21478.3828125\n",
      "Validation Loss: 21426.00390625\n",
      "Validation Loss: 21373.990234375\n",
      "Validation Loss: 21322.328125\n",
      "Validation Loss: 21271.0234375\n",
      "Validation Loss: 21220.060546875\n",
      "Validation Loss: 21169.4453125\n",
      "Validation Loss: 21119.16796875\n",
      "Epoch 210, Loss 11723.5224609375\n",
      "Validation Loss: 21069.2265625\n",
      "Validation Loss: 21019.615234375\n",
      "Validation Loss: 20970.328125\n",
      "Validation Loss: 20921.375\n",
      "Validation Loss: 20872.734375\n",
      "Validation Loss: 20824.416015625\n",
      "Validation Loss: 20776.412109375\n",
      "Validation Loss: 20728.71484375\n",
      "Validation Loss: 20681.326171875\n",
      "Validation Loss: 20634.23828125\n",
      "Epoch 220, Loss 11451.947265625\n",
      "Validation Loss: 20587.453125\n",
      "Validation Loss: 20540.96484375\n",
      "Validation Loss: 20494.771484375\n",
      "Validation Loss: 20448.869140625\n",
      "Validation Loss: 20403.25\n",
      "Validation Loss: 20357.91796875\n",
      "Validation Loss: 20312.8671875\n",
      "Validation Loss: 20268.095703125\n",
      "Validation Loss: 20223.603515625\n",
      "Validation Loss: 20179.37890625\n",
      "Epoch 230, Loss 11197.1796875\n",
      "Validation Loss: 20135.423828125\n",
      "Validation Loss: 20091.7421875\n",
      "Validation Loss: 20048.3203125\n",
      "Validation Loss: 20005.162109375\n",
      "Validation Loss: 19962.2578125\n",
      "Validation Loss: 19919.6171875\n",
      "Validation Loss: 19877.224609375\n",
      "Validation Loss: 19835.0859375\n",
      "Validation Loss: 19793.1953125\n",
      "Validation Loss: 19751.546875\n",
      "Epoch 240, Loss 10957.51953125\n",
      "Validation Loss: 19710.1484375\n",
      "Validation Loss: 19668.990234375\n",
      "Validation Loss: 19628.072265625\n",
      "Validation Loss: 19587.38671875\n",
      "Validation Loss: 19546.939453125\n",
      "Validation Loss: 19506.71875\n",
      "Validation Loss: 19466.732421875\n",
      "Validation Loss: 19426.97265625\n",
      "Validation Loss: 19387.44140625\n",
      "Validation Loss: 19348.125\n",
      "Epoch 250, Loss 10731.498046875\n",
      "Validation Loss: 19309.0390625\n",
      "Validation Loss: 19270.166015625\n",
      "Validation Loss: 19231.513671875\n",
      "Validation Loss: 19193.072265625\n",
      "Validation Loss: 19154.84765625\n",
      "Validation Loss: 19116.83203125\n",
      "Validation Loss: 19079.0234375\n",
      "Validation Loss: 19041.42578125\n",
      "Validation Loss: 19004.02734375\n",
      "Validation Loss: 18966.837890625\n",
      "Epoch 260, Loss 10517.84765625\n",
      "Validation Loss: 18929.84765625\n",
      "Validation Loss: 18893.056640625\n",
      "Validation Loss: 18856.462890625\n",
      "Validation Loss: 18820.06640625\n",
      "Validation Loss: 18783.86328125\n",
      "Validation Loss: 18747.849609375\n",
      "Validation Loss: 18712.02734375\n",
      "Validation Loss: 18676.39453125\n",
      "Validation Loss: 18640.94921875\n",
      "Validation Loss: 18605.6875\n",
      "Epoch 270, Loss 10315.453125\n",
      "Validation Loss: 18570.61328125\n",
      "Validation Loss: 18535.71875\n",
      "Validation Loss: 18501.005859375\n",
      "Validation Loss: 18466.46875\n",
      "Validation Loss: 18432.11328125\n",
      "Validation Loss: 18397.931640625\n",
      "Validation Loss: 18363.923828125\n",
      "Validation Loss: 18330.091796875\n",
      "Validation Loss: 18296.42578125\n",
      "Validation Loss: 18262.93359375\n",
      "Epoch 280, Loss 10123.34765625\n",
      "Validation Loss: 18229.609375\n",
      "Validation Loss: 18196.451171875\n",
      "Validation Loss: 18163.458984375\n",
      "Validation Loss: 18130.6328125\n",
      "Validation Loss: 18097.966796875\n",
      "Validation Loss: 18065.46484375\n",
      "Validation Loss: 18033.119140625\n",
      "Validation Loss: 18000.93359375\n",
      "Validation Loss: 17968.90625\n",
      "Validation Loss: 17937.03515625\n",
      "Epoch 290, Loss 9940.6591796875\n",
      "Validation Loss: 17905.31640625\n",
      "Validation Loss: 17873.75\n",
      "Validation Loss: 17842.33984375\n",
      "Validation Loss: 17811.078125\n",
      "Validation Loss: 17779.970703125\n",
      "Validation Loss: 17749.0078125\n",
      "Validation Loss: 17718.193359375\n",
      "Validation Loss: 17687.5234375\n",
      "Validation Loss: 17657.0\n",
      "Validation Loss: 17626.62109375\n",
      "Epoch 300, Loss 9766.63671875\n",
      "Validation Loss: 17596.380859375\n",
      "Validation Loss: 17566.2890625\n",
      "Validation Loss: 17536.3359375\n",
      "Validation Loss: 17506.515625\n",
      "Validation Loss: 17476.837890625\n",
      "Validation Loss: 17447.296875\n",
      "Validation Loss: 17417.89453125\n",
      "Validation Loss: 17388.625\n",
      "Validation Loss: 17359.484375\n",
      "Validation Loss: 17330.482421875\n",
      "Epoch 310, Loss 9600.591796875\n",
      "Validation Loss: 17301.609375\n",
      "Validation Loss: 17272.869140625\n",
      "Validation Loss: 17244.2578125\n",
      "Validation Loss: 17215.775390625\n",
      "Validation Loss: 17187.421875\n",
      "Validation Loss: 17159.19140625\n",
      "Validation Loss: 17131.09375\n",
      "Validation Loss: 17103.119140625\n",
      "Validation Loss: 17075.259765625\n",
      "Validation Loss: 17047.529296875\n",
      "Epoch 320, Loss 9441.9228515625\n",
      "Validation Loss: 17019.921875\n",
      "Validation Loss: 16992.4375\n",
      "Validation Loss: 16965.072265625\n",
      "Validation Loss: 16937.822265625\n",
      "Validation Loss: 16910.6953125\n",
      "Validation Loss: 16883.68359375\n",
      "Validation Loss: 16856.787109375\n",
      "Validation Loss: 16830.0078125\n",
      "Validation Loss: 16803.34765625\n",
      "Validation Loss: 16776.796875\n",
      "Epoch 330, Loss 9290.083984375\n",
      "Validation Loss: 16750.36328125\n",
      "Validation Loss: 16724.041015625\n",
      "Validation Loss: 16697.828125\n",
      "Validation Loss: 16671.7265625\n",
      "Validation Loss: 16645.734375\n",
      "Validation Loss: 16619.853515625\n",
      "Validation Loss: 16594.08203125\n",
      "Validation Loss: 16568.416015625\n",
      "Validation Loss: 16542.859375\n",
      "Validation Loss: 16517.408203125\n",
      "Epoch 340, Loss 9144.5908203125\n",
      "Validation Loss: 16492.064453125\n",
      "Validation Loss: 16466.818359375\n",
      "Validation Loss: 16441.68359375\n",
      "Validation Loss: 16416.650390625\n",
      "Validation Loss: 16391.716796875\n",
      "Validation Loss: 16366.884765625\n",
      "Validation Loss: 16342.158203125\n",
      "Validation Loss: 16317.5302734375\n",
      "Validation Loss: 16293.0029296875\n",
      "Validation Loss: 16268.5703125\n",
      "Epoch 350, Loss 9005.0048828125\n",
      "Validation Loss: 16244.244140625\n",
      "Validation Loss: 16220.0107421875\n",
      "Validation Loss: 16195.8701171875\n",
      "Validation Loss: 16171.83203125\n",
      "Validation Loss: 16147.8896484375\n",
      "Validation Loss: 16124.0390625\n",
      "Validation Loss: 16100.28515625\n",
      "Validation Loss: 16076.62109375\n",
      "Validation Loss: 16053.056640625\n",
      "Validation Loss: 16029.580078125\n",
      "Epoch 360, Loss 8870.9189453125\n",
      "Validation Loss: 16006.1943359375\n",
      "Validation Loss: 15982.9052734375\n",
      "Validation Loss: 15959.701171875\n",
      "Validation Loss: 15936.591796875\n",
      "Validation Loss: 15913.5703125\n",
      "Validation Loss: 15890.63671875\n",
      "Validation Loss: 15867.791015625\n",
      "Validation Loss: 15845.0341796875\n",
      "Validation Loss: 15822.3623046875\n",
      "Validation Loss: 15799.77734375\n",
      "Epoch 370, Loss 8741.9765625\n",
      "Validation Loss: 15777.283203125\n",
      "Validation Loss: 15754.87109375\n",
      "Validation Loss: 15732.5419921875\n",
      "Validation Loss: 15710.30078125\n",
      "Validation Loss: 15688.142578125\n",
      "Validation Loss: 15666.0654296875\n",
      "Validation Loss: 15644.072265625\n",
      "Validation Loss: 15622.16015625\n",
      "Validation Loss: 15600.3349609375\n",
      "Validation Loss: 15578.587890625\n",
      "Epoch 380, Loss 8617.8515625\n",
      "Validation Loss: 15556.9169921875\n",
      "Validation Loss: 15535.33203125\n",
      "Validation Loss: 15513.8232421875\n",
      "Validation Loss: 15492.3984375\n",
      "Validation Loss: 15471.0498046875\n",
      "Validation Loss: 15449.775390625\n",
      "Validation Loss: 15428.58203125\n",
      "Validation Loss: 15407.466796875\n",
      "Validation Loss: 15386.423828125\n",
      "Validation Loss: 15365.462890625\n",
      "Epoch 390, Loss 8498.236328125\n",
      "Validation Loss: 15344.572265625\n",
      "Validation Loss: 15323.759765625\n",
      "Validation Loss: 15303.0224609375\n",
      "Validation Loss: 15282.3623046875\n",
      "Validation Loss: 15261.7685546875\n",
      "Validation Loss: 15241.2529296875\n",
      "Validation Loss: 15220.80859375\n",
      "Validation Loss: 15200.439453125\n",
      "Validation Loss: 15180.142578125\n",
      "Validation Loss: 15159.9130859375\n",
      "Epoch 400, Loss 8382.8642578125\n",
      "Validation Loss: 15139.755859375\n",
      "Validation Loss: 15119.673828125\n",
      "Validation Loss: 15099.65625\n",
      "Validation Loss: 15079.7109375\n",
      "Validation Loss: 15059.837890625\n",
      "Validation Loss: 15040.033203125\n",
      "Validation Loss: 15020.294921875\n",
      "Validation Loss: 15000.6240234375\n",
      "Validation Loss: 14981.0234375\n",
      "Validation Loss: 14961.490234375\n",
      "Epoch 410, Loss 8271.4775390625\n",
      "Validation Loss: 14942.025390625\n",
      "Validation Loss: 14922.6240234375\n",
      "Validation Loss: 14903.2890625\n",
      "Validation Loss: 14884.021484375\n",
      "Validation Loss: 14864.8203125\n",
      "Validation Loss: 14845.68359375\n",
      "Validation Loss: 14826.6123046875\n",
      "Validation Loss: 14807.6015625\n",
      "Validation Loss: 14788.66015625\n",
      "Validation Loss: 14769.7841796875\n",
      "Epoch 420, Loss 8163.84619140625\n",
      "Validation Loss: 14750.962890625\n",
      "Validation Loss: 14732.212890625\n",
      "Validation Loss: 14713.51953125\n",
      "Validation Loss: 14694.892578125\n",
      "Validation Loss: 14676.32421875\n",
      "Validation Loss: 14657.8203125\n",
      "Validation Loss: 14639.376953125\n",
      "Validation Loss: 14620.9912109375\n",
      "Validation Loss: 14602.6708984375\n",
      "Validation Loss: 14584.40625\n",
      "Epoch 430, Loss 8059.76220703125\n",
      "Validation Loss: 14566.20703125\n",
      "Validation Loss: 14548.0615234375\n",
      "Validation Loss: 14529.978515625\n",
      "Validation Loss: 14511.9521484375\n",
      "Validation Loss: 14493.9873046875\n",
      "Validation Loss: 14476.0771484375\n",
      "Validation Loss: 14458.2265625\n",
      "Validation Loss: 14440.43359375\n",
      "Validation Loss: 14422.69921875\n",
      "Validation Loss: 14405.0224609375\n",
      "Epoch 440, Loss 7959.02978515625\n",
      "Validation Loss: 14387.396484375\n",
      "Validation Loss: 14369.8310546875\n",
      "Validation Loss: 14352.318359375\n",
      "Validation Loss: 14334.8662109375\n",
      "Validation Loss: 14317.4638671875\n",
      "Validation Loss: 14300.119140625\n",
      "Validation Loss: 14282.83203125\n",
      "Validation Loss: 14265.59375\n",
      "Validation Loss: 14248.4169921875\n",
      "Validation Loss: 14231.287109375\n",
      "Epoch 450, Loss 7861.45947265625\n",
      "Validation Loss: 14214.216796875\n",
      "Validation Loss: 14197.1943359375\n",
      "Validation Loss: 14180.228515625\n",
      "Validation Loss: 14163.314453125\n",
      "Validation Loss: 14146.451171875\n",
      "Validation Loss: 14129.6396484375\n",
      "Validation Loss: 14112.8818359375\n",
      "Validation Loss: 14096.1748046875\n",
      "Validation Loss: 14079.521484375\n",
      "Validation Loss: 14062.919921875\n",
      "Epoch 460, Loss 7766.890625\n",
      "Validation Loss: 14046.3662109375\n",
      "Validation Loss: 14029.86328125\n",
      "Validation Loss: 14013.4091796875\n",
      "Validation Loss: 13997.0068359375\n",
      "Validation Loss: 13980.654296875\n",
      "Validation Loss: 13964.3515625\n",
      "Validation Loss: 13948.095703125\n",
      "Validation Loss: 13931.888671875\n",
      "Validation Loss: 13915.734375\n",
      "Validation Loss: 13899.6279296875\n",
      "Epoch 470, Loss 7675.16748046875\n",
      "Validation Loss: 13883.568359375\n",
      "Validation Loss: 13867.556640625\n",
      "Validation Loss: 13851.591796875\n",
      "Validation Loss: 13835.6787109375\n",
      "Validation Loss: 13819.806640625\n",
      "Validation Loss: 13803.986328125\n",
      "Validation Loss: 13788.208984375\n",
      "Validation Loss: 13772.482421875\n",
      "Validation Loss: 13756.7998046875\n",
      "Validation Loss: 13741.1630859375\n",
      "Epoch 480, Loss 7586.14501953125\n",
      "Validation Loss: 13725.57421875\n",
      "Validation Loss: 13710.02734375\n",
      "Validation Loss: 13694.5302734375\n",
      "Validation Loss: 13679.07421875\n",
      "Validation Loss: 13663.6630859375\n",
      "Validation Loss: 13648.302734375\n",
      "Validation Loss: 13632.982421875\n",
      "Validation Loss: 13617.703125\n",
      "Validation Loss: 13602.4716796875\n",
      "Validation Loss: 13587.2841796875\n",
      "Epoch 490, Loss 7499.68994140625\n",
      "Validation Loss: 13572.1396484375\n",
      "Validation Loss: 13557.0390625\n",
      "Validation Loss: 13541.978515625\n",
      "Validation Loss: 13526.9677734375\n",
      "Validation Loss: 13511.9921875\n",
      "Validation Loss: 13497.0625\n",
      "Validation Loss: 13482.17578125\n",
      "Validation Loss: 13467.3310546875\n",
      "Validation Loss: 13452.529296875\n",
      "Validation Loss: 13437.7646484375\n",
      "Epoch 500, Loss 7415.67822265625\n",
      "Validation Loss: 13423.044921875\n",
      "Validation Loss: 13408.3671875\n",
      "Validation Loss: 13393.7294921875\n",
      "Validation Loss: 13379.1328125\n",
      "Validation Loss: 13364.576171875\n",
      "Validation Loss: 13350.060546875\n",
      "Validation Loss: 13335.5859375\n",
      "Validation Loss: 13321.150390625\n",
      "Validation Loss: 13306.75390625\n",
      "Validation Loss: 13292.400390625\n",
      "Epoch 510, Loss 7333.98974609375\n",
      "Validation Loss: 13278.08203125\n",
      "Validation Loss: 13263.8037109375\n",
      "Validation Loss: 13249.5693359375\n",
      "Validation Loss: 13235.3701171875\n",
      "Validation Loss: 13221.212890625\n",
      "Validation Loss: 13207.0888671875\n",
      "Validation Loss: 13193.0078125\n",
      "Validation Loss: 13178.96484375\n",
      "Validation Loss: 13164.95703125\n",
      "Validation Loss: 13150.9921875\n",
      "Epoch 520, Loss 7254.51708984375\n",
      "Validation Loss: 13137.060546875\n",
      "Validation Loss: 13123.1640625\n",
      "Validation Loss: 13109.314453125\n",
      "Validation Loss: 13095.494140625\n",
      "Validation Loss: 13081.712890625\n",
      "Validation Loss: 13067.96875\n",
      "Validation Loss: 13054.2607421875\n",
      "Validation Loss: 13040.591796875\n",
      "Validation Loss: 13026.95703125\n",
      "Validation Loss: 13013.361328125\n",
      "Epoch 530, Loss 7177.16015625\n",
      "Validation Loss: 12999.7958984375\n",
      "Validation Loss: 12986.2685546875\n",
      "Validation Loss: 12972.779296875\n",
      "Validation Loss: 12959.326171875\n",
      "Validation Loss: 12945.904296875\n",
      "Validation Loss: 12932.51953125\n",
      "Validation Loss: 12919.169921875\n",
      "Validation Loss: 12905.85546875\n",
      "Validation Loss: 12892.578125\n",
      "Validation Loss: 12879.33203125\n",
      "Epoch 540, Loss 7101.8232421875\n",
      "Validation Loss: 12866.1240234375\n",
      "Validation Loss: 12852.9482421875\n",
      "Validation Loss: 12839.8046875\n",
      "Validation Loss: 12826.6982421875\n",
      "Validation Loss: 12813.625\n",
      "Validation Loss: 12800.58203125\n",
      "Validation Loss: 12787.57421875\n",
      "Validation Loss: 12774.603515625\n",
      "Validation Loss: 12761.6630859375\n",
      "Validation Loss: 12748.755859375\n",
      "Epoch 550, Loss 7028.416015625\n",
      "Validation Loss: 12735.880859375\n",
      "Validation Loss: 12723.0390625\n",
      "Validation Loss: 12710.23046875\n",
      "Validation Loss: 12697.453125\n",
      "Validation Loss: 12684.708984375\n",
      "Validation Loss: 12671.998046875\n",
      "Validation Loss: 12659.322265625\n",
      "Validation Loss: 12646.671875\n",
      "Validation Loss: 12634.056640625\n",
      "Validation Loss: 12621.4716796875\n",
      "Epoch 560, Loss 6956.85595703125\n",
      "Validation Loss: 12608.9208984375\n",
      "Validation Loss: 12596.400390625\n",
      "Validation Loss: 12583.908203125\n",
      "Validation Loss: 12571.44921875\n",
      "Validation Loss: 12559.0234375\n",
      "Validation Loss: 12546.625\n",
      "Validation Loss: 12534.2607421875\n",
      "Validation Loss: 12521.9248046875\n",
      "Validation Loss: 12509.619140625\n",
      "Validation Loss: 12497.345703125\n",
      "Epoch 570, Loss 6887.0634765625\n",
      "Validation Loss: 12485.1015625\n",
      "Validation Loss: 12472.8857421875\n",
      "Validation Loss: 12460.703125\n",
      "Validation Loss: 12448.548828125\n",
      "Validation Loss: 12436.423828125\n",
      "Validation Loss: 12424.328125\n",
      "Validation Loss: 12412.263671875\n",
      "Validation Loss: 12400.2265625\n",
      "Validation Loss: 12388.21875\n",
      "Validation Loss: 12376.2421875\n",
      "Epoch 580, Loss 6818.9658203125\n",
      "Validation Loss: 12364.294921875\n",
      "Validation Loss: 12352.37109375\n",
      "Validation Loss: 12340.4833984375\n",
      "Validation Loss: 12328.6171875\n",
      "Validation Loss: 12316.7841796875\n",
      "Validation Loss: 12304.9794921875\n",
      "Validation Loss: 12293.201171875\n",
      "Validation Loss: 12281.44921875\n",
      "Validation Loss: 12269.7294921875\n",
      "Validation Loss: 12258.037109375\n",
      "Epoch 590, Loss 6752.490234375\n",
      "Validation Loss: 12246.3701171875\n",
      "Validation Loss: 12234.7333984375\n",
      "Validation Loss: 12223.1240234375\n",
      "Validation Loss: 12211.5390625\n",
      "Validation Loss: 12199.9833984375\n",
      "Validation Loss: 12188.4560546875\n",
      "Validation Loss: 12176.953125\n",
      "Validation Loss: 12165.4794921875\n",
      "Validation Loss: 12154.033203125\n",
      "Validation Loss: 12142.611328125\n",
      "Epoch 600, Loss 6687.57373046875\n",
      "Validation Loss: 12131.216796875\n",
      "Validation Loss: 12119.8505859375\n",
      "Validation Loss: 12108.5107421875\n",
      "Validation Loss: 12097.1953125\n",
      "Validation Loss: 12085.908203125\n",
      "Validation Loss: 12074.646484375\n",
      "Validation Loss: 12063.41015625\n",
      "Validation Loss: 12052.201171875\n",
      "Validation Loss: 12041.0146484375\n",
      "Validation Loss: 12029.857421875\n",
      "Epoch 610, Loss 6624.15185546875\n",
      "Validation Loss: 12018.72265625\n",
      "Validation Loss: 12007.6162109375\n",
      "Validation Loss: 11996.5380859375\n",
      "Validation Loss: 11985.4794921875\n",
      "Validation Loss: 11974.4482421875\n",
      "Validation Loss: 11963.4404296875\n",
      "Validation Loss: 11952.462890625\n",
      "Validation Loss: 11941.50390625\n",
      "Validation Loss: 11930.5732421875\n",
      "Validation Loss: 11919.666015625\n",
      "Epoch 620, Loss 6562.16943359375\n",
      "Validation Loss: 11908.787109375\n",
      "Validation Loss: 11897.9287109375\n",
      "Validation Loss: 11887.09375\n",
      "Validation Loss: 11876.28515625\n",
      "Validation Loss: 11865.501953125\n",
      "Validation Loss: 11854.740234375\n",
      "Validation Loss: 11844.0078125\n",
      "Validation Loss: 11833.29296875\n",
      "Validation Loss: 11822.60546875\n",
      "Validation Loss: 11811.9443359375\n",
      "Epoch 630, Loss 6501.56591796875\n",
      "Validation Loss: 11801.302734375\n",
      "Validation Loss: 11790.68359375\n",
      "Validation Loss: 11780.091796875\n",
      "Validation Loss: 11769.521484375\n",
      "Validation Loss: 11758.974609375\n",
      "Validation Loss: 11748.451171875\n",
      "Validation Loss: 11737.9521484375\n",
      "Validation Loss: 11727.4755859375\n",
      "Validation Loss: 11717.01953125\n",
      "Validation Loss: 11706.58984375\n",
      "Epoch 640, Loss 6442.29638671875\n",
      "Validation Loss: 11696.1826171875\n",
      "Validation Loss: 11685.794921875\n",
      "Validation Loss: 11675.4326171875\n",
      "Validation Loss: 11665.08984375\n",
      "Validation Loss: 11654.771484375\n",
      "Validation Loss: 11644.4765625\n",
      "Validation Loss: 11634.203125\n",
      "Validation Loss: 11623.955078125\n",
      "Validation Loss: 11613.7255859375\n",
      "Validation Loss: 11603.5185546875\n",
      "Epoch 650, Loss 6384.30224609375\n",
      "Validation Loss: 11593.3349609375\n",
      "Validation Loss: 11583.171875\n",
      "Validation Loss: 11573.03125\n",
      "Validation Loss: 11562.9091796875\n",
      "Validation Loss: 11552.8115234375\n",
      "Validation Loss: 11542.7373046875\n",
      "Validation Loss: 11532.68359375\n",
      "Validation Loss: 11522.650390625\n",
      "Validation Loss: 11512.63671875\n",
      "Validation Loss: 11502.646484375\n",
      "Epoch 660, Loss 6327.54052734375\n",
      "Validation Loss: 11492.677734375\n",
      "Validation Loss: 11482.73046875\n",
      "Validation Loss: 11472.7998046875\n",
      "Validation Loss: 11462.8974609375\n",
      "Validation Loss: 11453.01171875\n",
      "Validation Loss: 11443.14453125\n",
      "Validation Loss: 11433.3037109375\n",
      "Validation Loss: 11423.48046875\n",
      "Validation Loss: 11413.6748046875\n",
      "Validation Loss: 11403.8974609375\n",
      "Epoch 670, Loss 6271.96923828125\n",
      "Validation Loss: 11394.134765625\n",
      "Validation Loss: 11384.392578125\n",
      "Validation Loss: 11374.669921875\n",
      "Validation Loss: 11364.96875\n",
      "Validation Loss: 11355.2890625\n",
      "Validation Loss: 11345.6279296875\n",
      "Validation Loss: 11335.98828125\n",
      "Validation Loss: 11326.3662109375\n",
      "Validation Loss: 11316.7646484375\n",
      "Validation Loss: 11307.18359375\n",
      "Epoch 680, Loss 6217.541015625\n",
      "Validation Loss: 11297.623046875\n",
      "Validation Loss: 11288.0810546875\n",
      "Validation Loss: 11278.556640625\n",
      "Validation Loss: 11269.056640625\n",
      "Validation Loss: 11259.57421875\n",
      "Validation Loss: 11250.1083984375\n",
      "Validation Loss: 11240.6640625\n",
      "Validation Loss: 11231.23828125\n",
      "Validation Loss: 11221.83203125\n",
      "Validation Loss: 11212.4453125\n",
      "Epoch 690, Loss 6164.21923828125\n",
      "Validation Loss: 11203.078125\n",
      "Validation Loss: 11193.7265625\n",
      "Validation Loss: 11184.3974609375\n",
      "Validation Loss: 11175.087890625\n",
      "Validation Loss: 11165.794921875\n",
      "Validation Loss: 11156.521484375\n",
      "Validation Loss: 11147.263671875\n",
      "Validation Loss: 11138.029296875\n",
      "Validation Loss: 11128.810546875\n",
      "Validation Loss: 11119.6083984375\n",
      "Epoch 700, Loss 6111.96337890625\n",
      "Validation Loss: 11110.42578125\n",
      "Validation Loss: 11101.265625\n",
      "Validation Loss: 11092.1201171875\n",
      "Validation Loss: 11082.9921875\n",
      "Validation Loss: 11073.8828125\n",
      "Validation Loss: 11064.7958984375\n",
      "Validation Loss: 11055.72265625\n",
      "Validation Loss: 11046.666015625\n",
      "Validation Loss: 11037.630859375\n",
      "Validation Loss: 11028.609375\n",
      "Epoch 710, Loss 6060.73779296875\n",
      "Validation Loss: 11019.607421875\n",
      "Validation Loss: 11010.625\n",
      "Validation Loss: 11001.66015625\n",
      "Validation Loss: 10992.7109375\n",
      "Validation Loss: 10983.7802734375\n",
      "Validation Loss: 10974.8662109375\n",
      "Validation Loss: 10965.970703125\n",
      "Validation Loss: 10957.091796875\n",
      "Validation Loss: 10948.23046875\n",
      "Validation Loss: 10939.38671875\n",
      "Epoch 720, Loss 6010.509765625\n",
      "Validation Loss: 10930.560546875\n",
      "Validation Loss: 10921.748046875\n",
      "Validation Loss: 10912.955078125\n",
      "Validation Loss: 10904.1806640625\n",
      "Validation Loss: 10895.4228515625\n",
      "Validation Loss: 10886.6796875\n",
      "Validation Loss: 10877.955078125\n",
      "Validation Loss: 10869.2451171875\n",
      "Validation Loss: 10860.5537109375\n",
      "Validation Loss: 10851.876953125\n",
      "Epoch 730, Loss 5961.2412109375\n",
      "Validation Loss: 10843.2197265625\n",
      "Validation Loss: 10834.578125\n",
      "Validation Loss: 10825.951171875\n",
      "Validation Loss: 10817.341796875\n",
      "Validation Loss: 10808.7490234375\n",
      "Validation Loss: 10800.173828125\n",
      "Validation Loss: 10791.61328125\n",
      "Validation Loss: 10783.0693359375\n",
      "Validation Loss: 10774.541015625\n",
      "Validation Loss: 10766.03125\n",
      "Epoch 740, Loss 5912.9052734375\n",
      "Validation Loss: 10757.5322265625\n",
      "Validation Loss: 10749.052734375\n",
      "Validation Loss: 10740.58984375\n",
      "Validation Loss: 10732.1416015625\n",
      "Validation Loss: 10723.7119140625\n",
      "Validation Loss: 10715.2919921875\n",
      "Validation Loss: 10706.8935546875\n",
      "Validation Loss: 10698.5078125\n",
      "Validation Loss: 10690.140625\n",
      "Validation Loss: 10681.787109375\n",
      "Epoch 750, Loss 5865.47119140625\n",
      "Validation Loss: 10673.44921875\n",
      "Validation Loss: 10665.1259765625\n",
      "Validation Loss: 10656.8173828125\n",
      "Validation Loss: 10648.5263671875\n",
      "Validation Loss: 10640.2490234375\n",
      "Validation Loss: 10631.9873046875\n",
      "Validation Loss: 10623.7421875\n",
      "Validation Loss: 10615.51171875\n",
      "Validation Loss: 10607.294921875\n",
      "Validation Loss: 10599.0947265625\n",
      "Epoch 760, Loss 5818.9052734375\n",
      "Validation Loss: 10590.908203125\n",
      "Validation Loss: 10582.7392578125\n",
      "Validation Loss: 10574.583984375\n",
      "Validation Loss: 10566.4423828125\n",
      "Validation Loss: 10558.3193359375\n",
      "Validation Loss: 10550.2080078125\n",
      "Validation Loss: 10542.111328125\n",
      "Validation Loss: 10534.029296875\n",
      "Validation Loss: 10525.96484375\n",
      "Validation Loss: 10517.912109375\n",
      "Epoch 770, Loss 5773.185546875\n",
      "Validation Loss: 10509.8740234375\n",
      "Validation Loss: 10501.849609375\n",
      "Validation Loss: 10493.83984375\n",
      "Validation Loss: 10485.8466796875\n",
      "Validation Loss: 10477.869140625\n",
      "Validation Loss: 10469.90234375\n",
      "Validation Loss: 10461.951171875\n",
      "Validation Loss: 10454.013671875\n",
      "Validation Loss: 10446.091796875\n",
      "Validation Loss: 10438.181640625\n",
      "Epoch 780, Loss 5728.28271484375\n",
      "Validation Loss: 10430.2900390625\n",
      "Validation Loss: 10422.4091796875\n",
      "Validation Loss: 10414.54296875\n",
      "Validation Loss: 10406.6923828125\n",
      "Validation Loss: 10398.8544921875\n",
      "Validation Loss: 10391.029296875\n",
      "Validation Loss: 10383.2197265625\n",
      "Validation Loss: 10375.4228515625\n",
      "Validation Loss: 10367.640625\n",
      "Validation Loss: 10359.8720703125\n",
      "Epoch 790, Loss 5684.17431640625\n",
      "Validation Loss: 10352.115234375\n",
      "Validation Loss: 10344.373046875\n",
      "Validation Loss: 10336.64453125\n",
      "Validation Loss: 10328.931640625\n",
      "Validation Loss: 10321.2314453125\n",
      "Validation Loss: 10313.5439453125\n",
      "Validation Loss: 10305.869140625\n",
      "Validation Loss: 10298.208984375\n",
      "Validation Loss: 10290.560546875\n",
      "Validation Loss: 10282.9267578125\n",
      "Epoch 800, Loss 5640.83544921875\n",
      "Validation Loss: 10275.306640625\n",
      "Validation Loss: 10267.6982421875\n",
      "Validation Loss: 10260.1044921875\n",
      "Validation Loss: 10252.5234375\n",
      "Validation Loss: 10244.955078125\n",
      "Validation Loss: 10237.4013671875\n",
      "Validation Loss: 10229.859375\n",
      "Validation Loss: 10222.330078125\n",
      "Validation Loss: 10214.81640625\n",
      "Validation Loss: 10207.3115234375\n",
      "Epoch 810, Loss 5598.23974609375\n",
      "Validation Loss: 10199.82421875\n",
      "Validation Loss: 10192.345703125\n",
      "Validation Loss: 10184.8828125\n",
      "Validation Loss: 10177.4306640625\n",
      "Validation Loss: 10169.990234375\n",
      "Validation Loss: 10162.564453125\n",
      "Validation Loss: 10155.15234375\n",
      "Validation Loss: 10147.75\n",
      "Validation Loss: 10140.3642578125\n",
      "Validation Loss: 10132.98828125\n",
      "Epoch 820, Loss 5556.369140625\n",
      "Validation Loss: 10125.623046875\n",
      "Validation Loss: 10118.2744140625\n",
      "Validation Loss: 10110.935546875\n",
      "Validation Loss: 10103.6103515625\n",
      "Validation Loss: 10096.296875\n",
      "Validation Loss: 10088.99609375\n",
      "Validation Loss: 10081.7080078125\n",
      "Validation Loss: 10074.431640625\n",
      "Validation Loss: 10067.16796875\n",
      "Validation Loss: 10059.916015625\n",
      "Epoch 830, Loss 5515.2021484375\n",
      "Validation Loss: 10052.6748046875\n",
      "Validation Loss: 10045.44921875\n",
      "Validation Loss: 10038.232421875\n",
      "Validation Loss: 10031.0302734375\n",
      "Validation Loss: 10023.8388671875\n",
      "Validation Loss: 10016.6572265625\n",
      "Validation Loss: 10009.4892578125\n",
      "Validation Loss: 10002.3359375\n",
      "Validation Loss: 9995.1904296875\n",
      "Validation Loss: 9988.05859375\n",
      "Epoch 840, Loss 5474.716796875\n",
      "Validation Loss: 9980.9384765625\n",
      "Validation Loss: 9973.830078125\n",
      "Validation Loss: 9966.732421875\n",
      "Validation Loss: 9959.6484375\n",
      "Validation Loss: 9952.576171875\n",
      "Validation Loss: 9945.513671875\n",
      "Validation Loss: 9938.462890625\n",
      "Validation Loss: 9931.4248046875\n",
      "Validation Loss: 9924.3974609375\n",
      "Validation Loss: 9917.3818359375\n",
      "Epoch 850, Loss 5434.89453125\n",
      "Validation Loss: 9910.3779296875\n",
      "Validation Loss: 9903.3857421875\n",
      "Validation Loss: 9896.4033203125\n",
      "Validation Loss: 9889.4345703125\n",
      "Validation Loss: 9882.478515625\n",
      "Validation Loss: 9875.5283203125\n",
      "Validation Loss: 9868.591796875\n",
      "Validation Loss: 9861.6689453125\n",
      "Validation Loss: 9854.755859375\n",
      "Validation Loss: 9847.853515625\n",
      "Epoch 860, Loss 5395.716796875\n",
      "Validation Loss: 9840.9609375\n",
      "Validation Loss: 9834.08203125\n",
      "Validation Loss: 9827.21484375\n",
      "Validation Loss: 9820.35546875\n",
      "Validation Loss: 9813.509765625\n",
      "Validation Loss: 9806.673828125\n",
      "Validation Loss: 9799.8486328125\n",
      "Validation Loss: 9793.03515625\n",
      "Validation Loss: 9786.23046875\n",
      "Validation Loss: 9779.44140625\n",
      "Epoch 870, Loss 5357.16455078125\n",
      "Validation Loss: 9772.658203125\n",
      "Validation Loss: 9765.888671875\n",
      "Validation Loss: 9759.1298828125\n",
      "Validation Loss: 9752.3779296875\n",
      "Validation Loss: 9745.6416015625\n",
      "Validation Loss: 9738.912109375\n",
      "Validation Loss: 9732.197265625\n",
      "Validation Loss: 9725.4912109375\n",
      "Validation Loss: 9718.794921875\n",
      "Validation Loss: 9712.1103515625\n",
      "Epoch 880, Loss 5319.22119140625\n",
      "Validation Loss: 9705.4345703125\n",
      "Validation Loss: 9698.7705078125\n",
      "Validation Loss: 9692.1171875\n",
      "Validation Loss: 9685.474609375\n",
      "Validation Loss: 9678.841796875\n",
      "Validation Loss: 9672.220703125\n",
      "Validation Loss: 9665.609375\n",
      "Validation Loss: 9659.0048828125\n",
      "Validation Loss: 9652.416015625\n",
      "Validation Loss: 9645.8349609375\n",
      "Epoch 890, Loss 5281.86962890625\n",
      "Validation Loss: 9639.263671875\n",
      "Validation Loss: 9632.705078125\n",
      "Validation Loss: 9626.154296875\n",
      "Validation Loss: 9619.61328125\n",
      "Validation Loss: 9613.083984375\n",
      "Validation Loss: 9606.5625\n",
      "Validation Loss: 9600.0537109375\n",
      "Validation Loss: 9593.5556640625\n",
      "Validation Loss: 9587.0654296875\n",
      "Validation Loss: 9580.5849609375\n",
      "Epoch 900, Loss 5245.0947265625\n",
      "Validation Loss: 9574.115234375\n",
      "Validation Loss: 9567.654296875\n",
      "Validation Loss: 9561.205078125\n",
      "Validation Loss: 9554.7666015625\n",
      "Validation Loss: 9548.3359375\n",
      "Validation Loss: 9541.9169921875\n",
      "Validation Loss: 9535.5078125\n",
      "Validation Loss: 9529.107421875\n",
      "Validation Loss: 9522.716796875\n",
      "Validation Loss: 9516.3359375\n",
      "Epoch 910, Loss 5208.88134765625\n",
      "Validation Loss: 9509.962890625\n",
      "Validation Loss: 9503.6015625\n",
      "Validation Loss: 9497.251953125\n",
      "Validation Loss: 9490.9091796875\n",
      "Validation Loss: 9484.5771484375\n",
      "Validation Loss: 9478.2548828125\n",
      "Validation Loss: 9471.94140625\n",
      "Validation Loss: 9465.63671875\n",
      "Validation Loss: 9459.3427734375\n",
      "Validation Loss: 9453.05859375\n",
      "Epoch 920, Loss 5173.21337890625\n",
      "Validation Loss: 9446.78125\n",
      "Validation Loss: 9440.517578125\n",
      "Validation Loss: 9434.259765625\n",
      "Validation Loss: 9428.015625\n",
      "Validation Loss: 9421.77734375\n",
      "Validation Loss: 9415.548828125\n",
      "Validation Loss: 9409.330078125\n",
      "Validation Loss: 9403.119140625\n",
      "Validation Loss: 9396.919921875\n",
      "Validation Loss: 9390.7294921875\n",
      "Epoch 930, Loss 5138.0791015625\n",
      "Validation Loss: 9384.546875\n",
      "Validation Loss: 9378.373046875\n",
      "Validation Loss: 9372.2109375\n",
      "Validation Loss: 9366.056640625\n",
      "Validation Loss: 9359.91015625\n",
      "Validation Loss: 9353.7763671875\n",
      "Validation Loss: 9347.6474609375\n",
      "Validation Loss: 9341.53125\n",
      "Validation Loss: 9335.421875\n",
      "Validation Loss: 9329.3203125\n",
      "Epoch 940, Loss 5103.4619140625\n",
      "Validation Loss: 9323.2314453125\n",
      "Validation Loss: 9317.1474609375\n",
      "Validation Loss: 9311.076171875\n",
      "Validation Loss: 9305.01171875\n",
      "Validation Loss: 9298.95703125\n",
      "Validation Loss: 9292.9111328125\n",
      "Validation Loss: 9286.873046875\n",
      "Validation Loss: 9280.84375\n",
      "Validation Loss: 9274.82421875\n",
      "Validation Loss: 9268.8134765625\n",
      "Epoch 950, Loss 5069.35009765625\n",
      "Validation Loss: 9262.8115234375\n",
      "Validation Loss: 9256.81640625\n",
      "Validation Loss: 9250.8330078125\n",
      "Validation Loss: 9244.857421875\n",
      "Validation Loss: 9238.888671875\n",
      "Validation Loss: 9232.9296875\n",
      "Validation Loss: 9226.98046875\n",
      "Validation Loss: 9221.037109375\n",
      "Validation Loss: 9215.103515625\n",
      "Validation Loss: 9209.181640625\n",
      "Epoch 960, Loss 5035.73046875\n",
      "Validation Loss: 9203.2646484375\n",
      "Validation Loss: 9197.357421875\n",
      "Validation Loss: 9191.458984375\n",
      "Validation Loss: 9185.5693359375\n",
      "Validation Loss: 9179.6875\n",
      "Validation Loss: 9173.8134765625\n",
      "Validation Loss: 9167.9501953125\n",
      "Validation Loss: 9162.091796875\n",
      "Validation Loss: 9156.2451171875\n",
      "Validation Loss: 9150.404296875\n",
      "Epoch 970, Loss 5002.59228515625\n",
      "Validation Loss: 9144.572265625\n",
      "Validation Loss: 9138.75\n",
      "Validation Loss: 9132.9345703125\n",
      "Validation Loss: 9127.126953125\n",
      "Validation Loss: 9121.326171875\n",
      "Validation Loss: 9115.537109375\n",
      "Validation Loss: 9109.755859375\n",
      "Validation Loss: 9103.982421875\n",
      "Validation Loss: 9098.2158203125\n",
      "Validation Loss: 9092.4580078125\n",
      "Epoch 980, Loss 4969.9189453125\n",
      "Validation Loss: 9086.708984375\n",
      "Validation Loss: 9080.966796875\n",
      "Validation Loss: 9075.232421875\n",
      "Validation Loss: 9069.5087890625\n",
      "Validation Loss: 9063.791015625\n",
      "Validation Loss: 9058.08203125\n",
      "Validation Loss: 9052.37890625\n",
      "Validation Loss: 9046.685546875\n",
      "Validation Loss: 9041.0\n",
      "Validation Loss: 9035.3251953125\n",
      "Epoch 990, Loss 4937.70361328125\n",
      "Validation Loss: 9029.6552734375\n",
      "Validation Loss: 9023.9931640625\n",
      "Validation Loss: 9018.3408203125\n",
      "Validation Loss: 9012.6943359375\n",
      "Validation Loss: 9007.056640625\n",
      "Validation Loss: 9001.4267578125\n",
      "Validation Loss: 8995.8056640625\n",
      "Validation Loss: 8990.19140625\n",
      "Validation Loss: 8984.5859375\n",
      "Validation Loss: 8978.984375\n",
      "Test Loss: 9932.6953125\n"
     ]
    }
   ],
   "source": [
    "#early stopping with output from NF\n",
    "input_dir = \"/dhc/home/youngbin.ko/glow_brain/model/output\" # Shape (1,2539520 ) npy\n",
    "input_files = sorted(os.listdir(input_dir))\n",
    "input_data = np.concatenate([np.load(os.path.join(input_dir, f)) for f in input_files], axis=0)\n",
    "print(input_data.shape) #(120,2539520)\n",
    "\n",
    "output_data = np.load(\"/dhc/home/youngbin.ko/brain_data/data/survival_days.npy\")  # Shape (120,)\n",
    "train_x, test_x, train_y, test_y = train_test_split(input_data, output_data, test_size=0.2, random_state=10)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.25, random_state=10)\n",
    "\n",
    "train_x = torch.from_numpy(train_x).float().to(device)\n",
    "train_y = torch.from_numpy(train_y).float().to(device)\n",
    "valid_x = torch.from_numpy(valid_x).float().to(device)\n",
    "valid_y = torch.from_numpy(valid_y).float().to(device)\n",
    "test_x = torch.from_numpy(test_x).float().to(device)\n",
    "test_y = torch.from_numpy(test_y).float().to(device)\n",
    "\n",
    "# Define Gaussian Process Regression model\n",
    "class GaussianProcessRegression(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Initialize model and likelihood\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "model = GaussianProcessRegression(train_x, train_y, likelihood).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.train()\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "best_loss = float(\"inf\")\n",
    "patience = 10  # Stop training if validation loss does not improve for 10 epochs\n",
    "counter = 0\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss {loss.item()}\")\n",
    "\n",
    "    # Evaluate the model on the validation set and check for improvement\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(valid_x)\n",
    "        loss = -mll(output, valid_y)\n",
    "        print(f\"Validation Loss: {loss.item()}\")\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"No improvement in {patience} epochs, stopping early.\")\n",
    "            break\n",
    "    model.train()\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(test_x)\n",
    "    loss = -mll(output, test_y)\n",
    "    print(f\"Test Loss: {loss.item()}\")\n",
    "\n",
    "    f_preds = model(test_x)\n",
    "    y_preds = likelihood(f_preds)\n",
    "    f_mean = f_preds.mean\n",
    "    f_var = f_preds.variance\n",
    "    f_covar = f_preds.covariance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m ax\u001b[39m.\u001b[39mplot(train_x\u001b[39m.\u001b[39mcpu(), train_y\u001b[39m.\u001b[39mcpu(), \u001b[39m'\u001b[39m\u001b[39mk*\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Plot predictive means as blue line\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m ax\u001b[39m.\u001b[39mplot(test_x\u001b[39m.\u001b[39mcpu(), y_preds\u001b[39m.\u001b[39;49mmean\u001b[39m.\u001b[39;49mnumpy(), \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Shade between the lower and upper confidence bounds\u001b[39;00m\n\u001b[1;32m     12\u001b[0m ax\u001b[39m.\u001b[39mfill_between(test_x\u001b[39m.\u001b[39mcpu(), lower\u001b[39m.\u001b[39mnumpy(), upper\u001b[39m.\u001b[39mnumpy(), alpha\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = y_preds.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(train_x.cpu(), train_y.cpu(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.cpu(), y_preds.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.cpu(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glowenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
